#+Title: Optimizations for the metric dimension of the hypercube
#+Latex_header: \newcommand{\B}{\{0,1\}}
* Translations
Let $V$ denote $\{0,1\}$, and $\oplus$ denote elementwise exclusive
or, $e$ denote the vector of dimension $n$ all whose coordinates
are 1.  If $v \in V^n$, denote by, $w(v)$ its Hamming weight and by
$R(v)$ the set of pairs of vectors resolved by $x$.  That is $R(v) =
\{ (x,y) : x \ne y, w(v \oplus x) != w(v \oplus y) \}$.
1) If $v \in V^n$, then $R(v \oplus e) = R(v)$.  Thus, without loss of
   generality, we may assume that any vector in the resolving set has
   weight $\le n/2$.
2) If $S$ is a resolving set, and $v \in V^n$, then $S \oplus v = \{u
   \oplus v : u \in S\}$ is also a resolving set.  Thus, without loss
   of generality we may assume that $0 \in S$.

So from now on, implictly assume that $0 \in S$ and only consider the
nonzero members.

From now one we write the nonzero elements of $S$ as an $m \times n$
matrix $A$ whose rows are the elements of $S$

Note that $0$ resolves all pairs of distinct vectors of different
weights.  So we may assume that the pairs to be considered have the
same weight.  A short calculation shows that $v \in V^n$ resolves
$(x,y)$ of the same weight if and only if $v \cdot (x-y) \ne 0$, where
that calculation is done over the reals.  Thus we only need to
consider vectors of the form $x-y$.  These will exactly be nonzero
vectors whose coordinates are in $\{0,1,-1\}$ and the sum of the all
the coordinates is 0.  We can represent such a vector as a pair of 0/1
vectors $(x,y)$ so that $x_i$ and $y_i$ can't be 1 at the same time,
and $w(x) = w(y)$.  There is an additional symmetry since $x-y$ and
$y-x$ both occur: the first nonzero element of $x-y$ should be 1.
Also resolving all  pairs $(x,y)$ with $w(x) = w(y) = 1$ is exactly
equivalent to all columns of the $A$ matrix being distinct.  We can
omit those pairs and require that $w(x) = w(y) \ge 2$.

Since $S$ is a set, we can permute any of the rows of $A$, so, a
symmetry break would be make sure that the rows strictly increase in
lexicographic order.  Additionally, since the set of pairs $(x,y)$ is
invariant under permuting the coordinates, and, as above, they must be
distinct, we can also add the constraint that the columns are
lexicographically stricltly increasing.

There are other symmetry breaking constraints that I found, but, for
some reason they seemed to slow things down.  I'll discuss them later.

* More symmetry breaking constraints

The set of pairs $(x,y)$ is invariant under permuting coordinates.
Therefore, so is the set possible set of $A$ matrices.  As before, the
all 0 vector may be assumed to be in the resolving set, so we don't
explicitly represent it.  Imagine building up the $A$ matrix one row
at a time.  Since we may permute the columns in any way, without loss
of generality we may assume that the first row has the form
$0^r 1^s$, where $r+s=n$.  That is the first $r$ entries are 0 and the
remaining $s$ are 1.  In filling out the second row, we are free to
permute the first $r$ columns among themselves, the the last $s$
columns among themseves.  Therefore, we may assume that the second row
has the form $0^t 1^u 0^v 1^w$, where $t+u=r$ and $v+w=s$.  In
general, by induction, when adding the next row we are free to permute
columns which have equal values.    We also will build them up so that
all equal columns will be adjacent to one another.  Accordingly, we
can define variables $e(r,i)$ meaning that the column consisting of
the first $r$ rows of column $i$ is equal to the column consisting of
the first $r$ rows of columns $i+1$.  These are defined (1-origin) for
$1 \le r \le m$ and $1 \le i \le n-1$.  They can be defined
inductively by $c(r,i) = c(r-1,i) & (a(r,i) == a(r,i+1))$.  Call the a
set of columns a "region" if all of those columns through row $r-1$
are equal.  We want to have the 1's in a region moved the "right".  We
can do this by the constraint $((a(r,i) & c(r-1,i)) => a(r,i+1))$.
Notice that, since the constraints implied by resolving pairs $(x,y)$
where $x$ and $y$ have weight 1 are exactly equivalent to the columns
of $a$ being distinct, that setting $c(m,i)$ to false for all $1 \le i
\le n-1$ will guarantee this.

The above should take care of the symmetries of the columns.  To take
care of the symmetries of the rows, I think that lexicographic
ordering is valid.  It also might be useful to have a hybrid ordering:
that the weights of the rows are nondecreasing, and ties are broken
lexicographically.  The latter are more involved.  I'm not sure which
will work better.

* Resolving vectors
Question: is there a good score that we can compute for the
counterexamples?

As part of the score calculation, we want to see how many matrices a
set of balanced vectors will rule out?  if $X$ is a set of resolving
vectors we want to calculate

We really want
\begin{displaymath}
\{ A \in \B^{m \times n} : A x \ne 0 \forall x \in X\}.
\end{displaymath}
The complement of that set is:
\begin{displaymath}
U(X) := \{ A \in \B^{m \times n} : \exists x \in X, A x = 0\} =
\bigcup_{x \in X} \{ A \in \B^{m \times n} : Ax = 0\}.
\end{displaymath}
[Wrong: Again a failure of quantifiers]

What we'd like to do is to score potential $x$.  We have a subset $B
\subseteq X$ of vectors which we already have in hand.  For each
possible $x$ we'd like to pick the most effective one to add.  For
each $x \in B$ there is the set $C(x) := \{ A : A x = 0\}$ of "bad"
matrices.  We really are interested in the complement of
$\bigcap_{x \in B} C(x)$.
We want to describe the set of $A$ which resolve *some*
element of $B$.  This will be $\bigcup_{x \in B} \overline{C(x)}$.  Its
complement is $\bigcap_{x \in B} C(x)$.

Note that $\overline{C(x)}$ is exactly the set of matrices that
resolve the vector $x$.  Since we want to resolve all vectors, we are
interested in the set $\bigcap_{x \in B} \overline{C(x)}$.  That is,
we want to know if there's a matrix that resolves all the vectors.
It is easier to deal with th complement of this set:
$H(B) := \bigcup_{x \in B} C(x)$.  If, for a given number of rows, $m$ we are
interested in showing that there is *no* such matrix, the heuristic
that we use is to try to make this set as big as possible.  We can try
to estimate the size of this set using inclusion/exclusion.  This
potentially works very well for this problem, since the indivdual
terms are of the form $|\{A : Ax = 0, \forall x \in S\}|$, for some
subset $S$.  Since this is a linear algebra problem, and we know that
all $x$ vectors lie in an $n-1$-dimensional space, we only need to
consider subsets, $S$, of cardinality $\le n-1$.

We're interested in $H(B) := \bigcap_{x \in B} \overline{C(x)}$.  This
will be the set of matrices which resolve some element in $B$.  When
potentially adding a new vector, $y$ to $B$ we want to choose the one
that cuts down the largest number.  We have

\begin{displaymath}
\overline{H(B)} = \cup_{x \in B} F(x).
\end{displaymath}

We can estimate this via inclusion/exclusion.  Note: there's something
a little strange: $Ax = 0$ if and only if every row of $A$ is
annihiliated by $x$.  Note that using linear algebra, the maximum
dimension of $\langle x \in S \rangle$ is $n-1$, where $S \subseteq
B$.

If we're looking to prove UNSAT, it makes sense to try to maxmize
this.  We can use inclusion-exclusion

\begin{displaymath}
\# U(X) := \sum_{\emptyset \ne S \subset X} (-1)^{|S| + 1} \#  \{ A \in \B^{m \times n} :  A x = 0, \forall x \in S\}.
\end{displaymath}

We first note, since the subsets $S$ impose a linear relation on the
$A$'s that we only need to consider subsets $S$ of cardinality $\le
n-1$ (it is $n-1$ since all $x \in X$ satisfy $e^T x = 0$, where $e$
is the all 1's vector).

Note that for a given subset $S$ of cardinality $n-1$ one can write a
closed form for the cardinality of the $A$'s that are orthogonal to
it.  It will be a sum of partition numbers.  If $S$ has exactly 1
element, this is easy.  Furthermore if one arranges the elements of
$S$ in a $|S| \times n$ array, and column/row permutation of that
array will yield the same dimension.

It is a bit inconvenient to calculate the above, so we calculate the
complement of this set:

\begin{displaymath}
\# \bigcap_{x \in X}\{ A \in \B^{m \times n} : A x = 0 \}.
\end{displaymath}

Let $V$ denote the real vector space spanned by the members of $X$.
The the above quantity is the same as

\begin{displaymath}
\# \bigcap_{v  \in V}\{ A \in \B^{m \times n} : A v = 0 \},
\end{displaymath}

which is the same as

\begin{displaymath}
\# \bigcap_{v  \in S}\{ A \in \B^{m \times n} : A v = 0 \},
\end{displaymath}

where $S$ is an set that spans $V$.  In particular, we may take $S$ to
be $\{x_1, \dots, x_m\}$, for some $x_i \in X$ and $m = \dim V$.  This
has the advantage of using linear algebra.  We only need to take a
linearly independent set of $x$.  Since the dimension of the possible
$A$'s is $mn$ at best this restricts $A$ to a subspace of dimension
$mn - n$.

\begin{displaymath}
\sum_{\emptyset \ne X \subset X} (-1)^{|S| + 1} # \{ A \in \B^{m \times n} : A x \ne 0 \}.
\end{displaymath}

* A suggestion from Ahmed Irfan
In the ping-pong method we deal with a problem of the form:
$\exists a \forall x f(a) \wedge g(x) \Rightarrow \phi(x,a)$.
The ping pong method splits this into two cooperating SAT solvers:
A) $\exists a f(a) \wedge \phi(x_1,a) \wedge \dots \wedge \phi(x_m,a)$, and
B) $\exists x g(x) \wedge \neg \phi(x,A)$, where $A$ is an assumption
generated by SAT solver (A), and $x_1, \dots, x_m$ are counterexamples
found by SAT solver (B) in previous iterations.

Ahmed suggested that, in (B) we try to find a core (a minimal
unsatisfiable set.  More specifically, if (B) is UNSAT, it finds a
subset of the assumptions, which, when set, makes the problem UNSAT,
and that subset is minimal with respect to inclusion), so we need to
transform problem (B) into an UNSAT problem.  It is not sufficient to
negate the clauses.  However, the following might work:  Run the SAT
solver for (B) to find a counterexamples, $x_1, \dots, x_r$, and
consider the problem:
$\exists x, \bigvee_i (x == x_i) \wedge \phi(x,A)$.
as clauses to the problem (B') $\exists x, g(x), \phi(x,A)$, where $A$
is the same assumption used in problem (B).  Or possibly, run a
version of problem (A) with $A$ as assumptions, after adding the
counterexamples, and then asking for a core.

* An alternative using Max Sat

An alternative to the above might be the following:

Given a putative matrix, A, set up the Max Sat problem, with hard
clauses $g(x) \Rightarrow \phi(x,A)$, and soft clauses (all with
weight 1) for all $i,j$: $a_{i,j} \ne A_{i,j}$.  That way the we are
just forbidding some subset of the $A$'s, and solver (A) doesn't
involve $x$ variables at all.  I don't think that that will work.qq
