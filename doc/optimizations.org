#+Title: Optimizations for the metric dimension of the hypercube
#+Latex_header: \newcommand{\B}{\{0,1\}}
* Translations
Let $V$ denote $\{0,1\}$, and $\oplus$ denote elementwise exclusive
or, $e$ denote the vector of dimension $n$ all whose coordinates
are 1.  If $v \in V^n$, denote by, $w(v)$ its Hamming weight and by
$R(v)$ the set of pairs of vectors resolved by $x$.  That is $R(v) =
\{ (x,y) : x \ne y, w(v \oplus x) != w(v \oplus y) \}$.
1) If $v \in V^n$, then $R(v \oplus e) = R(v)$.  Thus, without loss of
   generality, we may assume that any vector in the resolving set has
   weight $\le n/2$.
2) If $S$ is a resolving set, and $v \in V^n$, then $S \oplus v = \{u
   \oplus v : u \in S\}$ is also a resolving set.  Thus, without loss
   of generality we may assume that $0 \in S$.

So from now on, implictly assume that $0 \in S$ and only consider the
nonzero members.

From now one we write the nonzero elements of $S$ as an $m \times n$
matrix $A$ whose rows are the elements of $S$

Note that $0$ resolves all pairs of distinct vectors of different
weights.  So we may assume that the pairs to be considered have the
same weight.  A short calculation shows that $v \in V^n$ resolves
$(x,y)$ of the same weight if and only if $v \cdot (x-y) \ne 0$, where
that calculation is done over the reals.  Thus we only need to
consider vectors of the form $x-y$.  These will exactly be nonzero
vectors whose coordinates are in $\{0,1,-1\}$ and the sum of the all
the coordinates is 0.  We can represent such a vector as a pair of 0/1
vectors $(x,y)$ so that $x_i$ and $y_i$ can't be 1 at the same time,
and $w(x) = w(y)$.  There is an additional symmetry since $x-y$ and
$y-x$ both occur: the first nonzero element of $x-y$ should be 1.
Also resolving all  pairs $(x,y)$ with $w(x) = w(y) = 1$ is exactly
equivalent to all columns of the $A$ matrix being distinct.  We can
omit those pairs and require that $w(x) = w(y) \ge 2$.

Since $S$ is a set, we can permute any of the rows of $A$, so, a
symmetry break would be make sure that the rows strictly increase in
lexicographic order.  Additionally, since the set of pairs $(x,y)$ is
invariant under permuting the coordinates, and, as above, they must be
distinct, we can also add the constraint that the columns are
lexicographically stricltly increasing.

There are other symmetry breaking constraints that I found, but, for
some reason they seemed to slow things down.  I'll discuss them later.

* More symmetry breaking constraints

The set of pairs $(x,y)$ is invariant under permuting coordinates.
Therefore, so is the set possible set of $A$ matrices.  As before, the
all 0 vector may be assumed to be in the resolving set, so we don't
explicitly represent it.  Imagine building up the $A$ matrix one row
at a time.  Since we may permute the columns in any way, without loss
of generality we may assume that the first row has the form
$0^r 1^s$, where $r+s=n$.  That is the first $r$ entries are 0 and the
remaining $s$ are 1.  In filling out the second row, we are free to
permute the first $r$ columns among themselves, the the last $s$
columns among themseves.  Therefore, we may assume that the second row
has the form $0^t 1^u 0^v 1^w$, where $t+u=r$ and $v+w=s$.  In
general, by induction, when adding the next row we are free to permute
columns which have equal values.    We also will build them up so that
all equal columns will be adjacent to one another.  Accordingly, we
can define variables $e(r,i)$ meaning that the column consisting of
the first $r$ rows of column $i$ is equal to the column consisting of
the first $r$ rows of columns $i+1$.  These are defined (1-origin) for
$1 \le r \le m$ and $1 \le i \le n-1$.  They can be defined
inductively by $c(r,i) = c(r-1,i) & (a(r,i) == a(r,i+1))$.  Call the a
set of columns a "region" if all of those columns through row $r-1$
are equal.  We want to have the 1's in a region moved the "right".  We
can do this by the constraint $((a(r,i) & c(r-1,i)) => a(r,i+1))$.
Notice that, since the constraints implied by resolving pairs $(x,y)$
where $x$ and $y$ have weight 1 are exactly equivalent to the columns
of $a$ being distinct, that setting $c(m,i)$ to false for all $1 \le i
\le n-1$ will guarantee this.

The above should take care of the symmetries of the columns.  To take
care of the symmetries of the rows, I think that lexicographic
ordering is valid.  It also might be useful to have a hybrid ordering:
that the weights of the rows are nondecreasing, and ties are broken
lexicographically.  The latter are more involved.  I'm not sure which
will work better.

* Resolving vectors
Question: is there a good score that we can compute for the
counterexamples?

As part of the score calculation, we want to see how many matrices a
set of balanced vectors will rule out?  if $X$ is a set of resolving
vectors we want to calculate

We really want
\begin{displaymath}
\{ A \in \B^{m \times n} : A x \ne 0 \forall x \in X\}.
\end{displaymath}
The complement of that set is:
\begin{displaymath}
U(X) := \{ A \in \B^{m \times n} : \exists x \in X, A x = 0\} =
\bigcup_{x \in X} \{ A \in \B^{m \times n} : Ax = 0\}.
\end{displaymath}
[Wrong: Again a failure of quantifiers]

What we'd like to do is to score potential $x$.  We have a subset $B
\subseteq X$ of vectors which we already have in hand.  For each
possible $x$ we'd like to pick the most effective one to add.  For
each $x \in B$ there is the set $C(x) := \{ A : A x = 0\}$ of "bad"
matrices.  We really are interested in the complement of
$\bigcap_{x \in B} C(x)$.
We want to describe the set of $A$ which resolve *some*
element of $B$.  This will be $\bigcup_{x \in B} \overline{C(x)}$.  Its
complement is $\bigcap_{x \in B} C(x)$.

Note that $\overline{C(x)}$ is exactly the set of matrices that
resolve the vector $x$.  Since we want to resolve all vectors, we are
interested in the set $\bigcap_{x \in B} \overline{C(x)}$.  That is,
we want to know if there's a matrix that resolves all the vectors.
It is easier to deal with th complement of this set:
$H(B) := \bigcup_{x \in B} C(x)$.  If, for a given number of rows, $m$ we are
interested in showing that there is *no* such matrix, the heuristic
that we use is to try to make this set as big as possible.  We can try
to estimate the size of this set using inclusion/exclusion.  This
potentially works very well for this problem, since the indivdual
terms are of the form $|\{A : Ax = 0, \forall x \in S\}|$, for some
subset $S$.  Since this is a linear algebra problem, and we know that
all $x$ vectors lie in an $n-1$-dimensional space, we only need to
consider subsets, $S$, of cardinality $\le n-1$.

We're interested in $H(B) := \bigcap_{x \in B} \overline{C(x)}$.  This
will be the set of matrices which resolve some element in $B$.  When
potentially adding a new vector, $y$ to $B$ we want to choose the one
that cuts down the largest number.  We have

\begin{displaymath}
\overline{H(B)} = \cup_{x \in B} F(x).
\end{displaymath}

We can estimate this via inclusion/exclusion.  Note: there's something
a little strange: $Ax = 0$ if and only if every row of $A$ is
annihiliated by $x$.  Note that using linear algebra, the maximum
dimension of $\langle x \in S \rangle$ is $n-1$, where $S \subseteq
B$.

If we're looking to prove UNSAT, it makes sense to try to maxmize
this.  We can use inclusion-exclusion

\begin{displaymath}
\# U(X) := \sum_{\emptyset \ne S \subset X} (-1)^{|S| + 1} \#  \{ A \in \B^{m \times n} :  A x = 0, \forall x \in S\}.
\end{displaymath}

We first note, since the subsets $S$ impose a linear relation on the
$A$'s that we only need to consider subsets $S$ of cardinality $\le
n-1$ (it is $n-1$ since all $x \in X$ satisfy $e^T x = 0$, where $e$
is the all 1's vector).

Note that for a given subset $S$ of cardinality $n-1$ one can write a
closed form for the cardinality of the $A$'s that are orthogonal to
it.  It will be a sum of partition numbers.  If $S$ has exactly 1
element, this is easy.  Furthermore if one arranges the elements of
$S$ in a $|S| \times n$ array, and column/row permutation of that
array will yield the same dimension.

It is a bit inconvenient to calculate the above, so we calculate the
complement of this set:

\begin{displaymath}
\# \bigcap_{x \in X}\{ A \in \B^{m \times n} : A x = 0 \}.
\end{displaymath}

Let $V$ denote the real vector space spanned by the members of $X$.
The the above quantity is the same as

\begin{displaymath}
\# \bigcap_{v  \in V}\{ A \in \B^{m \times n} : A v = 0 \},
\end{displaymath}

which is the same as

\begin{displaymath}
\# \bigcap_{v  \in S}\{ A \in \B^{m \times n} : A v = 0 \},
\end{displaymath}

where $S$ is an set that spans $V$.  In particular, we may take $S$ to
be $\{x_1, \dots, x_m\}$, for some $x_i \in X$ and $m = \dim V$.  This
has the advantage of using linear algebra.  We only need to take a
linearly independent set of $x$.  Since the dimension of the possible
$A$'s is $mn$ at best this restricts $A$ to a subspace of dimension
$mn - n$.

\begin{displaymath}
\sum_{\emptyset \ne X \subset X} (-1)^{|S| + 1} # \{ A \in \B^{m \times n} : A x \ne 0 \}.
\end{displaymath}

* A suggestion from Ahmed Irfan
In the ping-pong method we deal with a problem of the form:
$\exists a \forall x f(a) \wedge g(x) \Rightarrow \phi(x,a)$.
The ping pong method splits this into two cooperating SAT solvers:
A) $\exists a f(a) \wedge \phi(x_1,a) \wedge \dots \wedge \phi(x_m,a)$, and
B) $\exists x g(x) \wedge \neg \phi(x,A)$, where $A$ is an assumption
generated by SAT solver (A), and $x_1, \dots, x_m$ are counterexamples
found by SAT solver (B) in previous iterations.

Ahmed suggested that, in (B) we try to find a core (a minimal
unsatisfiable set.  More specifically, if (B) is UNSAT, it finds a
subset of the assumptions, which, when set, makes the problem UNSAT,
and that subset is minimal with respect to inclusion), so we need to
transform problem (B) into an UNSAT problem.  It is not sufficient to
negate the clauses.  However, the following might work:  Run the SAT
solver for (B) to find a counterexamples, $x_1, \dots, x_r$, and
consider the problem:
$\exists x, \bigvee_i (x == x_i) \wedge \phi(x,A)$.
as clauses to the problem (B') $\exists x, g(x), \phi(x,A)$, where $A$
is the same assumption used in problem (B).  Or possibly, run a
version of problem (A) with $A$ as assumptions, after adding the
counterexamples, and then asking for a core.

* An alternative using Max Sat

An alternative to the above might be the following:

If $A$ is a putative resolving matrix which is resolving
then $\exists x, g(x) \& \phi(A,x)$ is UNSAT.  If it is not resolving,
we would like to find a maximal subset, $S$, of $i,j$ that can be assigned,
so that $g(x) \& \phi(a,x) \bigwedge_{(i,j) \in S} (a_{i,j} =
A_{i,j})$ is SAT.  This means that there an assignment of the
variables outside of $S$ for which there is a counterexample.  If it
is maximal (or maximum), adding one more assignment outside of $S$
makes it UNSAT.  That means that $S$ plus that element can be extended
to a resolving set.  So we can assume all the values in $S$, and
reduce it to a smaller problem.  We could also add counterexamples to
help the search along.

We can instantiate
indicator variables for each element of $A$.  That is, for each $i,j$
we have a variable $b_{i,j}$ and the hard clause
$b_{i,j} \Rightarrow (a_{i,j} = A_{i,j})$ (note that since $A_{i,j}$
is a constant, this is a 2-clause).  We want to maximize the the
number of $b_{i,j}$ which are true.  This will give us a subset of the
$i,j$ so that if we add one more assignment outside of that set, we
will get UNSAT.  So that gives us a collection of forbidden
assignments to $A$.

* A realization
I just realized why choosing the smallest weight $x$ vectors was a
good strategy.  The simple reason is that the support of the clauses
generated by these is the smallest.  For example, if the support is 4
(remember they're all even, and that support 2 are "baked into" the
symmetry breaking constraints), then the conflict in question will
forbid a bunch of settings in its support.  If the support is smaller,
in general, that will forbid more $A$'s.

It would be interesting to have the conflictor stop when the weight
goes up.

* Another question

I'm interested in finding "short" proofs that $A$ *is* a balanced
detecting matrix.  Instead of using a DRUP (or DRAT) proof, I wonder
if we can have a Farkas lemma proof.  Here are the details:

Given $A$, we want to show that there are *no* nonzero balanced $x$
such that $Ax = 0$.  We can define a polytope as follows:

$P$: $x,y \in \RR^n$ such that $0 \le x_i, y_i \le 1$ for
$i=1,\dots.n$, $x_i + y_i \le 1$, for $i=1, \dots, n$,
$\sum_i x_i = \sum_i y_i \ge 1$, and $A(x-y) = 0$.  The polytope $P$
is probably not empty, but we can try generating Gomory cuts.  The
question is: will the first level of  the Chvatal-Gomory hierarchy be
enough to make the polytope empty?

* Proofs
Here are the details of some proofs:

Definition: Let $G$ be a finite, simple, connected graph, and let
$d_G(x,y)$ denote the length of the shortest path betwee $x,y \in
V(G)$.  A subset $S \subseteq V(G)$ is *resolving* if for all $x \ne y
\in V(G)$, there is $z \in S$ such that $d_G(x,z) \ne d_G(y,z)$.
The cardinality of the smallest resolving set is called the *metric
dimension* of $G$.

Definition: Let $n$ be a positive integer.  The $n$-dimensional
hypercube $Q^n$ is the graph whose vertices are labeled with all $2^n$
$n$-tuples of 0/1 vectors.  There is an edge between vertices labeled
$x$ and $y$ if and only if $w(x \oplus y) = 1$, where $w$ is the
Hamming weight, and $\oplus$ indicates coordinatewise XOR.

Notation: We denote by $\beta_n$ the metric dimension of $Q^n$.

Definition: An $m \times n$ 0/1 matrix, $A$, is a *resolving matrix*
if, for all 0/1 $n$-vectors, $x \ne y$, there is an $i$ such that $A_i
\oplus x \ne A_i \oplus y$, where $a \oplus b$ is obtained by XORing
each coordinate.  If an $m \times n$ resolving matrix exists, we say
that $m$ is resolving for $n$.

Remark: We have $b_n \le m$ if and only if $m$ is resolving for $n$.

Lemma: If $A$ is a resolving matrix, and $b$ is a 0/1 vectors, then
the matrix $B$ is resolving, where $B_i = b \oplus A_i$ for all $i$.

Proof: Note that if $x,y, b$ are 0/1 vectors, then $x \ne y$ if and
only if $x \oplus b \ne y \oplus b$.   Thus $B_i \oplus x \ne B_i
\oplus y$ if and only if $A_i \oplus x \ne A_i \oplus y$.

Corollary: We have $m$ is resolving for $n$ if and only if there is a
resolving matrix $B$ in which $B_1 = 0$.

Proof:  Let $A$ be a resolving matrix which is $m \times n$.  Let $b =
A_1$.  Then $B$ has the form indicated.  The converse is trivial.

Definition: An $m \times n$ 0/1 matrix, $A$, is *balanced detecting*
if $Ax = 0 \Rightarrow x = 0$, for $x$ a 0/1/-1 vector with $\sum_i
x_i = 0$.

Proposition: We have $m$ is resolving for $n$ if and only if there is
an $(m-1) \times n$ balanced detecting matrix $A$.

Proof: We have $m$ is resolving for $n$, by the corollary, if and only
if there is an $m \times n$ resolving matrix $B$, with $B_1 = 0$.  If
$w(x) \ne w(y)$, then $w(B_1 \oplus ) \ne w(B_1 \oplus y)$.  In order
for $B$ to be resolving it is necessary and sufficient that for all $x
\ne y$ with $w(x) = w(y)$, there is an $i > 1$ such that $w(B_i \oplus
x) \ne w(B_i \oplus y)$.  Note that
$w(B_i \oplus x) = \sum_j B_{i,j} + x_j - 2 B_{i,j} x_j = w(B_i) +
w(x) - 2 \sum_j B_{i,j} x_j$ and similarly
for $w(B_i \oplus y)$.  Thus $w(B_i \oplus x) - w(B_i \oplus y) =
w(x) - w(y) - 2 \sum_j B_{i,j} (x_j - y_j)$  If $w(x) = w(y)$ this
quantity if nonzero for some $i > 1$ if and only if $B(x-y) \ne 0$.

Lemma: An $(m-1) \times n$ matrix $B$ is balanced detecting if and
only if $P B S$ is balanced detecting, where $P$ is an $(m-1)\times
(m-1)$ permutation matrix, and $S$ is an $n \times n$ permutation
matrix.

Proof:  First note that if $B$ is a 0/1 matrix then so is $P B S$.
Second $P B S x \ne 0$ if and only if $B S x \ne 0$ since $P$ is
invertible. Third, $Sx$ is a balanced 0/1/-1 vector if and only if $x$
is the same.

Lemma: If $B$ is a balanced detecting matrix, then so is $C$ where
$C_1 = B_1$, and $C_i = B_i \oplus B_1$ for $i > 1$.

Proof:  If $i > 1$ we have $C_{i,j} = B_{1,j} + B_{i,j} - 2 B_{1,j}
B_{i,j}$.  Let $x$ be a balanced 0/1/-1 vector.
So $C_i \cdot x = \sum_j B_{1,j} x_j + B_{i,j} x_j - 2
B_{1,j} B_{i,j} x_j$.   If $B_1
\cdot x \ne 0$ there is nothing to prove.  If it is not, then
$B_1 \cdot x = 0$.  Let $i > 1$ satisfy $B_i \cdot (B_1 \oplus x) \ne 0$.
Then $C_i \cdot (B_1 \oplus x) = \sum_j B_{i,j} (B_1 \oplus x)_j - 2  \sum_j B_{1,j}  B_{i,j}
(B_1 \oplus x)_j$.  Since $(B_1 \oplus x)_j = B_{1,j} + x_j - 2
B_{1,j} x_j$, we may substitute:
$C_i \cdot (B_1 \oplus x) = \sum_j B_{i,j} B_{1,j} + \sum_j B_{i,j}
x_j - 2 \sum_j B_{1,j} B_{i,j} x_j - 2 \sum_j B_{1,j}^2 B_{i,j} - 2
\sum_j B_{1,j} B_{i,j} x_j + 4 \sum_j B_{1,j}^2 B_{i,j} x_j$.
However, since $B_{1,j} \in \{0,1\}$ we have $B_{1,j}^2 = B_{1,j}$.

$(B_1 \oplus x)_j = B_{1,j} + x_j - 2 B_{1,j} x_j$.  Thus
$2 B_{1,j} B_{i,j} x_j = B_{i,j} (B_{1,j} + x_j - (B_1 \oplus x)_j)$.
So $2 \sum_j B_{1,j} B_{i,j} x_j = \sum_j B_{i,j} B_{1,j} + \sum_j
B_{i,j} x_j - \sum_j B_{i,j} (B_1 \oplus x)_j$.  Thus
$C_i \cdot

Definition: An $m \times n$ 0/1 matrix $A$ is *detecting* if and only
if $Ax \ne 0$ for all 0/1/-1 nonzero vectors $x$.  When an $m \times
n$ detecting matrix exists we say that $m$ is feasible for $n$.
Denote by $\gamma_n$ the smallest $m$ which is feasible for $n$.

Lemma: We have $\beta_n - 1 \le \gamma_n \le beta_n$.

Proof: For $\gamma_n \le \beta_n$, note that if $B$ is an $(m-1)
\times n$ balanced detecting matrix, then the $m \times n$ matrix $A$
with $A_1 = e$ (the all 1's vector), and $A_{i+1} = B_i$ for $i=1,
\dots, m-1$ is a detecting matrix.   For $\beta_n \le \gamma_n + 1$,
note that if $A$ is a detecting matrix, then it is also a balanced
detecting matrix, since the latter is a weaker condition.

* <2023-08-21 Mon> UNSAT proofs
For general SAT solving the "standard" way of providing a proof of
UNSAT is either the DRUP or DRAT proof, both of which can be produced
mechanically in many solvers.  What I'd like to do is to produce
fairly compact proofs that a particular 0/1 matrix is acutally a
detecting or balanced detecting, without having to list the
constraints explicitly.  The idea is that if $A$ is an $m \times n$
matrix we want to show that $Ax \ne 0$ for all the $x$ vectors of the
desired form.  For detecting, that set will be $x_i \in \{0,\pm 1\}$,
and $x \ne 0$.  For balanced detecting we have the condition $\sum_i
x_i = 0$.

We can describe the feasible $x$ as 0/1 points in a polytope.  In
particular, we write $x = y - z$ for $0/1$ vectors $y,z$.  We have the
additional constraints $y_i + z_i \le 1$, and $\sum_i x_i \ge 1,
\sum_i y_i \ge 1$.  For balanced, we add the constraint $\sum_i (x_i -
y_i) = 0$.  Call the polytope in question, $P$.  Ideally, we would
like to have the polytope being empty.  If that were the case, then we
can generated a proof of this via the Farkas lemma.  We usually won't
be so lucky.  However, what we can do, is to make use of
Chvatal-Gomory cuts.  It is known that a finite number of them suffice
to show that that there are no integer points in the polytope.  A
simpler set of cuts to generate are the $0$-$\frac 12$ cuts.  A paper
of Koster, Zymolka and Kutschka indicates a practical method to find
them.

An alternative would be to use lattice reduction.  In particular, one
could use the Maximum Entropy method to find a distribution with which
to rescale, to make an ellipsoidal approximation.  This converts it to
a CVP problem, which, with lattice reduction one can given a fairly
short proof of emptiness.

Without looking at the details of the Koster et. al. paper, if we have
the polytope specified by $A x \le b, Cx = d, x \ge 0$, we can find
matrices $B,D$ with all elements of $B$ $\ge 0$, and consider $(B A) +
\lambda (DC) \le B b + \lambda D d$ for some $\lambda$.  In our case
all entries of $B$ and $C$ are in $\{0,1\}$.  We'd like $B,D$ to have
the property that $BA + DC$ is integral and $B A + DC \equiv 0 \bmod
2$, but $Bb + D d \not \equiv 0 \bmod 2$. In that case we will get a
cutting plane, since we can divide the left hand side by 2, and remain
integral, but the right hand side will not.  Can we take all entries
of $C$ to be in $\{0,1\}$?

* A BDD for the conflicts
Recall that a conflict is a pair of 0/1 n-vectors: $(x,y)$ such that
a) $\sum_i x_i = \sum_i y_i$, and $x_i + y_i \le 1$ (alternatively, as
boolean variables and CNF clause $\neg x_i \ve \neg y_i$.

We may descibe the pair $(x,y)$ by a BDD as follows:
The variable order is $x_1, y_1, x_2, y_2, x_3, y_3, \dots$.
Each node will have a state of a pair of counters with values in
$[0,\lfloor n/2 \rfloor]$.  In addition, the $y$ nodes will have the
value of the previous $x$ node.  We can optimize the counters, since
variables at level $\ell$ can only have counters in $[0, \ell - 1]$.
If we wish to symmetry break, by requiring that the first non-zero
among the $(x_i,y_i)$ occurs for $x$, we keep track of whether or not
all preceeding values are 0.  If we are at an $x$ node, this give no
restriction.  If we are at a $y$ node, this forces the $y$ value to
be 0.  Actually we can optimize things a bit more.  We only need to
keep a counter which is the difference between the $x$ counter and the
$y$ counter.  This will then be in the range $[-\lfloor n/2 \rfloor,
\lfloor n/2 \rfloor]$.  We have an initial state:
$(0,0)$ with transitions
$(0,0): (s,t) = (s, t)$
$(1,0): (s,t) = (1, t+1)$
$(0,1): (s,t) = \bot$ if $s=0$ else $(s, t-1)$
$(1,1): (s,t) = \bot$.
The final state is $(1,0)$.

But this doesn't immediately help, as it still an *exists* statement.



