#+Title: Optimizations for the metric dimension of the hypercube
#+Latex_header: \newcommand{\B}{\{0,1\}}
* Translations
Let $V$ denote $\{0,1\}$, and $\oplus$ denote elementwise exclusive
or, $e$ denote the vector of dimension $n$ all whose coordinates
are 1.  If $v \in V^n$, denote by, $w(v)$ its Hamming weight and by
$R(v)$ the set of pairs of vectors resolved by $x$.  That is $R(v) =
\{ (x,y) : x \ne y, w(v \oplus x) != w(v \oplus y) \}$.
1) If $v \in V^n$, then $R(v \oplus e) = R(v)$.  Thus, without loss of
   generality, we may assume that any vector in the resolving set has
   weight $\le n/2$.
2) If $S$ is a resolving set, and $v \in V^n$, then $S \oplus v = \{u
   \oplus v : u \in S\}$ is also a resolving set.  Thus, without loss
   of generality we may assume that $0 \in S$.

So from now on, implictly assume that $0 \in S$ and only consider the
nonzero members.

From now one we write the nonzero elements of $S$ as an $m \times n$
matrix $A$ whose rows are the elements of $S$

Note that $0$ resolves all pairs of distinct vectors of different
weights.  So we may assume that the pairs to be considered have the
same weight.  A short calculation shows that $v \in V^n$ resolves
$(x,y)$ of the same weight if and only if $v \cdot (x-y) \ne 0$, where
that calculation is done over the reals.  Thus we only need to
consider vectors of the form $x-y$.  These will exactly be nonzero
vectors whose coordinates are in $\{0,1,-1\}$ and the sum of the all
the coordinates is 0.  We can represent such a vector as a pair of 0/1
vectors $(x,y)$ so that $x_i$ and $y_i$ can't be 1 at the same time,
and $w(x) = w(y)$.  There is an additional symmetry since $x-y$ and
$y-x$ both occur: the first nonzero element of $x-y$ should be 1.
Also resolving all  pairs $(x,y)$ with $w(x) = w(y) = 1$ is exactly
equivalent to all columns of the $A$ matrix being distinct.  We can
omit those pairs and require that $w(x) = w(y) \ge 2$.

Since $S$ is a set, we can permute any of the rows of $A$, so, a
symmetry break would be make sure that the rows strictly increase in
lexicographic order.  Additionally, since the set of pairs $(x,y)$ is
invariant under permuting the coordinates, and, as above, they must be
distinct, we can also add the constraint that the columns are
lexicographically stricltly increasing.

There are other symmetry breaking constraints that I found, but, for
some reason they seemed to slow things down.  I'll discuss them later.

* More symmetry breaking constraints

The set of pairs $(x,y)$ is invariant under permuting coordinates.
Therefore, so is the set possible set of $A$ matrices.  As before, the
all 0 vector may be assumed to be in the resolving set, so we don't
explicitly represent it.  Imagine building up the $A$ matrix one row
at a time.  Since we may permute the columns in any way, without loss
of generality we may assume that the first row has the form
$0^r 1^s$, where $r+s=n$.  That is the first $r$ entries are 0 and the
remaining $s$ are 1.  In filling out the second row, we are free to
permute the first $r$ columns among themselves, the the last $s$
columns among themseves.  Therefore, we may assume that the second row
has the form $0^t 1^u 0^v 1^w$, where $t+u=r$ and $v+w=s$.  In
general, by induction, when adding the next row we are free to permute
columns which have equal values.    We also will build them up so that
all equal columns will be adjacent to one another.  Accordingly, we
can define variables $e(r,i)$ meaning that the column consisting of
the first $r$ rows of column $i$ is equal to the column consisting of
the first $r$ rows of columns $i+1$.  These are defined (1-origin) for
$1 \le r \le m$ and $1 \le i \le n-1$.  They can be defined
inductively by $c(r,i) = c(r-1,i) & (a(r,i) == a(r,i+1))$.  Call the a
set of columns a "region" if all of those columns through row $r-1$
are equal.  We want to have the 1's in a region moved the "right".  We
can do this by the constraint $((a(r,i) & c(r-1,i)) => a(r,i+1))$.
Notice that, since the constraints implied by resolving pairs $(x,y)$
where $x$ and $y$ have weight 1 are exactly equivalent to the columns
of $a$ being distinct, that setting $c(m,i)$ to false for all $1 \le i
\le n-1$ will guarantee this.

The above should take care of the symmetries of the columns.  To take
care of the symmetries of the rows, I think that lexicographic
ordering is valid.  It also might be useful to have a hybrid ordering:
that the weights of the rows are nondecreasing, and ties are broken
lexicographically.  The latter are more involved.  I'm not sure which
will work better.

* Resolving vectors
Question: is there a good score that we can compute for the
counterexamples?

As part of the score calculation, we want to see how many matrices a
set of balanced vectors will rule out?  if $X$ is a set of resolving
vectors we want to calculate

We really want
\begin{displaymath}
\{ A \in \B^{m \times n} : A x \ne 0 \forall x \in X\}.
\end{displaymath}
The complement of that set is:
\begin{displaymath}
U(X) := \{ A \in \B^{m \times n} : \exists x \in X, A x = 0\} =
\bigcup_{x \in X} \{ A \in \B^{m \times n} : Ax = 0\}.
\end{displaymath}
[Wrong: Again a failure of quantifiers]

What we'd like to do is to score potential $x$.  We have a subset $B
\subseteq X$ of vectors which we already have in hand.  For each
possible $x$ we'd like to pick the most effective one to add.  For
each $x \in B$ there is the set $C(x) := \{ A : A x = 0\}$ of "bad"
matrices.  We really are interested in the complement of
$\bigcap_{x \in B} C(x)$.
We want to describe the set of $A$ which resolve *some*
element of $B$.  This will be $\bigcup_{x \in B} \overline{C(x)}$.  Its
complement is $\bigcap_{x \in B} C(x)$.

Note that $\overline{C(x)}$ is exactly the set of matrices that
resolve the vector $x$.  Since we want to resolve all vectors, we are
interested in the set $\bigcap_{x \in B} \overline{C(x)}$.  That is,
we want to know if there's a matrix that resolves all the vectors.
It is easier to deal with th complement of this set:
$H(B) := \bigcup_{x \in B} C(x)$.  If, for a given number of rows, $m$ we are
interested in showing that there is *no* such matrix, the heuristic
that we use is to try to make this set as big as possible.  We can try
to estimate the size of this set using inclusion/exclusion.  This
potentially works very well for this problem, since the indivdual
terms are of the form $|\{A : Ax = 0, \forall x \in S\}|$, for some
subset $S$.  Since this is a linear algebra problem, and we know that
all $x$ vectors lie in an $n-1$-dimensional space, we only need to
consider subsets, $S$, of cardinality $\le n-1$.

We're interested in $H(B) := \bigcap_{x \in B} \overline{C(x)}$.  This
will be the set of matrices which resolve some element in $B$.  When
potentially adding a new vector, $y$ to $B$ we want to choose the one
that cuts down the largest number.  We have

\begin{displaymath}
\overline{H(B)} = \cup_{x \in B} F(x).
\end{displaymath}

We can estimate this via inclusion/exclusion.  Note: there's something
a little strange: $Ax = 0$ if and only if every row of $A$ is
annihiliated by $x$.  Note that using linear algebra, the maximum
dimension of $\langle x \in S \rangle$ is $n-1$, where $S \subseteq
B$.

If we're looking to prove UNSAT, it makes sense to try to maxmize
this.  We can use inclusion-exclusion

\begin{displaymath}
\# U(X) := \sum_{\emptyset \ne S \subset X} (-1)^{|S| + 1} \#  \{ A \in \B^{m \times n} :  A x = 0, \forall x \in S\}.
\end{displaymath}

We first note, since the subsets $S$ impose a linear relation on the
$A$'s that we only need to consider subsets $S$ of cardinality $\le
n-1$ (it is $n-1$ since all $x \in X$ satisfy $e^T x = 0$, where $e$
is the all 1's vector).

Note that for a given subset $S$ of cardinality $n-1$ one can write a
closed form for the cardinality of the $A$'s that are orthogonal to
it.  It will be a sum of partition numbers.  If $S$ has exactly 1
element, this is easy.  Furthermore if one arranges the elements of
$S$ in a $|S| \times n$ array, and column/row permutation of that
array will yield the same dimension.

It is a bit inconvenient to calculate the above, so we calculate the
complement of this set:

\begin{displaymath}
\# \bigcap_{x \in X}\{ A \in \B^{m \times n} : A x = 0 \}.
\end{displaymath}

Let $V$ denote the real vector space spanned by the members of $X$.
The the above quantity is the same as

\begin{displaymath}
\# \bigcap_{v  \in V}\{ A \in \B^{m \times n} : A v = 0 \},
\end{displaymath}

which is the same as

\begin{displaymath}
\# \bigcap_{v  \in S}\{ A \in \B^{m \times n} : A v = 0 \},
\end{displaymath}

where $S$ is an set that spans $V$.  In particular, we may take $S$ to
be $\{x_1, \dots, x_m\}$, for some $x_i \in X$ and $m = \dim V$.  This
has the advantage of using linear algebra.  We only need to take a
linearly independent set of $x$.  Since the dimension of the possible
$A$'s is $mn$ at best this restricts $A$ to a subspace of dimension
$mn - n$.

\begin{displaymath}
\sum_{\emptyset \ne X \subset X} (-1)^{|S| + 1} # \{ A \in \B^{m \times n} : A x \ne 0 \}.
\end{displaymath}

* A suggestion from Ahmed Irfan
In the ping-pong method we deal with a problem of the form:
$\exists a \forall x f(a) \wedge g(x) \Rightarrow \phi(x,a)$.
The ping pong method splits this into two cooperating SAT solvers:
A) $\exists a f(a) \wedge \phi(x_1,a) \wedge \dots \wedge \phi(x_m,a)$, and
B) $\exists x g(x) \wedge \neg \phi(x,A)$, where $A$ is an assumption
generated by SAT solver (A), and $x_1, \dots, x_m$ are counterexamples
found by SAT solver (B) in previous iterations.

Ahmed suggested that, in (B) we try to find a core (a minimal
unsatisfiable set.  More specifically, if (B) is UNSAT, it finds a
subset of the assumptions, which, when set, makes the problem UNSAT,
and that subset is minimal with respect to inclusion), so we need to
transform problem (B) into an UNSAT problem.  It is not sufficient to
negate the clauses.  However, the following might work:  Run the SAT
solver for (B) to find a counterexamples, $x_1, \dots, x_r$, and
consider the problem:
$\exists x, \bigvee_i (x == x_i) \wedge \phi(x,A)$.
as clauses to the problem (B') $\exists x, g(x), \phi(x,A)$, where $A$
is the same assumption used in problem (B).  Or possibly, run a
version of problem (A) with $A$ as assumptions, after adding the
counterexamples, and then asking for a core.

* An alternative using Max Sat

An alternative to the above might be the following:

If $A$ is a putative resolving matrix which is resolving
then $\exists x, g(x) \& \phi(A,x)$ is UNSAT.  If it is not resolving,
we would like to find a maximal subset, $S$, of $i,j$ that can be assigned,
so that $g(x) \& \phi(a,x) \bigwedge_{(i,j) \in S} (a_{i,j} =
A_{i,j})$ is SAT.  This means that there an assignment of the
variables outside of $S$ for which there is a counterexample.  If it
is maximal (or maximum), adding one more assignment outside of $S$
makes it UNSAT.  That means that $S$ plus that element can be extended
to a resolving set.  So we can assume all the values in $S$, and
reduce it to a smaller problem.  We could also add counterexamples to
help the search along.

We can instantiate
indicator variables for each element of $A$.  That is, for each $i,j$
we have a variable $b_{i,j}$ and the hard clause
$b_{i,j} \Rightarrow (a_{i,j} = A_{i,j})$ (note that since $A_{i,j}$
is a constant, this is a 2-clause).  We want to maximize the the
number of $b_{i,j}$ which are true.  This will give us a subset of the
$i,j$ so that if we add one more assignment outside of that set, we
will get UNSAT.  So that gives us a collection of forbidden
assignments to $A$.

* A realization
I just realized why choosing the smallest weight $x$ vectors was a
good strategy.  The simple reason is that the support of the clauses
generated by these is the smallest.  For example, if the support is 4
(remember they're all even, and that support 2 are "baked into" the
symmetry breaking constraints), then the conflict in question will
forbid a bunch of settings in its support.  If the support is smaller,
in general, that will forbid more $A$'s.

It would be interesting to have the conflictor stop when the weight
goes up.

* Another question

I'm interested in finding "short" proofs that $A$ *is* a balanced
detecting matrix.  Instead of using a DRUP (or DRAT) proof, I wonder
if we can have a Farkas lemma proof.  Here are the details:

Given $A$, we want to show that there are *no* nonzero balanced $x$
such that $Ax = 0$.  We can define a polytope as follows:

$P$: $x,y \in \RR^n$ such that $0 \le x_i, y_i \le 1$ for
$i=1,\dots.n$, $x_i + y_i \le 1$, for $i=1, \dots, n$,
$\sum_i x_i = \sum_i y_i \ge 1$, and $A(x-y) = 0$.  The polytope $P$
is probably not empty, but we can try generating Gomory cuts.  The
question is: will the first level of  the Chvatal-Gomory hierarchy be
enough to make the polytope empty?

* Proofs
Here are the details of some proofs:

Definition: Let $G$ be a finite, simple, connected graph, and let
$d_G(x,y)$ denote the length of the shortest path betwee $x,y \in
V(G)$.  A subset $S \subseteq V(G)$ is *resolving* if for all $x \ne y
\in V(G)$, there is $z \in S$ such that $d_G(x,z) \ne d_G(y,z)$.
The cardinality of the smallest resolving set is called the *metric
dimension* of $G$.

Definition: Let $n$ be a positive integer.  The $n$-dimensional
hypercube $Q^n$ is the graph whose vertices are labeled with all $2^n$
$n$-tuples of 0/1 vectors.  There is an edge between vertices labeled
$x$ and $y$ if and only if $w(x \oplus y) = 1$, where $w$ is the
Hamming weight, and $\oplus$ indicates coordinatewise XOR.

Notation: We denote by $\beta_n$ the metric dimension of $Q^n$.

Definition: An $m \times n$ 0/1 matrix, $A$, is a *resolving matrix*
if, for all 0/1 $n$-vectors, $x \ne y$, there is an $i$ such that $A_i
\oplus x \ne A_i \oplus y$, where $a \oplus b$ is obtained by XORing
each coordinate.  If an $m \times n$ resolving matrix exists, we say
that $m$ is resolving for $n$.

Remark: We have $b_n \le m$ if and only if $m$ is resolving for $n$.

Lemma: If $A$ is a resolving matrix, and $b$ is a 0/1 vectors, then
the matrix $B$ is resolving, where $B_i = b \oplus A_i$ for all $i$.

Proof: Note that if $x,y, b$ are 0/1 vectors, then $x \ne y$ if and
only if $x \oplus b \ne y \oplus b$.   Thus $B_i \oplus x \ne B_i
\oplus y$ if and only if $A_i \oplus x \ne A_i \oplus y$.

Corollary: We have $m$ is resolving for $n$ if and only if there is a
resolving matrix $B$ in which $B_1 = 0$.

Proof:  Let $A$ be a resolving matrix which is $m \times n$.  Let $b =
A_1$.  Then $B$ has the form indicated.  The converse is trivial.

Definition: An $m \times n$ 0/1 matrix, $A$, is *balanced detecting*
if $Ax = 0 \Rightarrow x = 0$, for $x$ a 0/1/-1 vector with $\sum_i
x_i = 0$.

Proposition: We have $m$ is resolving for $n$ if and only if there is
an $(m-1) \times n$ balanced detecting matrix $A$.

Proof: We have $m$ is resolving for $n$, by the corollary, if and only
if there is an $m \times n$ resolving matrix $B$, with $B_1 = 0$.  If
$w(x) \ne w(y)$, then $w(B_1 \oplus ) \ne w(B_1 \oplus y)$.  In order
for $B$ to be resolving it is necessary and sufficient that for all $x
\ne y$ with $w(x) = w(y)$, there is an $i > 1$ such that $w(B_i \oplus
x) \ne w(B_i \oplus y)$.  Note that
$w(B_i \oplus x) = \sum_j B_{i,j} + x_j - 2 B_{i,j} x_j = w(B_i) +
w(x) - 2 \sum_j B_{i,j} x_j$ and similarly
for $w(B_i \oplus y)$.  Thus $w(B_i \oplus x) - w(B_i \oplus y) =
w(x) - w(y) - 2 \sum_j B_{i,j} (x_j - y_j)$  If $w(x) = w(y)$ this
quantity if nonzero for some $i > 1$ if and only if $B(x-y) \ne 0$.

Lemma: An $(m-1) \times n$ matrix $B$ is balanced detecting if and
only if $P B S$ is balanced detecting, where $P$ is an $(m-1)\times
(m-1)$ permutation matrix, and $S$ is an $n \times n$ permutation
matrix.

Proof:  First note that if $B$ is a 0/1 matrix then so is $P B S$.
Second $P B S x \ne 0$ if and only if $B S x \ne 0$ since $P$ is
invertible. Third, $Sx$ is a balanced 0/1/-1 vector if and only if $x$
is the same.

Lemma: If $B$ is a balanced detecting matrix, then so is $C$ where
$C_1 = B_1$, and $C_i = B_i \oplus B_1$ for $i > 1$.

Proof:  If $i > 1$ we have $C_{i,j} = B_{1,j} + B_{i,j} - 2 B_{1,j}
B_{i,j}$.  Let $x$ be a balanced 0/1/-1 vector.
So $C_i \cdot x = \sum_j B_{1,j} x_j + B_{i,j} x_j - 2
B_{1,j} B_{i,j} x_j$.   If $B_1
\cdot x \ne 0$ there is nothing to prove.  If it is not, then
$B_1 \cdot x = 0$.  Let $i > 1$ satisfy $B_i \cdot (B_1 \oplus x) \ne 0$.
Then $C_i \cdot (B_1 \oplus x) = \sum_j B_{i,j} (B_1 \oplus x)_j - 2  \sum_j B_{1,j}  B_{i,j}
(B_1 \oplus x)_j$.  Since $(B_1 \oplus x)_j = B_{1,j} + x_j - 2
B_{1,j} x_j$, we may substitute:
$C_i \cdot (B_1 \oplus x) = \sum_j B_{i,j} B_{1,j} + \sum_j B_{i,j}
x_j - 2 \sum_j B_{1,j} B_{i,j} x_j - 2 \sum_j B_{1,j}^2 B_{i,j} - 2
\sum_j B_{1,j} B_{i,j} x_j + 4 \sum_j B_{1,j}^2 B_{i,j} x_j$.
However, since $B_{1,j} \in \{0,1\}$ we have $B_{1,j}^2 = B_{1,j}$.

$(B_1 \oplus x)_j = B_{1,j} + x_j - 2 B_{1,j} x_j$.  Thus
$2 B_{1,j} B_{i,j} x_j = B_{i,j} (B_{1,j} + x_j - (B_1 \oplus x)_j)$.
So $2 \sum_j B_{1,j} B_{i,j} x_j = \sum_j B_{i,j} B_{1,j} + \sum_j
B_{i,j} x_j - \sum_j B_{i,j} (B_1 \oplus x)_j$.  Thus
$C_i \cdot

Definition: An $m \times n$ 0/1 matrix $A$ is *detecting* if and only
if $Ax \ne 0$ for all 0/1/-1 nonzero vectors $x$.  When an $m \times
n$ detecting matrix exists we say that $m$ is feasible for $n$.
Denote by $\gamma_n$ the smallest $m$ which is feasible for $n$.

Lemma: We have $\beta_n - 1 \le \gamma_n \le beta_n$.

Proof: For $\gamma_n \le \beta_n$, note that if $B$ is an $(m-1)
\times n$ balanced detecting matrix, then the $m \times n$ matrix $A$
with $A_1 = e$ (the all 1's vector), and $A_{i+1} = B_i$ for $i=1,
\dots, m-1$ is a detecting matrix.   For $\beta_n \le \gamma_n + 1$,
note that if $A$ is a detecting matrix, then it is also a balanced
detecting matrix, since the latter is a weaker condition.

* <2023-08-21 Mon> UNSAT proofs
For general SAT solving the "standard" way of providing a proof of
UNSAT is either the DRUP or DRAT proof, both of which can be produced
mechanically in many solvers.  What I'd like to do is to produce
fairly compact proofs that a particular 0/1 matrix is acutally a
detecting or balanced detecting, without having to list the
constraints explicitly.  The idea is that if $A$ is an $m \times n$
matrix we want to show that $Ax \ne 0$ for all the $x$ vectors of the
desired form.  For detecting, that set will be $x_i \in \{0,\pm 1\}$,
and $x \ne 0$.  For balanced detecting we have the condition $\sum_i
x_i = 0$.

We can describe the feasible $x$ as 0/1 points in a polytope.  In
particular, we write $x = y - z$ for $0/1$ vectors $y,z$.  We have the
additional constraints $y_i + z_i \le 1$, and $\sum_i x_i \ge 1,
\sum_i y_i \ge 1$.  For balanced, we add the constraint $\sum_i (x_i -
y_i) = 0$.  Call the polytope in question, $P$.  Ideally, we would
like to have the polytope being empty.  If that were the case, then we
can generated a proof of this via the Farkas lemma.  We usually won't
be so lucky.  However, what we can do, is to make use of
Chvatal-Gomory cuts.  It is known that a finite number of them suffice
to show that that there are no integer points in the polytope.  A
simpler set of cuts to generate are the $0$-$\frac 12$ cuts.  A paper
of Koster, Zymolka and Kutschka indicates a practical method to find
them.

An alternative would be to use lattice reduction.  In particular, one
could use the Maximum Entropy method to find a distribution with which
to rescale, to make an ellipsoidal approximation.  This converts it to
a CVP problem, which, with lattice reduction one can given a fairly
short proof of emptiness.

Without looking at the details of the Koster et. al. paper, if we have
the polytope specified by $A x \le b, Cx = d, x \ge 0$, we can find
matrices $B,D$ with all elements of $B$ $\ge 0$, and consider $(B A) +
\lambda (DC) \le B b + \lambda D d$ for some $\lambda$.  In our case
all entries of $B$ and $C$ are in $\{0,1\}$.  We'd like $B,D$ to have
the property that $BA + DC$ is integral and $B A + DC \equiv 0 \bmod
2$, but $Bb + D d \not \equiv 0 \bmod 2$. In that case we will get a
cutting plane, since we can divide the left hand side by 2, and remain
integral, but the right hand side will not.  Can we take all entries
of $C$ to be in $\{0,1\}$?

* A BDD for the conflicts
Recall that a conflict is a pair of 0/1 n-vectors: $(x,y)$ such that
a) $\sum_i x_i = \sum_i y_i$, and $x_i + y_i \le 1$ (alternatively, as
boolean variables and CNF clause $\neg x_i \ve \neg y_i$.

We may descibe the pair $(x,y)$ by a BDD as follows:
The variable order is $x_1, y_1, x_2, y_2, x_3, y_3, \dots$.
Each node will have a state of a pair of counters with values in
$[0,\lfloor n/2 \rfloor]$.  In addition, the $y$ nodes will have the
value of the previous $x$ node.  We can optimize the counters, since
variables at level $\ell$ can only have counters in $[0, \ell - 1]$.
If we wish to symmetry break, by requiring that the first non-zero
among the $(x_i,y_i)$ occurs for $x$, we keep track of whether or not
all preceeding values are 0.  If we are at an $x$ node, this give no
restriction.  If we are at a $y$ node, this forces the $y$ value to
be 0.  Actually we can optimize things a bit more.  We only need to
keep a counter which is the difference between the $x$ counter and the
$y$ counter.  This will then be in the range $[-\lfloor n/2 \rfloor,
\lfloor n/2 \rfloor]$.  We have an initial state:
$(0,0)$ with transitions
$(0,0): (s,t) = (s, t)$
$(1,0): (s,t) = (1, t+1)$
$(0,1): (s,t) = \bot$ if $s=0$ else $(s, t-1)$
$(1,1): (s,t) = \bot$.
The final state is $(1,0)$.

But this doesn't immediately help, as it still an *exists* statement.

** Double sorting of matrices
There appears to be the following theorem:
Let $M$ be an $m \times n$ matrix with entries in a totally ordered set
$S$.  There are two operations: $R$: permute the rows of $M$ so that
they are in nondecreasing lexicographic order, and $C$, doing the same
with columns.  Then
Theorem: The operations $RC$ has a finite iteration fixed point.  That
is, there is an integer $r$ such that $(RC)^r (M) = (RC)^{r+1}(M)$.

For the case when the entries of $M$ are in $\{0,1\}$ we have the
following:

Define a *score*: $s(M) = \sum_{i=1}^m \sum_{j=1}^n 2^{i + j} M_{i,j}$.
We can define column and row scores:
$c(i,M) = \sum_{j=1}^n 2^j M_{i,j}$
$r(j,M) = \sum_{i=1}^m 2^i M_{i,j}$.
Note that $s(M) = \sum_{i=1}^m c(i,M) = \sum_{j=1}^n r(j,M)$.
If $c(i,M) > c(i',M)$ for some $i < i'$, then swapping those will
change the score $s(M)$ by $(2^{i'} - 2^i) c(i,M) - (2^{i'} - 2^i)
c(i',M) = (2^{i'} - 2^i)(c(i,M) - c(i',M)) \ge 2^i$,  However, the maximum
score is $(2^{m+1} - 1) (2^{n+1} - 1)$.  Similarly for $r(j,M)$.  So
this shows that the double sorting process stabilizes after a finite
time.

An interesting question is what is the real maximum?

* Lexicographic comparisons

A method of symmetry breaking in SAT is the following:

Let $F$ be a formula with the variables $x_1, \dots, x_n$.  Supposed
that $\tau$ is a maps from $\{0,1\}^n$ to itself, so that if $\alpha$
satsifies $F$ then so does $\tau(\alpha)$.  We can define $\tau$ by a
set of clauses involving the variables $x_1, \dots, x_n$ and new
variables $x_1', \dots, x_n'$ plus some number of auxilliary
variables.  We then can demand that $x < x'$, where $<$ denotes
lexicographic order, where we've fixed the order of the the variables
in $x$ and $x'$.  As long as we use the same order of variables, such
inequalities for different endomorphisms are all valid.

Note that if $\tau$ is a signed permutation of the variables $x_i$
then we do not need to introduce new variables $x'$ and auxilliaries.

A method of trimming the lex order, is that if corresponding variables
are equal, they may be omitted, from comparison.  If two variables are
complemented, this "short circuits" the comparison: everything after
them may be dropped.

Proposition: Let $S$ be a totally ordered set, and let $<$ and $\le$
denote lexicographic order on $S^k$ for various $k$.  If $a,b \in S$, then
$(a,b) < (b,a)$ if and only if $a < b$.  Also $(a,b) \le (b,a)$ if and
only if $a \le b$.

Proof: By definition, $(a,b) < (b,a)$ if and only if either $a=b$ and
$b < a$, or $a < b$.  The first alternative is impossible.  Also by
definition $(a,b) \le (b,a)$ if and only if either $a=b$ and $b \le a$
or $a \le b$.  The first alternative can only happen if $a=b$, and the
second alternative subsumes that.

By the above discussion, if $A$ is a feasible 0/1 array for balanced
detecting problem, then so is $B$, where $B_j = A_i \oplus A_j$ for
all $j \ne i$, and $B_i = A_i$.  Note that a feasible array also
satisfies $A_i \ne 0$ for all $i$.  If we fix the boolean variables
order for lexicographic comparison at $A_{1,1}, \dots, A_{1, n}, A_{2,
1}, \dots A_{2,n}, \dots$, that is we flatten the array by rows, then
we have $A \ne B$, since $A_i \ne 0$, and $A < B$ if and only if
$either $A_1 < B_1$ if $i \ne 1$ of $A_2 < B_2$ if $i=1$.  In the
former case this is $A_1 < A_1 \oplus A_i$, and in the latter,
$A_2 < A_2 \oplus A_1$.  The proof of that follows since
writing out $A$ is the determined lex order we have
$(A_1, A_2, \dots, A_m) < (B_1, B_2, \dots, B_m)$.  In the case that
$i \ne 1$ we have $A_1 \ne B_1 = A_1 \oplus A_i$, since $A_i \ne 0$.
Thus, by definition of lex order we have $A  < B$ if and only if $A_1
< B_1$.  Similarly, if $i=1$, we have $A_1 = B_1$, and $A_2 \ne B_2$.

After the $i$-th operation when $i > 1, $B_j = A_j \oplus A_i$ for $j
\ne i$, and $B_i = A_i$.  Suppose we apply the $i'$-th operation to
$B$ for $i' \ne 1, i$.  Then $B'_j =B_j \oplus B_{i'}$ for $j \ne i'$
and $B'_{i'} = B_{i'}$.  If $j \ne i,i'$, then $B'_j = A_j \oplus A_i
\oplus A_{i'} \oplus A_i = A_j \oplus A_{i'}$.  If $j = i$, we have
$B'_i = A_i \oplus A_{i'} \oplus A_i = A_{i'}$, and $B'_{i'} = B_{i'}
= A_{i'} \oplus A_i$.

In the above variable order, the inequality from swapping two
adjacent rows, after removing all trivial equalities will be of the
form $(a,b) < (b,a)$, where $a$ is the first row and $b$ the second.

The inequality from swapping two columns after removing trivial
inequalities will of the form $(a_1, b_1, a_2, b_2, \dots, a_m, b_m) <
(b_1, a_1, b_2, a_1, \dots, b_m, a_m)$ where $a$ is the first column
in order, and $b$ is the second.  Inductively using the fact
that $(a,b) \le (b,a) \Leftright arrow a \le b$ and $(a,b) = (b,a)
\Leftrightarrow a = b$ on each pair, show that this is equivalent to
$(a_1, \dots, a_m) < (b_1, \dots, b_m)$.

If $a,b$ are two 0/1 $n$-tuples, and $a < b$, let $i$ be the smallest
index such that $a_j = b_j$ for all $j < i$ and $a_i \ne b_i$.  Since
$a < b$, we must have $a_i = 0$ and $b_i = 1$.  Then $a \oplus b$
starts with $0^{i-1} 1$.

* Reduction of lex constraints
 1) If $\alpha = \gamma$ entails $x=y$, then a constraint $c$ of the
    form $\alpha x \beta \le_{\tt lex} \gamma y \delta$ may be
    replaced with $$\alpha \beta \le_{\tt lex} \gamma \delta$.
2) Let $C = C' \cup \{\alpha x \le_{\tt lex} \gamma y\}$ be a set
   of constraints.  If $C \cup \{\alpha = \gamma\}$ entails $x=y$, then
   $C$ can be replaced with $C' \cup \{\alpha  \le_{\tt lex}
   \gamma\}$.
3') Let $C = C' \cup \{\alpha x \beta \le_{\tt lex} \gamma y \delta\}$
be a set of constraints.  If $C \cup \{\alpha = \gamma\}$ entails
$x=y$, then $C$ can be replaced with $C' \cup \{\alpha  \beta \le_{\tt lex}
   \gamma \delta\}$. 

<<<<<<< HEAD
* Algorithms
Reference: "Minimal ordering constraints for some families of variable
symmetries" by Grayland, Jefferson, Miguel and Roney-Dougal

In that paper they give the three rules above.  On page 81 and
following they describe a graph-based algorithm for reducing sets of
lexicographic constraints.  I think that, implicitly, all
lexicographic inequalities only involve strings of variables of the
same length.  Given a collection of lexicographic inequalities, and
variable inequalities construct a directed graph $G$ whose nodes are
labeled by the variables in the support of all the clauses.  Here,
$x,y$ and $z$ denote variables (in the SAT case literals)

a) If $x \le y$, draw an edge from $x$ to $y$.
b) if $x = y$, draw an edge from $x$ to $y$ and one from $y$ to $x$.
c) If $x \alpha \le_{\tt lex} y \beta$ for some strings of variables
$\alpha, \beta$, draw an edge from $x$ to $y$.

Next take the transitive closure of this graph.  Note that variables
in the same strongly connected component are equal.   We can now use
the equality deletion rule to modify the constraints.  If there are
new heads of any of the lex constraints, they will add new edges to
the graph.

* An important special case

Let $M$ be an $m \times n$ matrix of boolean variables, and we know
that if $M$ is a solution to our SAT problem then so is $M'$, obtained
from $M$ by permuting rows and columns.  That is, the symmetry group
of the problem constains $S_m \times S_n$.  Consider the variables
order $M_1, M_2, \dots, M_n$, where $M_i is the sequence $M_{i,1},
M_{i,2}, \dots, M_{i,n}$.  The following notation will be useful:
$M_i$ will denote the variables in row $i$ in column order: $M_{i,1}
\cdots M_{i,n}$
$M_{i:j}$ will denote the string of variables $M_i M_{i+1} \cdots
M_j$, and the empty string if $j < i$.

For $1 \le i < m$, let $\sigma_i$ denote
the permutation which swaps $M_i$ and $M_{i+1}$.  Then the lex-leader
constraint correspond to $\sigma_i$ is
$M_{1:i-1} M_i M_{i+1}   \cdots \le_{\tt lex}
M_{1:i-1} M_{i+1} M_i M_{i+2:n}$
Removing equal strings makes this equivalent to
$M_i M_{i+1} \le_{\tt lex} M_{i+1} M_i$.  By the above discussion this
is equivalent to $M_i \le_{\tt lex} M_{i+1}$.

Similarly, let $\tau_i$ for $1 \le i < n$, denote the permutation that
swaps columns $i$ and $i+1$.  The lex-leader constraint corresponding
to $\tau_i$ is
$M_{1,1:i-1} M_{1,i} M_{1, i+1} M_{1,i+2:n}
M_{2,1:i-1} M_{2,i} M_{2, i+1} M_{2,i+2:n}
\dots
M_{m,1:i-1} M_{m,i} M_{m, i+1} M_{m,i+2:n}
\le_{\tt lex}
M_{1,1:i-1}  M_{1, i+1} M_{1,i} M_{1,i+2:n}
M_{2,1:i-1}  M_{2, i+1} M_{2,i} M_{2,i+2:n}
\dots
M_{m,1:i-1}  M_{m, i+1} M_{m,i}M_{m,i+2:n}$

Removing equal variables in corresponding positions yields an equivalent
$M_{1,i} M_{1, i+1}
M_{2,i} M_{2, i+1}
\dots
M_{m,i} M_{m, i+1}
\le_{\tt lex}
M_{1, i+1} M_{1,i}
M_{2, i+1} M_{2,i}
\dots
M_{m, i+1} M_{m,i}$

Use the fact that $ab \le_{\tt lex} ba$ is equivalent to $a \le_{\tt
lex} b$ for each pair yields an equivalent:

$M_{1,i} M_{2,i} \dots M_{m,i}  \le_{\tt lex}
M_{1, i+1}M_{2, i+1}\dots M_{m, i+1}$.

* Automatically finding syntactical symmetries
If $F$ is a CNF formula, a *syntactical symmetry* is a *signed
permutation* of the variables in the support of $F$.  If $V$ is a set
of boolean variables, a signed permutation of $V$ is a permutation,
$\pi$, of $L$ the set of literals over $V$ (i.e. $L = V \cup
\{\overline{v}: v \in V\}$), such that $\overline{\pi(v)} =
\pi(\overline{v})$.  A signed permutation *leaves $F$ invariant* if,
for all clauses $C \in F$, we have $\{\pi(\ell) : \ell \in C\} \in F$.

The following is true:

Definition: If $G$ is an undirected graph, an autmorphism, $\pi$ of
$G$, is a one-to-one map $\pi: V(G) \rightarrow V(G)$, such that, for
all $\{v,w\} \in E(G)$, we have $\{\pi(v), \pi(w)\} \in E(G)$.  If $G$
is colored, i.e. there is a map $c: V(G) \rightarrow C$, to some set
$C$, a colored automorphism of $G$, is an automorphism, $\pi$ such
that $c(\pi(v)) = c(v)$ for all $v \in V(G)$.

Theorem: Given a CNF formula $F$, without unit clauses and with
support $V$, construct the vertex colored undirected graph $G$ as
follows: The vertices of $G$ is the set of literals, $L$ over $V$,
along with all clauses $C$ in $F$ of width $\ge 3$.  For each $v \in
V$ there is an edge joining $v$ to $\overline{v}$.  For each clause
$C$ of with $\ge 3$ there is an edge joining $\ell$ to $C$ for each
literal $\ell \in C$.  For each clause, $C = \{\ell, \ell'\}$, of width 2, there is an
edge joining $\ell$ and $\ell'$.  All vertices corresponding to
literals are colored red, and all vertices corresponding to clauses
are colored blue.  Then the colored automorphism group of $G$ is
exactly the group of signed permutations of $F$.

Proof: Let $\pi$ is a signed permutation of $F$. First, if $v \in V$,
the support of $F$, there is an edge in $G$ of the form $\{v,
\overline{v}\}$. Clearly $\pi$ leaves this edge invariant.  If
$\{\ell, \ell'\} \in F$ is a 2-clause, then, by definition of signed
permutation, $\{\pi(\ell), \pi(\ell') \}$ is also a clause in $F$, and
so is also an edge in $G$.  Finally, if $C$ is $k$-clause for $k \ge
3$, $\pi(C)$ is also a $k$-clause, and is connected to exactly the
literals in $\pi(C)$ in $G$.  Conversely, let $\pi$ be a colored
automorphism of $G$.  Since $\pi$ respects colors, it can only map
literals to literals.  Since the edge $\{v, \overline{v}\}$ is in $G$,
the edge $\{pi(v), \pi(\overline{v})\}$ is also in $G$.  Note that the
edges containing a literal $\ell$ are exactly $\{\ell,
\overline\{\ell}\}$ along with all 2-clauses $C$ containing $\ell$.
Thus $\pi(\overline{\ell})$ must be in the set $\{\overline{\ell}\}
\cup \{\mu \in C: \mu \ne \ell, \ell \in C \text{ a 2-clause}\}$.  Why
does that pin it down?

* A set covering problem

For $n$ a positive integer, define $B(n) = \{x \in \{0,1,-1\}^n :
\sum_{i=1}^n x_i = 0, x \ne 0 \}$.  Given $x \in B(n)$, define $N(x) =
\{y \in \{0,1\}^n : x^T y = 0 \}$.  A subset $C \subseteq B(n)$ is a
*cover* if $\cup_{x \in C} N(x) = \{0,1\}^n$.  Note that $C = B(n)$ is
a cover if $n \ge 3$.  We are interested in finding a minimum
cardinality set $C$ which is a cover.

The integer programming specification of this is as follows:

We have a 0/1 variables $y_x$ for each $x \in B(n)$
minimize $\sum_{x \in B(n)} y_x$ subject to:

for each $z \in \{0,1\}^n$ a constraint $C(z) \ge 1$
$\sum_{x, z \in N(x)} y_x \ge 1$.

This has $2^n$ constraints in $\#B(n)$ variables.

Question: Can we solve this via cutting planes, or use the large
symmetry group to reduce the size of the LP.

First, the constraint $C(z) \ge 1$: it is violated if and only if
$y_x = 0$ for all $x$ such that $z^T x = 0$.

A simple greedy algorithm is the following:
Let $\cS$ be the set of $x \in B(n)$ already chosen.  Initially $\cS =
\emptyset$.
At each stage we want to choose $x$ so as to minimize the number of
uncovered $z$ in $\{x\} \cup \cS$.

Question: Can we use representation theory to reduce the IP?


For a given $x \in B(n)$ it's easy to explicitly describe the set of
$z$ such that $x^T z = 0$.  Let $S_1 = \{i : x_i = 1\}, S_{-1} = \{i :
x_i = -1\}$, and $T$ the support of $z$.  It is necessary and
sufficient that $\# T \cap S_1 = \# T \cap S_{-1}$.  Thus, if $x \in
B_m(n)$, we have the number of such $z$ is $\sum_{2j + k \le n}
\binom{m}{j}^2\binom{n-m}{k}$.

The symmetry group $S_n$ acts on the LP.  The orbits acting on $B(m)$
are $B_m(n)$.

Note that $z \in N(\sigma x)$ if and only if $z^{\sigma^{-1}}  \in
N(x)$.  So if we calculate $\sum_{\sigma} C(z_0 ^\sigma)$.

Note that we can relax the linear program as follows:
If $c_z \ge 0$, then the constraint
$\sum_z c_z C(z) \ge \sum_z c_z$ is valid.

At each stage we want to choose $x \not \on \cS$ so that $\dim \langle
\{x\} \cup \cS \rangle$ is minimal


Note that $z \in N(x)$ if and only if $z^T x = 0$,  In particular, let
$S$ be the support of $z$, and $S'$ its complement.

The dual of of the LP relaxation is a packing problem:

There is a variable $w_z \in [0,1]$ for each $z \in \{0,1\}^n$.
We then
maximize $

* 01 Covering set

Definition: A subset $S \subseteq B(n)$ is a *01 Covering Set* if for
all $x \in \{0,1,\}^n$ there is an $y \in S$ such that $y^T x = 0$.
Note that for $n \ge 3$, that $B(n)$ is a 01 covering set (proof
below).  Thus we have the problem of finding a 01 covering set of
minimal cardinality

Lemma: Let $A \in \{0,1\}^{m \times n}$, and $S$ be a 01 covering set.
Then $A$ is a balanced detecting matrix if and only if $Ax \ne 0$ for
all $x \in S$.

Proof: Note that $A$ is a balanced detecting matrix if and only if
there is a row $A_i$ such that $A_i$.


 
=======
Notice that (1) is a special case if (3'), when $C'$ is the empty set.


A simple and useful special case of (1) is if $x=y$ because $x$ and
$y$ are identical.  Another useful rule is

Let $C = C' \cup \{\alpha x \beta \le_{\tt lex} \gamma y \delta\}$
be a set of constraints.  If $C \cup \{\alpha = \gamma\}$ entails
$x\ne y$, then $C$ can be replaced with $C' \cup \{\alpha \le_{\tt
lex}  \gamma\}$. 

In the above it's not clear, in practice, what "entailment" algorithm
is used.  I would think that something like unit propagation would be
a good compromise.

* Automatically finding syntactic symmetries.

Let $V$ be a finite set of variables, and $L$ the corresponding set of
literals over $V$: $V \cup \{\sim v : v \in V\}$.  A *signed permutation*
of $V$, is a one-to-one map $\pi: L \rightarrow L$, such that,
$\sim \pi(v) = \pi(\sim v)$ for all $v \in V$, where $\sim (\sim v) = v$.

Treating a clause as a subset of $2^L$, we define $\pi(C) =
\{\pi(\ell) : \ell \in C\}$.

Definition: if $F$ is a CNF formula, with support $V$, then a signed
permutation $\pi$ of $V$ is an *automorphism* of $F$ if
$\pi(C) \in F$ for all $C \in F$.  Such a signed automorphism is also
called a *syntatic automorphism*.

The group of all syntactic symmetries may be found by reducing the
problem to finding the group of *colored automorphisms of a graph*.

More specifically construct an undirected graph whose vertices are
$L$, colored red, and $C \in F$, colored blue.    There is an edge
$\{v, \sim v\}$ for all $v \in V$.  There is also an edge $\{\ell,
C\}$ for all $\ell \in C$.  This was the construction in Crawford, Ginsberg,
Luks, and Roy "Symmetry Breaking predicates for search problems", in
Proceedings of Knowledge Repersentation and Reasoning, November 1996, pp
148-159.

Aloul ("Symmetry in Boolean Satisfiability", Symmetry 2010, no, 2,
1121-1134) refines this construction  by omitting 2-clauses, and,
instead, adding the edge $\{\ell, \ell'\}$ for a 2-clause with those
constituent literals.
>>>>>>> 1afcfe6 (Added more paragraphs in writeup.)

