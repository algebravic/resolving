#+Title: Metric Dimension
#+Latex_header: \DeclareMathOperator{\wt}{wt}
#+Latex_header: \newcommand{\RR}{\mathbb{R}}
#+Latex_header: \DeclareMathOperator{\ker}{ker}
#+Latex_header: \DeclareMathOperator{\Tr}{Tr}
#+bibliography: ~/BibFiles/graphs.bib
#+cite_export: biblatex backend=bibtex,style=alphabetic,url=true

* <2023-03-23 Thu> Introduction
Let $G$ denote a finite graph (directed or undirected). If $u,v \in
V(G)$ are two vertices of $G$ define $d(u,v)$ as being the length of
the shortest path from $u$ to $v$, and $+\infty$ if there is no such
path.  A *resolving set* for $G$ is a subset $S \subseteq V(G)$ such
that for each $u \ne v \in V(G)$ there is a $w \in S$ such that
$d(u,w) \ne d(v,w)$.  Clearly resolving sets exist, since $S = V(G)$
is one such.  The minimum cardinality of a resolving set is called the
*metric dimension* of $G$.

We are particularly interested in the metric dimension of the
hypercube $Q_n$ whose vertices are $\mathbb{F}_2^n$, and edges are
$(u,v)$ such that $u + v$ has weight 1.  Thus the distance metric $d$
is the *Hamming distance*.
** Using MAXSAT
One can approach this problem using ILP (integer linear programming)
or MAXSAT (maximum satisfiability).

For each pair $(u,v) \in V(G)$ with $u \ne v$ define the set
$P_{u,v} := \{w \in V(G) : d(v,w) \ne d(u,w)\}$.  We'll call this set the *pair
resolver*.

Clearly a set $S$ is resolving if and only if for each
$u \ne v \in  V(G)$
 we have $S \cap P_{u,v} \ne \emptyset$.  That is, $S$ contains
at least one element of each $P_{u,v}$.  In other terminology, $S$ is
a *hitting set* for the set system $\{P_{u,v} : u \ne v \in V(G) \}$.
** Questions
This brings up a number of practical questions:

1) Given a subset $S$ which is not a resolving set, efficiently
   produce a pair $(u,v)$ such that $d(w,u) = d(w,v)$ for all $w \in
   S$.  Can we do this without explicitly listing all resolving pairs?
2) Note that if $\sigma \in \text{Aut}(G)$ and $S$ is resolving then
   so is $\sigma(S)$.  This is true because $d(u,v) = d(\sigma u,
   \sigma v)$ for all $u,v \in V(G)$.

Possible approach: One only need to characterize the set of pairs
which have the same Hamming weight.

One might be able to do this inductively: Given a subset $S$, split it
into two subsets $S_0$ and $S_1$, distinguished by the first
coordinate, and denote by $\pi: \mathbb{F}_2^n \rightarrow
\mathbb{F}_2^{n-1}$ the projection onto the last $n-1$ coordinates.
A pair $(u,v)$ will be resolved by $S$ if and only if
a) $u_1 = v_1$ and $(\pi(u), \pi(v))$ is distinguished by $\pi(S)$.
b) $u_1 \ne v_1$ and $\pi(u)$ and $\pi(v)$ ...

An auxilliary question: Given a subset $S \subseteq \mathbb{F}_2^n$,
find an element $u \in \mathbb{F}_2^n$, such that $\max_{v \in S}
d(u,v)$ is minimized.  Call such a $u$ a *centroid* for $S$.

Maximum is hard to deal with.  Use the fact that average is a lower
bound to the maximum.  Suppose that $A_{i,j}$ is the $i$-th bit of the
$j$-th word in $S$.  Suppose that $x$ is the unknown centroid.

Form
$$
\begin{split}
\sum_j \sum_i (x_i - A_{i,j})^2 & = \sum_i \sum_j (x_i + A_{i,j} -
2 x_i A_{i,j}) \\
 & = \sum_{i,j} A_{i,j} + |S| \text{wt}(x) - 2 \sum_i (\sum_j
A_{i,j}) x_i \\
& = \sum_{i,j} A_{i,j} - 2\sum_i (\sum_j (A_{i,j} - 1/2) - x_i).
\end{split}
$$

** Stabilizer by the hyperoctahedral group
The hyperoctahedral group is the group of automorphisms of $Q_n$.
More concretely, each automorphism is a pair $(\sigma, a)$ where
$\sigma \in S_n$, and $a \in \mathbb{F}_2^n$.  This acts as
$x \mapsto y$, where $y_i = x_{\sigma(i)} + a_i$.  What is the
stabilizer of an element?  We must have $x_i + a_i = x_{\sigma(i)}$.
This means that for each $\sigma$ there is a unique $a$ which makes
this true.  So each element has a stabilizer of order $n!$.  This
makes sense since the group is transitive.  What about subsets?
* Extended formulations
Here's the question: can we describe the hitting set problem more
economically?  More specifically can we specify the collection of
resolving sets more compactly with auxilliary variables?

So, to be more precise, Let $\mathcal{F}$ be a CNF formula in the set
of variables $V$.  We can think of it as specify a subset of $2^V$.  A
hitting set of this would be another subset $\mathcal{H}$ such that
for every $S \in \mathcal{F}$ there is a $T \in \mathcal{H}$ such that
$T \cap S \ne \emptyset$. 

* Some Results.

Let $Q^n$ denote the Hypecube, whose vertices are members
of $\mathbb{F}_2^n$ and whose edges connect two vertices whose weigh
differs by 1.   If $x \in \mathbb{F}_2^n$, let $\wt(x)$ denote the
*Hamming weight* of $x$, $\#\{1 \le i \le n: x_i = 1\}$, and $d(x,y)
:= \wt(x \oplus y)$.

Given $u,v \in \mathbb{F}_2^n$ let $R_{u,v} := \{ x \in
\mathbb{F}_2^n : d(x,u) \ne d(x,v) \}$.

Proposition: We have $R_{u,v} = u \oplus R_{0, u \oplus v}$.

Proposition: We have $R_{0,u} = \{ x \in \mathbb{F}_2^n:  2 \wt(u
\wedge x) \ne \wt(u)\}$. In particular, it includes all elements of
$\mathbb{F}_2^n$ if $u$ has odd weight.

Proof: Note that $\sum_{i=1}^n (-1)^{x_i} = \sum_{i=1}^n (1 - 2 * x_i)
= n - 2 \wt(x)$.  We have
$\wt(u \oplus x)  - \wt(x) = \sum_{i=1}^n ((u_i \oplus x_i) - x_i) =
$\sum_{i, u_i=1} (1- 2 * x_i) = \wt(u) - 2 \wt(u \wedge x)$.  So if $\wt(u)$ is
odd, this is always non-zero.

The equations to detect $(u,v)$ are similar (but not the same) as the
original ones:

* Bases
If $G$ is a permutation group, acting on a finite set $\Omega$, then a
*base* for $G$ is a subset $S \subseteq \Omega$ such that the subgroup
$\{ g \in G: g x = x, \forall x \in S\}$ is trivial. If $x_1, \dots,
x_m \in \Omega$ denote by $G_{x_1, \dots, x_m}$ the pointwise
stabilizer of all the $x_i$.  That is $G_{x_1, \dots, x_m} = \{g \in
G: g x_i = x_i, \forall 1 \le i \le m \}$.  The symmetry breaking
constaints can be of the following form:

Once we have chosen $x_1, \dots, x_m$, choose $x_{m+1}$ being members
of a set of representatives of distinct orbits of $\Omega$ under
$G_{x_1, \dots, x_m}$.  Call that set $S_{x_1, \dots, x_m}$.  The
clauses reprsenting this will be
$(x_1 \wedge \dots \wedge x_m \Rightarrow x)$ for all $x \in S_{x_1,
\dots, x_m}$.

Define: Let $G$ be a finite graph.  If $x,y \in V(G)$ the *resolving
set* of $(x,y)$, $R_{x,y} = \{ z \in V(G) : d(z,x) \ne d(z,y)\}$ is
the set of vertices which are at distinct distances from $x$ and $y$.

Define: An equivalence relation on the vertex set $V(G)$.  Say that
$x \sim y$ if, for all $z,w \in V(G)$ we have $d(x,z) = d(x,w)$ if and
only if $d(y,z) = d(y,w)$.  It is now clear that every resolving set
consists of a union of equivalence classes under this relation.
 
It is clear that if $\sigma$ is an automorphism of
$X$ then it is also a metric automorphism (i.e. $\sigma$ preserves
adjacency), but that might be other metric automorphisms.  A
particular example of a metric automorphism which is not a graph
automorphism occurs with the hypercube $Q^n$.  If $e$ is the all 1's
vector then $d(e \oplus x, y) = n - d(x,y)$. So the metric
automorphism group of $Q^n$ has order $2^{2n} n!$

* Some Proofs

Lemma: We have $R_{u,v} = u + R_{0,u+v}$.
Proof: By definition, $d(x,a) = \wt(x + a)$.  Thus, if $d(x,u) \ne
d(x,v)$, we have $\wt(x+u) \ne \wt(x+v)$.  But $\wt(x+u) = d(x+u,0)$
and $\wt(x+v) = d(x+u,u+v)$.

Corollary: If $S \subseteq V(Q^n)$ is a resolving set, then so is $u +
S$ for all $u \in V(Q^n)$.  Without loss of generality, we may assume
that any resolving set contains 0.

Lemma: If $e=(1, \dots, 1) \in V(Q^n)$, we have, for all $x \in
V(Q^n)$, $\wt(x+e) = n - \wt(x)$.

Corollary: For all $x \in R_{u,v}$ we have $x+e \in R_{u,v}$.

Definition: Denote by $\beta_n$ the metric dimension of $Q^n$.

Lemma: For all $n$ we have $\beta_n \le \beta_{n+1}$.
Proof:  Let $S$ be a minimum size resolving set for $Q^{n+1}$.  Let
$\pi : V(Q^{n+1}) \rightarrow V(Q^n)$ denote the map which removes the
last coordinate: $\pi((x_1, \dots, x_{n+1}) = (x_1, \dots, x_n)$, and
$\psi: V(Q^n) \rightarrow V(Q^{n+1})$ the map that adds a 0 coordinate
at the end.  That is $\psi((x_1, \dots, x_n)) = (x_1, \dots, x_n, 0)$.
Without loss of generality, by adding $e$ to any element of $S$ whose
last coordinate is 1, we may assume that the last coordinate of all
the elements of $S$ are 0.  It is clear that if $x,y \in V(Q^n)$ that
$d(x,y) = d(\psi(x), \psi(y))$.  However, by the assumption on $S$, we
have, for all $u \in S$, $u = \psi(\pi(u))$.  Thus $\pi(S)$ is a
resolving set for $Q^n$. QED.

Lemma: For all $m,n$ we have $\beta_{m+n} \le \beta_m + \beta_n$.  In
particular, since $\beta_1 = 1$, we have $\beta_{n+1} \le \beta_n +
1$.

Proof: Let $S$ be a resolving set for $Q^n$ and $T$ a resolving set
for $Q^m$.  Without loss of generality, we may assume that $0 \in S, 0
\in T$.  Define two maps $\phi: V(Q^m) \rightarrow V(Q^{m+n})$, $\phi:
V(Q^n) \rightarrow V(Q^{m+n})$ as follows $\phi((x_1, \dots, x_m)) =
(x_1, \dots, x_m, 0, \dots, 0)$, and $\psi((y_1, \dots, y_n)) = (1,
\dots, 1, y_1, \dots, y_n)$.  I claim that $U := \psi(S) \cup \phi(T)$
is a resolving set for $Q^{n+m}$.  Define maps $\rho: V(Q^{n+m})
\rightarrow V(Q^n)$ by $\rho((x_1, \dots, x_{m+n})) = (x_{m+1}, \dots,
x_{m+n}))$ and $\sigma: V(Q^{n+m}) \rightarrow V(Q^m)$ by
$\sigma((x_1, \dots, x_{m+n})) = (x_1, \dots, x_m)$.
We show that for all $x\ne y \in
V(Q^{m+n))$ there is an element of $U$ that resolves $(x,y)$. There
are three cases.

Note that for all $x,y$, $d(x,y) = d(\sigma(x), \sigma(y)) +
d(\rho(x), \rho(y))$.
Note that we have $\sigma \psi u = e$ and $\rho \psi u = u$,
$\sigma \phi v = v, \rho \phi v = 0$.

1) $\wt(\sigma(x)) = \wt(\sigma(y))$ and $\rho x \ne \rho y$.  Let $u
   \in S$ resolve  $(\rho(x),  \rho(y))$.  Then
$$ \begin{aligned}
   d(\psi(u), x) - d(\psi(u), y) &= d(\sigma \psi u, \sigma x) + d(\rho \psi u, \rho  x)
                                                     -(d(\sigma \psi u, \sigma y) + d(\rho \psi u, \rho x)) \\
                                               &= d(e, \sigma x)  + d(u, \rho x) - (d(e, \sigma y)  + d(u, \rho y)) \\
                                               & = d(u, \rho x) - d(u, \rho y)
  \end{aligned}$$
By hypothesis there is a $v \in T$ which resolves $(\rho x,  \rho y)$.
   
2) $\wt(\rho(x)) = \wt(\rho(y))$ and $\sigma x \ne \sigma y$.  For $v \in T$
$$ \begin{aligned}
   d(\phi(v), x) - d(\phi(v), y) &= d(\sigma \phi u, \sigma x) + d(\rho \phi u, \rho  x)
                                                     -(d(\sigma \phi u, \sigma y) + d(\rho \phi u, \rho x)) \\
                                               &= d(v, \sigma x)  + d(0, \rho x) - (d(v, \sigma y)  + d(0, \rho y)) \\
                                               & = d(v, \sigma x) - d(v, \sigma y)
  \end{aligned}$$
By hypothesis there is a $v \in T$ which resolves $(\sigma x,  \sigma y)$.
3) In the remaining cases either $\wt \sigma x \ne \wt \sigma y$ or
   $\wt \rho x \ne \wt \rho y$.  I assert that either 0, or $\psi 0$
   resolve $(x,y)$.  Namely
$$\begin{aligned}
   d(0, x) - d(0, y) &= d(\sigma 0, \sigma x) + d(\rho 0, \rho  x)
                                                     -(d(\sigma 0, \sigma y) + d(\rho 0, \rho x)) \\
                                               &= d(0, \sigma x)  + d(0, \rho x) - (d(0, \sigma y)  + d(0, \rho y)) \\
                                               & = (\wt \sigma x - \wt \sigma y) + (\wt \rho x - \wt \rho y)
  \end{aligned}$$
and
$$\begin{aligned}
   d(\psi 0, x) - d(\psi 0, y) &= d(\sigma \psi 0, \sigma x) + d(\rho 0, \rho  x)
                                                     -(d(\sigma \psi 0, \sigma y) + d(\rho \psi 0, \rho x)) \\
                                               &= d(e, \sigma x)  + d(0, \rho x) - (d(e, \sigma y)  + d(0, \rho y)) \\
                                               &= -(\wt \sigma x - \wt \sigma y) + (\wt \rho x - \wt \rho y)
   \end{aligned}$$

* More efficient generation of hitting sets

Since the resolving set $R_{u,v} = u + R_{0, u+v} = v + R_{0, u+v}$,
we concentrate, first, on describing $R_{0,u}$.

Lemma: The set $R_{0,u} = \{ x : 2\wt(x \wedge u) \ne \wt(u)\}$.  In
particular, this means that if $\wt(u)$ is odd, then R_{0,u} =
\mathbb{F}_2^n$.

Proof: We first describe the complement of $R_{0,u}$.  By definition
$x \not \in R_{0,u}$ if $\wt(x) = \wt(x + u)$.  However $x = x
\wedge u + x \wedge \neg u$, where the two terms are disjoint.
Similarly $x+u = \neg x \wedge u + x \wedge \neg u$.
Thus $\wt(x) = \wt(x+u)$ if and only if $\wt(x \wedge u) = \wt(\neg x \wedge
u)$. Here $\wedge$ is elementwise.  The latter is true if and only if
$2 \wt(x \wedge u) = \wt(u)$.

We know find the size of the orbits of $R_{0,u}$ under the action of
the hyperoctahedral group.  To do that first, we find the order of the
stabilizer.  It suffice to consider only those $u$ of the form $(1,
\dots, 1, 0, \dots 0)$.  That is those in which $u_i = 1$ for $i=1,
\dots, t$ for $t$ even, and $u_i = 0$ for $i=t+1, \dots, n$.

Lemma: If $\wt(u), \wt(u')$ are even and $u \ne u'$ then $R_{0,u} \ne
R_{0,u'}$.

Proof: Since we may identify elements of $\mathbb{F}_2^n$ with subsets
of $[n] := \{1, \dots, n\}$.  Without loss of generality we may assume
that $\#U$ and $\#U'$ are both even, and that $\#U \ge \#U'$.
Let $X_1$ be a subset of $U \cap U'$ of cardinality $\lfloor \#(U \cap
U') / 2 \rfloor$, $X_2$ a subset of $U' \backslash U$ of cardinality
$\lceil \#(U' \backslash U) / 2\rceil$.
If we choose $X$ so that $w := \# (X \cap U') \ne \frac 1
2 \#U'$, then $x \in R_{0,u'}$.  Choose $X$ so that $\#(X \cap (U
\backslash U')) = \lfloor \# (U \backslash U') / 2 \rfloor$.
R_{0, u'}$.  If $\#(U \backslash U') \ge \frac 1 2 \#U$, then we may
choose $X$ to be contained in $\#(U \backslash U')$ and have
cardinality $\frac 1 2 \#U$
It suffices to prove that there exists a
$X \subset [n]$ such that $2 \#(X \cap U) = \#U$, and $2 \#(X \cap U')
\ne \#U'$.   It is clear that such an $X$ exists if and only if such
an $X$ exists with $X \subseteq U \cup U'$.
So let $w_1 = \#(X \cap (U \cap U'))$, $w_2 = \#(X \cap (\overline{U}
\cap U'))$, $w_3 = \#(X \cap (U \cap \overline{U'}))$.
Then $\#(X \cap U) = w_1 + w_3$, $\#(X \cap U') = w_1 + w_2$.
Similarly let
$z_1 = \#(\overline{X} \cap (U \cap U'))$,
$z_2 = \#(\overline{X} \cap (\overline{U} \cap U'))$,
$z_3 = \#(\overline{X} \cap (U \cap \overline{U'}))$.
Then $\#U = w_1 + w_3 + z_1 + z_3$,
$\#U' = w_1 + w_2 + z_1 + z_2$.
$\#U = 2 w_1 + 2 w_3$,
$\#U' = 2 w_1 + 2 w_2 + 2 y$ with $y \ne 0$
$$A =
\begin{pmatrix}
1 & 0 & 1 & 1 & 0 & 1 & 0\\
1 & 1 & 0 & 1 & 1 & 0 & 0\\
2 & 0 & 2 & 0 & 0 & 0 & 0\\
2 & 2 & 0 & 0 & 0 & 0 & 2
\end{pmatrix}$$
Eliminating:
Elimnate $w_1$:
(a) $\#U - \#U'$ = w_3 - w_2 + z_3 - z_2$
(b) $\#U = 2 z_1 + 2 z_3 - 2 w_3$,
(c) $\#U' = 2 z_1 + 2 z_2 - 2 w_2 - y$.
Eliminate $z_1$: (b) - (c)
(d) $\#U - \#U' = 2 z_3 + 2 w_2 - 2 w_3 - 2 z_2 + 2 y$.
Add (a) + (1/2) (d): $\#U - \#U' = 2 z_3 - 2 z_2 +  y$

Without loss of generality we may assume that $\#U \ge \#U'$.
The kernel 
* <2023-04-25 Tue> Finding resolvers

Here's the problem:

We're given A partition $[r,s,s]$ of $n=r + 2s$ corresponding to
 putting all 0's in the $r$ part, all 1's in the first $s$ part and
 all $-1$s in the second $s$ part. We'd like to find all 0/1 tuples
 so that $x \cdot s != 0$ in efficient way.  First, the ones in the
 first part (or size $r$) can be arbitrary.  The only restriction both
 necessary and sufficient, is that the number of 1's in both of the
 $r$ parts are not equal.  Since we're only interested in those
 assignments which are inequivalent under permutations preserving the
 partition, we should put all the 1's first, before the 0's.  The
 number of assignments that yield 0 is then
$$2^r \sum_{j=0}^s \binom{s}{j}^2.$$

Recall that if $f(x) = \sum_{j=0}^s \binom{s}{j}x^j = (1+x)^s$, then
$f(x) ^ 2 = \sum_{j=0}^{2s} (\sum_{i=0}^{\min{j,s}} \binom{s}{i}
\binom{s}{j-i}) x^j$.  The coefficient of $x^s$ in this is
$\sum_{i=0}^s \binom{s}{i}\binom{s}{s-i} = \sum_{i=0}^s
\binom{s}{i}^2$.  Thus this is $\binom{2s}{s}$.

* <2023-04-25 Tue> Certifying resolving

If we're given a putative resolving set as an $m \times n$ matrix, $A$
whose rows are the members of the set, either it does not resolve, on
which case a SAT solver can find a pair of nodes which can't be
distinguished, say, $u,v$.  This is equivalent to $A (u-v) = 0$.  If
it does resolve, we'd like a good certificate.  This is equivalent to
showing that the only solution to $A x = 0$ where all the coordinates
of $x$ are $0,\pm 1$ is the 0 vector.  This would be equivalent to the
there being no 0/1 solutions to $A (x-y) = 0$ with $x_i + y_i \le 1$
and $\sum_i (x_i + y_i) \ge 1$.  So, how many $0,\frac 12$ cuts would
we need to show this?  Somehow I think that I can reduce this to a
decoding problem for linear codes over GF(2).

First note that we can massage the matrix $A$ so that it's a 0/1
matrix.  If $e$ is the all 1 vector we then have $Ax + Ay  \le A e$.
Adding this to $Ax - Ay = 0$ we get $2Ax \le A e$. 

* <2023-04-27 Thu> An LP certificate?

Suppose that we want to verify that the metric dimension is $> m$.
This would be equivalent to the fact that for all $m \times n$ 0/1
matrices $A$ we have $Ax = 0$ there exists $x \in \{0,1,-1\}^n$, such that
$e^T x = 0$, where $e$ is the all 1's vector, and $x \ne 0$.

Another try: Consider only those vectors $y$ which have precisely one
coordinate +1 and one coordinate -1 and the rest 0.   Note that all
test vectors $x$ can be written as a sum of such vectors.  Thus if all
$y$ vectors satisfy $Ay = 0$ then so do all $x$ vectors.  So, taking
negation, if there exists an $x$ vector with $Ax \ne 0$ then there
exists a $y$ vector with $Ay \ne 0$.  However, even if there is a $y$
vector with $Ay \ne 0$, that doesn't mean that there isn't a sum of
such that yields 0.

This suggests an incremental method.  Start with a solution to all
weight 2 vectors.  From the solution we can find a description of all
disjoint sums which yield 0.  Add those clauses as conflict clauses.
In particular, one can just consider pairs 

Incrementally we can do the following: set up two sat solvers.
One of them will either find a counterexample, or, by showing UNSAT,
that all pairs are resolved.  We can have assumptions which are set to
the the current assignment for the $A$ matrix.  If it finds a
counterexample it will also generate other counterexamples by
permuting coordinates, with each permutation preserving the current A
matrix.   There may be too many permutations, so one would just want
to use a small generating set of the stabilizer.  An observation,
since there are lots of symmetries, one would like symmetry breaking
clauses.  First, since negating a test vector with nonzero value also
gives one with a nonzero value one can assume that the the nonzero
value is $\ge 1$.  We also can assume that, within a sector, that the
-1 come first, then 0's, then 1.  We can encode that in terms of the
assumption variables.  Note that if we take as necessary that all
columns are distinct, then all sectors have size 1, so this is
unnecessary.

This looks like a possible Skolem function: Given a boolean formula
$\phi(X,Y)$, where we want $\forall X \exists Y \phi(X,Y)$, is there a
boolean function $F(X)$ such that $\phi(X,F(X))$ is a tautology?

$E(X,Y,Y') := \phi(X,Y) \vee \neg \phi(X,Y') \vee (Y' \Leftrightarrow F(X))$
If $E$ is UNSAT, things are good, otherwise it produces a
counterexample.  How to repair?  They use MAXSAT.  Hard clauses
$E(X,Y,Y') := \phi(X,Y) \vee \neg \phi(X,Y')$.
soft $Y' \Leftrightarrow F(X)$

The other SAT solver will have the initial "bare bones" problem with
successive conflict clauses.

Another aside: If $n$ is large it's probably better to use Hermite
normal form of the putative $A$ matrix to find its kernel.  We then
have a lattice which will contain all possible countexamples.  One
could aggresively reduce this.

This is equivalent (?) to the following: let $V = \{A \in
\RR^{m \times n} : A x = 0, \forall x \in \{0,-1,1\}^n, e^T x = 0\}$.
Then we want all 0/1 matrices $A$ to be contained in $V$.  Since $V$
is convex this is equivalent to $[0,1]^{m \times n} \subseteq V$.
More details:  It's clear that $V$ is a linear subspace.  So let $W$
be the linear subspaces of $\RR^n$ which is spanned by $\{x \in
\{0,-1,1\}^n, e^T x = 0, x \ne 0\}$.  Then $V = \{A \in \RR^{m \times
n} : A v = 0, \forall v \in W\}$.  Conversely, if $[0,1]^{m \times n}
\not \subseteq V$, that's the set in which we can find a resolving
matrix.

Something is wrong with above since $[0,1]^n$ is full dimensional but
$V$ clearly is not.

Aside: If $U, V$ are convex sets is $U \backslash V$ either empty or
convex?   No, clearly not, consider the complement of $[0,1]^n$.

* <2023-04-27 Thu> Using a solver with assumptions
A solver with assumptions has the following property:
You can invoke it giving it a list of literals which are assumed
true.  The use of this is to be able to "turn clauses on and off".
For example this is used in RC2.

* <2023-05-02 Tue> Negation
I thought that the following should produce a negation of a CNF
#+begin_src python
  def implies(pool: IDPool,
              form1: Iterable[CLAUSE],
              form2: Iterable[CLAUSE]) -> Iterable[CLAUSE]:
      """
      Clauses instantiating cl1 -> cl2.
      """
      avatars = []
      for clause in form1:
          # Make lit equisatisfiable with clause
          if len(clause) > 1:
              lit = pool._next()
              yield [-lit] + clause
              yield from ([-elt, lit] for elt in clause)
          else:
              lit = clause[0]
          avatars.append(- lit)
      yield from (avatars + clause for clause in form2)

      def negate(pool: IDPool, formula: Iterable[CLAUSE]) -> Iterable[CLAUSE]:
      """
      Negate a formula.
      """
      yield from implies(pool, formula, [[]])
#+end_src

Suppose that $F$ is a CNF, $F = \bigwedge_i C_i$ where $C_i$ is a clause
(a disjunction of literals), $C_i = \bigvee_j \ell_{i,j}$.  Introduce
new variables $x_i$, the clauses $\neg x_i \vee C_i$, and $\neg
\ell_{i,j} \vee x_i$ for all $i,j$.  Then $F$ is equisatisfiable with
$\bigwedge_i x_i$.  Thus, $\neg F$ is equisatisfiable with $\bigvee_i
\neg x_i$.  Suppose that $S$ is a compatible set of literals, such
that $F(S)$ is satisfiable (here $F(S)$ means the result of unit
propation after assuming all the literals in $S$).  Can it happen that
$\neg F(S)$ is satisfiable?  This would be equivalent to there exists
and $i$ such that $\neg C_i(S)$ is satisfiable.

More details: suppose that $F(X,Y)$ is a formula, where the variables
in $X$ and $Y$ are disjoint.  Suppose that $x_0$ is an assignment of the
variables in $X$, such that there is a $y_0$ with $F(x_0, y_0)$ true.
If we look at $\neg F(X,Y)$ is it true there there is no assignment
$y_1$ such that $F(x_0,y_1)$ is false?

Say $A$ is a desired set, and we find a formula such that
$A = \{ x : \exists y, F(x,y) \}$.  We want to describe the complement
of $A$.  This would mean that for all $x \in A$ for all $y$ $F(x,y)$
is false.

Suppose that we have $F(X,Y), G(X,Y)$ as CNF and we wish to encode
$F(X,Y) \vee G(X,Y)$.  Again, it might happen that if there are
$x,y_1, y_2$ such that $F(x,y_1)$ is true, $G(x,y_1)$ is false, but
$F(x,y_2)$ and $G(x,y_2)$ are both false.

But suppose that $z,w$ are new variables and we encode $F(X,Y)
\Rightarrow z, G(X,Y) \Rightarrow w$.

In the BDD encoding the auxilliary variables are completely determined
by the $X$ variables.

* <2023-05-02 Tue> The results

Definition: If $G$ is an unidirected graph, let $d_G(x,y)$ denote the
length of the shortest path in $G$ between $x$ and $y$.  A subset $S
\subseteq V(G)$ is *resolving* if for every $x \ne y \in V(G)$ there
is a $z \in S$ such that $d_G(x,z) \ne d_G(y,z)$.  The *metric
dimension* of $G$ is the cardinality of the smallest resolving set.

Definition: If $S$ is a set, the *Hamming Distance* between two
$n$-tuples in $S^n$ is the number of positions in which they differ.

Notation: If $n\ge 1$ is an integer, denote by $Q^n$ the *hypercube of
dimension $n$*.  This is the undirected graph whose vertices are
labeled with $n$-tuples of 0/1, with two vertices connected by an edge
if the *Hamming distance* between their labels is 1.

We are particularly interested in the metric dimension of the hypercube:
$Q^n$.   Denote by $\oplus$ the operation on the set $\{0,1\}$ given
by $0 \oplus 0 = 1 \oplus 1 = 0$, and $0 \oplus 1 = 1 \oplus 0 = 1$.
We note that for $x,y \in \{0,1\}$ we have $(-1)^{x \oplus y} =
(-1)^{x+y}$.  We also have $(-1)^x = 1 - 2x$.  We extend the $\oplus$
operation to $n$ tuples coordinatewise.

Notation: if $A$ is a 0/1 matrix, or $v$ a 0/1 vector, denote by $A'$
(resp. $v'$) the matrix whose $(i,j)$ entry is $(-1)^{A_{i,j})$
(resp. $i$-th entry is $(-1)^{v_i}$).
Definition: An $m \times n$ matrix, $A$, with entries in $\{0,1\}$ is a
*sensing matrix* if $A'u' \ne A'v'$ for all $u \ne v \in \{0,1\}^n$.

Note that if $u,v \in V(Q^n)$ then $\sum_i (-1)^{u_i} (-1)^{v_i} =
\sum_i (1 - 2 (u_i \oplus v_i)) = n - 2 d_{Q^n} (u,v)$.

Note that $d_{Q^n}(z,u) \ne d_{Q^n}(z,v)$ if and only if
$0 \ne \sum_i (1-2z_i)(1-2u_i) - (1-2z_i)(1-2v_i) =
2\sum_i (1-2z_i)(u_i-v_i)$.  Or
$\sum_i (u_i - v_i) = 2\sum_i z_i (u_i - v_i)$
Let $A$ denote the matrix whose rows are the $z_i$.
Then
$$B := 
\begin{bmatrix}
1 & 1 & \dots & 1 \\
& & -2A \\
\end{bmatrix}$$

Then $A$ is a sensing matrix if and only if $2A x = e^T x$ for all vectors
$x \ne 0$ whose coordinates are in $\{0,-1,1\}$, $e$ is the all 1's vector.

Note: If the vectors, $x$ are restricted to those of the form
$\phi(u) - \phi(v)$, then $e^T x = 0$.

Lemma: A subset $S \subset V(Q^n)$ is a resolving set for $Q^n$ if and
only if the $m\times n$ matrix whose rows are the labels of $S$ is a
sensing matrix.

Lemma: Let $a \in \{0,1\}^n$, and $e = (1, \dots, 1)$.  A set $S
\subseteq V(Q^n)$ is resolving if and only if $a \oplus S$ is
resolving, where $a + S := \{a + \ell(v) : v \in S\}$.  A set $S$ is
resolving if and only if for every $x \in S$ the set $S' =
S\backslash\{x\} \cup \{e + x\}$ is resolving.

Definition: A sensing matrix is normalized if its first row is 0.

Corollary: The metric dimension of $Q^n$ is $\ge m$ if and only if
there exists an $m \times n$ normalized sensing matrix.

Definition: A 0/1 matrix, $A$ is *subsensing* if none of its rows is 0
and $Ax \ne 0$ for all $x \in \{0,-1,1\}$ such that $e^T x = 0$.

Corollary: The metric dimension of $Q^n$ is $\ge m$ if and only if
there exists an $(m-1) \times n$ subsensing matrix.

Proof: Let $A$ be a normalized sensing matrix.  

Notation: If $A$ is a matrix and $S$ is a subset of its columns,
denote by $A_S$ the sum of the columns indexed by $S$. 

Proposition: An $m \times n$ 0/1 matrix $A$ is a subsensing matrix if
and only if the the quantities $A_S$ for $S$ all subsets of column
indices of cardinality $\lfloor n / 2 \rfloor$ are distinct.

Corollary: An $m \times n$ subsensing matrix exists if and only if one
exists whose rows and columns are non zero and lexicographically
strictly increasing with respect to row/column index.

Proof: By the proposition, it is clear that permuting the rows and/or
columns of a subsensing matrix does not affect the property of being
subsensing.

* <2023-05-03 Wed> Hamming Graphs
Let $S$ be a finite set.  The *Hamming distance* on the set $S^n$ is
defined by:
$d((a_1, \dots, a_n), (b_1, \dots, b_n)) = \#\{1 \le i \le n: a_i \ne b_i\}$

The $n$ th order Hamming graph over $S$ is a graph whose vertices are
labeled by distinct $n$ tuples of elements of $S$, with an edge
between two vertices when the Hamming distance between their labels is
1.

We define a map from $\phi: S^n \rightarrow \{0,1\}^{S \times n}$, by
a *one-hot* mapping, $s \mapsto v$ where $v_t = 0$ if $t\ne s$ and
$v_s=1$.

Lemma: If $s,t \in S^n$, then $2 H(s,t) = d_H(\phi(s), \phi(t))$.

* <2023-05-05 Fri> Quantitative statements

Definition: A *separating vector* of length $n$ is  a vector $x$  with
coordinates in $\{0,-1,1\}$ and satisfying $\sum_{i=1}^n x_i = 0$, and
with $\#\{ 1 \le i \le n : x_i = 1\} \ge 2$.

Lemma: The number of separating vectors of length $n$ is
$(3^n - 1)/2 -2 \binom{n}{2}$.

Proof: The number of separating vectors is
$\sum_{j=2}^{\lfloor n/2 \rfloor} 2^j \binom{n}{n-2j,j,j}$.
Namely, then number of separating vectors, $x$, with
$\#\{1 \le i \le n : x_i = 1\}$ is $2^j \binom{n}{n-2j,j,j}$.
However
$\binom{n}{n-2j,j,j} = \binom{n}{2j} \binom{2j}{j}$.
Let $F(X) = \sum_{j=0}^{\lfloor n/2 \rfloor} \binom{n}{2j} X^{j}$.
and $G(X) = \sum_{j=0}^{n} \binom{2j}{j} (2X)^j$.
Note that $2 F(X) = (1 + \sqrt{X})^n + (1 - \sqrt{X})^n$.

#+begin_src text
  From OEIS:

  A002426		Central trinomial coefficients: largest coefficient of (1 + x + x^2)^n. 

 a(n) is asymptotic to d*3^n/sqrt(n) with d around 0.5.. - Benoit Cloitre, Nov 02 2002,
 d = sqrt(3/Pi)/2 = 0.4886025119... - Alec Mihailovs (alec(AT)mihailovs.com), Feb 24 2005
#+end_src

* <2023-05-12 Fri> Inclusion-Exclusion
Suppose that we have a finite set $A$ of cardinality $n$, and a
collection of subsets $S_i \subseteq A$, each satisfying
$|S_i|/|A| \ge \alpha > \frac 1 2$. We wish to give a lower bound to
the a set $C$ of indices such that $\bigcap_{j \in C} S_i =
\emptyset$.  Let $T_i = A \backslash S_i$.
Let $\chi$ denote the characteristic function.
$|S_i \cap S_j| = \sum_{x \in A} \chi_i(x) \chi_j(x) = \sum_x
(\ch_i(x) - \alpha) \chi_j(x) + \alpha\sum_x \chi_j(x)$

Lovasz Local Lemma?  The events are indexed by 0/1/-1 vectors summing
to 0.  If we have a random 0/1 vector, the probability that dotting
that with the event vector will be $2^{-n}$ times $\sum_{j=0}^k\binom{n}{2j}
\binom{2j}{j}$, where the event vector has $k$ 1's and $k$ -1's.  So
which other events are conditionally independent.

I think that independence is just linear independence.

If $v_1, \dots, v_r$ are 0/1/-1 vectors let $S(v_i)$ denotes the set
of all 0/1 vectors in the whose dot product with all the $v_i$ are 0.

Case: $k$-vectors.  Call a set of indices a *sector* if all of the
columns corresponding to those indices are equal, and that subset is
maximal with respect to that property.  

* More symmetries?

Let $A$ be a feasible $m \times n$ matrix.  We know that any
permutation of the rows and/or columns of $A$ is also feasible.  In
addition, for any row $c$ the matrix $C \oplus A$ is feasible, where
$C$ is the $m \times n$ matrix all of whose rows are equal to $c$.  We
would like $c$ to be one of the rows of $A$, so that one of the rows
of $C \oplus A$ is the all 0 row.  Among all of these matrices, we'd
like to choose the one which is lexicographically minimal.  This seems
too complicated.

For SAT problems Knuth (in volume 4.6, section 7.2.2.2) points out
that it is only necessary to find an *endomorphism*.  That is, if $x
:= x_1, \dots, x_n$ are boolean variables, then $\tau: \{0,1\}^n
\rightarrow \{0,1\}^n$ is an *endomorphism* of a formula $F$ if
whenever $x$ satisfies $F$ then so does $\tau(x)$.  Notice that it is
not necessary to specify any behavior for $\tau(x)$ when $x$ does
*not* satisfy $F$.  He points out that it is then valid to add the
constraints $x \le \tau(x)$, where $\le$ is any total order (most
likely lex).

I conjecture that there is a feasible $A$ if and only if there is a
feasible $A$ all of whose nonzero rows have weight $\lfloor n/2
\rfloor$.  Here's a try: Let $c$ be a row whose weight is $< \lfloor
n/2 \rfloor$.   We need to show that if $x$ is a balanced $\{0, \pm
1\}$ vector such that $c \cdot x \ne 0$, then there is a position is
$c$ containing a 0, so that $c'$, obtained from $c$ by changing that
to a 1, has the property that $c' \cdot x \ne 0$.  It really reduces
to the case that $c \cdots x = 1$, since $-x$ is also a test vector.
So now this means that we can find a position where $c$ contains a 0,
but $x$ contains a 0 or a 1.  And we need to do this for all $x$ such
that $c \cdot x = 1$.  Note that the only nonzero elements of such an
$x$ that matter, are those under the support of $c$.  Suppose that
under that support there are $s$ 1's and $s-1$ -1's.  This means that
$2s-1 \le r$, where $r$ is the weight $c$.  So this can't work since,
if there's at least 1, -1 left outside of the support, it can go into
every coordinate position.  So an alternative would be to show that
the *number* of resolved $x$'s never decreases.  In particular, this
would show that for every good $x$ that's killed, at least one bad $x$
will be made good.  So, the situation is this: there are $s$ 1's and
$s-1$ -1's under the support of $c$, and $j$ is an index outside of
the support of $c$ such that there are $x$, when restricted to $c$ has
the pattern given, but $x_j = -1$.  Since $x$ is balanced, its sum
outside of the support of $c$ is $-1$.  The vectors that are bad of
$c$ are those with $t$ 1's and $t$ -1's under the support of $c$.  All
we have to do is to show that there is one such with a $\pm 1$ in
position $j$.  If $2s-1 < r$ then we can always move a -1 from outside
of the support of $c$ to the inside of the support of $c$, since $r
\le n/2$.  More specifically, there is always an $x$ with maximal
weight: $a$ 1's and $a$ -1's where $a = \lfloor n / 2 \rfloor$.
position under the support of $c$.

We want to show that for every $c$, such that $\wt(c) < \lfloor n/2
\rfloor$, there is a map $f$ from the set $S := \{x : c \cdot x = 1\}$ to
the set $T := \{x : c \cdot x = 0\}$ with the property that if $x_j =
-1, x \in S$ for $c_j = 0$ then $f(x)_j \ne 0$.

Here's a strategy: if the weight of $x$ is large enough, there will
always be a guarantee of a $+1$ outside of the support of $c$ in
addition to the $-1$.  We can then consider the vector that swaps
those two.  So we only have the case that the only nonzeros outside of
the support of $c$ are $-1$'s.  Since, by choice, this means that
there is precisely one $-1$.  The extreme case happens if the support
of $c$ has size 1.  When we set a position outside the support of $c$
from 0 to 1 we actually make a lot of bad vectors good: Any vector
whose support is disjoint from $c$, and any vector which isn't
balanced in $c \cup \{j\}$.  So we just need to calculate the
difference between the number of balanced vectors in $c \cup \{j\}$
and the unbalanced vectors in $c \cup \{j\}$.

Let $B(r)$ be the number of balanced vectors of length $r$.
At the beginning there are $B(n) - B(r)$ good vectors.
After setting $r \leftarrow r+1$ there are $B(n) - B(r+1)$ good
vectors.  So as long as $B(r+1) \le B(r)$ we've won.  But this ignores
the interaction with the other rows, since it is not true that the set
of good vectors is increasing, just their number.   We partition the
sets of columns into $P_1 \cup P_2 \cup \dots \cup P_s$ so that the
columns in each $P_i$ are constant (i.e. all have the same value).
Changing a 0 to a 1 in some position, $j$, will remove one column from the
part containing $j$ and either make a new part of size 1 or merge it
into another part.

A part is "pure" if all the elements are = 1, otherwise it's
"impure".  If all parts are impure then the set of bad vectors must
all be balanced within in each part (since all 0's kills everything).
Is this true?

* A False theorem

If $m$ is a positive integer, denotes by $[m] = \{1, \dots, m\}$.  If
p$U$ is a finite set, and $S_1, \dots, S_m \subseteq$.  If $T \subseteq
[m]$, denote by $B_m[V,T] = \bigcap_{j \in T} \cap \bigcap_{j \in
[m]\backslash T} (V \cap S_j) \left( V \cap (V \backslash S_j)\right)$.  If $x:U
\rightarrow A$, where $A$ is an additive abelian group, and $S
\subseteq U$, denote by $x(S) = \sum_{a \in S} x(a)$.

Proposition: Let $U$ be a finite set, $S_1, \dots, S_m \subseteq U$,
and $x:U \rightarrow A$, where $A$ is an additive abelian group.  Then
$x(U) = 0$ and $x(S_j) = 0$ for all $1 \le j \le m$ if and only if
$x(B_m[U,T]) = 0$ for all $T \subseteq [m]$.

Proof: The "only if" part is straightforward, since, for all $j$ we
have $S_j = \bigcup_{T \subset [m], j \in T} B_m[U,T]$, where the
union is disjoint.  For the "if" part we prove the statement by
induction on $m$.  If $m=1$, note that $x(U) = x(U \backslash S_1) +
x(S_1) = x(B_1[U,\emptyset]) + x(B_1[U,\{1\}])$. If $m > 1$, if $T
\subseteq [m-1]$, by induction, we have $x(B_{m-1}[U,T]) = 0$.
We have $x(B_{m-1}[U,T]) = x(B_{m-1}[U,T] \cap S_m) + x(B_{m-1}[U,T]
\backslash S_m)$. [This is what is wrong: it is not necessarily true
that the first term on the right hand side is 0].

Perhaps we can apply it to the case where the universe is $S_m$ and
the subsets are $S_m \cap S_1, \dots, S_m \cap S_{m-1}$.

$$\begin{bmatrix}
0&0&0&0 \\
1&1&0&0 \\
1&0&1&0
\end{bmatrix}$$

$$\begin{array}{ccccc}
x_1 &+ x_2 &+ x_3 &+ x_4 = &0 \\
x_1 &+ x_2 &           &         =  &0 \\
x_1 &          &+ x_3  &         = &0 
\end{array}$$

$$\begin{array}{ccc}
x_3 &+ x_4 &= 0 \\
x_1 &+ x_2 &= 0 \\
x_2 &- x_3  &= 0 \\
\end{array}$$

$$\Rightarrow x_1 = -x_2 = -x_3 = x_4$$

What I'd like to show is that if a nonzero row has weight $< \lfloor
n/2 \rfloor$, that there is a position in that row containing a 0,
which we can make a 1, but not allow more 0 sums in the other rows.
* <2023-07-17 Mon> Inclusion-Exclusion
In the ping-pong method, a possible resolving set, $A$, is proposed,
and the constrainer tries to look for one or more balanced $0, \pm 1$
vectors whose dot product with at least one row of $A$ is 0.  The
question is, what is the best such vector to use?  Right now, I give a
bound on the number of such vectors, and have the constrainer yield
that many vectors (or less, if that's all there are).  Perhaps we
should find a vector which maximizes the number of rows which are
annihilated by such vectors.

On the converse side, given a possible row of $A$ one can easily
calculate the number of balanced vectors annihiliated by that vector.
It only depends on the weight of the vector.  In a more complicated
calculation, one can calculate the number of vectors that are
annihilated by two different rows.  So, by inclusion-exclusion, one
can explictitly calculate the number of vectors annihilated by at
least one such row.  For any number of rows, $r$ one can make this
calculation, but now there are approximately $2^r$ cases.  Actually
more.

On a related note, one can calculate the lower bounds given by
Pippenger as a sum of binomial coefficients, to, perhaps, get a
stronger lower bound than the asymptotic analysis provides.

Another question: empirically, I've noticed that only a fairly small
set of balanced vectors is sufficient to prove that there is no $m
\times n$ array $A$ which is resolving.  For example, for $n=10$ and
$m=6$, there are about 86000 balanced vectors, but a set of about 300
is sufficient.  The question is: is there a graph derived from the
balanced vectors, so that a sufficient set corresponds to a vertex
cover of that graph?  In particular, is there a relation $R(v,w)$ so
that every set that resolves $v$ will also resolve $w$?  This would
mean that if $Av \ne 0$ then $Aw \ne 0$.  Taking the contrapositive
(which is easier to work with), if $Aw \eq 0$ then $A v \eq 0$?  In
linear algebra terms, we have $\dim \ker([v]) + \dim \ker([w])$.
Actually, linear algebra is not really the correct thing.  In terms of
sets we have the following: $v$ and $w$ correspond to pair of sets,
$v_1, v_2$ and $w_1, w_2$.  Satisfying the following:

1) $\# v_1 = \# v_2$ (same for $w$)
2) $v_1 \cap v_2 = \emptyset$
3) $v_1, v_2 \ne \emptyset$.

Given such a $v$ is there a $w$ satisfying the following:

$\forall s$ such that $\#(v_1 \cap s) = \#(v_2 \cap s)$
is it true that $\#(w_1 \cap s) = \#(w_2 \cap s)$.

We can try to calculate this.  Fix $v$ and then pose the calculation:

$\exists (w_1, w_2), \phi(w_1, w_2) \& \psi(w_1, w_2, v_1, v_2)
\forall s $\#(v_1\cap s) = \#(v_2 \cap s) \& \#(w_1 \cap s) = \#(w_2
\cap s) $, where $\phi(w_1, w_2)$ encodes being balanced, and $\psi$
encodes being $w$ different from $v$.

* <2023-07-21 Fri> Points for the talk
+ Relation to compressed sensing: recall that *compressed sensing*
  asks to recover a vector from the values of an undetermined linear
  linear operator acting on the the vector, along with *side
  information*, such as sparsity, or limits on the sizes of
  coordinates.
+ In this case, the metric dimension problem can be recast as follows:
FInd an $m \times n$ 0/1 matrix so that the the sums of all subsets of
columns of length $\lfloor n/2 \rfloor$ are distinct. We'll discuss
how that comes about.
+ In the case of metric dimension, we've shown that to test if the
  metric dimension of the $n$ dimensional hypercube is $\ge m$.  It
  suffices to produce an $(m-1) \times n$ 0/1 matrix, $A$, all of whose
  rows are nonzero, such that $Ax \ne 0$ for all nonzero $x$ whose coordinates
  are 0/1/-1 and $e^T x$, where $e$ denotes the all 1's vector.  The
  *sensing matrix* in this case will be the matrix, $A'$, obtained by
  adjoining a row of 1's to the top of $A$.  In this case, every
  $n$ dimensional 0/1 vector, $y$, is uniquely determined by $A' y$.
+ There is a closely related concept of a *separable matrix* which has
  applications in *group testing* and *cryptographic fingerprinting*.
  It's a 0/1 matrix with the property that the logical OR of all
  subsets of columns of length $\lfloor n/2 \rfloor$ are distinct.
+ The problem may be posed as a 2-level quantified formula:
$\forall a , f(a) \exists x, (\neg g(x) \vee \neg phi(a,x))$,
where $f(a)$ is a predicate restricting $a$ values (this will be used
in symmetry-breaking), $g(x)$ is a predicate on the $x$ values
defining the allowed values, and $\phi(a,x)$ is a predicate which says
when the $a$ values, corresponding to a putative matrix, do not
resolve the $x$ values.  We solve this sort of problem by having two
cooperating SAT solvers.  One, which we call the *resolver* contains
$f(a)\wege \neg phi(a,x_1) \wedge \dots \wedge, \neg \phi(x,a_t)$, for some values
$x_1, \dots, x_t$.  At the beginning, $t=0$.  The resolver finds one,
or more solutions.  If there are none, the problem is UNSAT,  If it
finds solutions, it the passes them to the *conflictor* which has
$g(x) \wedge \phi(x,a)$.  It then adjoins (via assumptions) each of
the solutions passed to it, and produces one more $x_i$ to give back
to the resolver. 

In this problem, we make heavy use of *cardinality constraints*
involved in calculating the Hamming weights of various 0/1 vectors.

* <2023-07-24 Mon> Symmetries

The problem, after reductions, is the following:

We're given $m < n$ positive integers.  We wish to find (or to prove
that there is no such) an $m \times n$ matrix $A$ whose entries are
all in 0/1, and rows all nonzero, with the following property: For all
*balanced* 0/1/-1 vectors $x$, we have $Ax \ne 0$.  Here balanced
means that the vector is nonzero and the sum of its coordinates is 0.
This has a fairly straightforward expression in SMT in terms of
bitvectors as a "exists/forall" problem.  However, the solution set
has a large group of symmetries.  Here are the symmetries:

1) If $A$ has the desired property, then any permutation of its rows
   also has the desired property.
2) If $A$ has the desired property, then any permutation of its
   columns has the desired property.  This follows from the fact that
   any permutation of coordinates of a balanced vector is also
   balanced.
3) If $A$ has the desired property, then if we complement any rows of
   $A$, it has the desired property.  This follows from the fact that
   the sum of coordinates of a balanced vector is 0.

Altogether this yields $2^m m! n!$ symmetries.  The symmetries
described in (3) are easily taken care of, by, for example,
restricting the weight of each row of $A$ to be $\le n/2$.

First, there is a simple observation that we can assume that all of
the rows of $A$ are distinct.  Thus we can break the symmetries in (1)
by requiring that the rows of $A$ are strictly increasing in some
convenient total order.  We might choose lexicographic order (but some
other choices might work better).

There is a fairly large literature about breaking symmetries given in
matrices with conditions (1) and (2).  One might proceed as follows:
first sort the rows of $A$, then sort the columns of $A$.  However,
this might "mess up" the sorted order of $A$.  One can continue
alternating the sorts.  It is not immediately clear that this process
terminates, but it does!  However, it is *not true* that two distinct
"doubly sorted" matrices can't be equivalent under row/column
permutations.  In fact, there can be a large number of such classes
[give reference].

* Ping Pong

The problem at hand can be described in the following way: we have two
set of variables, the $a$ variables, which describe the matrix $A$,
and the $x$ variables which describe the balanaced vectors.  In
addition, there is condition $\phi(a,x)$ which must be satisfied.  We
also have restricted predicate, $f(a)$ on the $a$ variables, and a
restricted predicate, $g(x)$ on the $x$ variables.  The predicate
$f(a)$ will be used for symmetry breaking.  That is, it will restrict
our consideration of the $a$ variables to be representatives in some
class under symmetry.  The predicate $g(x)$ will describe the balanced
vectors.  We find it convenient, in terms of bitvectors, to describe
the $x$ vector as a pair of vectors $x1$ and $x2$, such that $x1 \& x2
= 0$, and $w(x1) = w(x2)$, where $w$ is the *Hamming weight* of a 0/1
vector - the sum of its elements. The problem may then be described
as:

$\exists x, f(x) \wedge \forall g(x) \Rightarrow \phi(a,x)$.

The *ping pong* method instantiates two SAT solvers: the *resolver*
and the *conflictor*.  Since SAT solving solves a $\exists$ problem,
we rewrite the above as

$\exists x, f(x) \wedge \neg (\exists g(x) \wedge \neg \phi(a,x))$.

The resolver keeps the formula $f(a) \wedge \phi(a,x_1) \wedge \dots
\wedge \phi(a,x_t)$, where $x_1, \dots, x_t$ are specific values of
the $x$ variables supplied by the conflictor.  At the beginning, $t=0$
(i.e. there are no $\phi(x,a)$ conjuncts).  At each stage the resolver
either finds a value of the $a$ variables satisfying its clauses, or
reports that there is none.  In the latter case the whole problem is
UNSAT.  In the former case the resolver then passes one, or more, $a$
variable values to the conflictor.    For each such, the conflictor
either finds one or more values of $x$ variables satisfying its
clauses, or reports that there are none.  In the latter case the whole
problem is SAT.  In the former case, the values are passed to the
resolver to add more $\phi(a,x_i)$ clauses.  This must eventually
terminate, since there are only al finite number of balanced vectors.

The upshot is that the above architecture was implemented using the
=Python= library =python-sat= and the SAT solver =cadical 1.53=.  It
can show that the problem with $m=6$ and $n=10$ is UNSAT in 5 seconds
on an Apple M2 laptop.  By contrast, the best performance for an SMT
solver on this problem is 75 seconds for =boolector= on the same
laptop, using the same symmetry breaking constraints.

As a comment, the overall structure of resolver/conflictor is the same
sort of architecture used in a number solvers.  The input is normally
presented in either =SMTLIB= format or =QDIMACS= (or others).  The solvers
then need to extract things like the predicates $f(a)$ and $g(x)$ plus
the specification $\phi(a,x)$ from the input specification.

The symmetries may be broken in a number of ways for this problem.  As
an aside, symmetry breaking is a highly researched area.  On the
negative side, it is shown that the general problem of total symmetry
breaking is NP-hard.  

* <2023-08-01 Tue> Group Representations
Question: Can we use group representations to systematically generated
symmetry breaking constraints?  More specifically, we have the
generalized Fourier expansion, where $g \in G$, and $G$ is the group
in question, and $f$ is a complex valued function (or real valued):
$f(g) = \sum_{\rho}  \langle \pho(g), A_\rho \rangle$,
where $A_\rho = \frac{1}{|G|} \sum_{g \in G} \rho(g) f(g)$,
and the $\rho$ that occur are representatives of each class of
irreducible repersentations.  Here $\langle A, B \rangle := \Tr(A^*
B)$, and $\rho$ is unitary.  We can truncate the above series to get a
good approximation to $f(g)$.  For symmetry breaking which functions
$f$ should we use?

Many of the symmetry breaking proposals use the function:
$f(g) := \sum_{i=0}^{N-1} 2^i x_i^g$, where $x_i$ are the boolean
variables encoding the problem.  We are interested in finding $g$ so
that $f(g) = \min_{h \in G} f(h)$.  More generally, if $g$ acts on
literals, we encode $x_i$ in $\pm 1$, and so $x_i^g$ will be a sign
times some $x_j$.  So we can get linear functions by projecting onto
the irreducible invariant subspaces.

* <2023-08-02 Wed> Notes for the talk
** General Metric Spaces
Recall that a metric space, $S$ is a set with a map $d : S \times S
\rightarrow \RR_{\ge 0}$ such that
1) For all $x \in S$, $d(x,x) = 0$.
2) For all $x \ne y \in S$, $d(x,y) = d(y,x) > 0$.
3) For all $x,y,z \in S$, $d(x,y) \le d(x,z) + d(z,y)$ (The "triangle
   inequality")

Note that if $G$ is a simple, connected, undirected graph, then
$d_G(x,y) = d(x,y)$, the length of the shortest path from vertices $x$
to $y$, in $G$ is a metric.

A subset $A \subseteq S$ is *resolving* if for all $x \ne y \in S$,
there is a $z \in S$ such that $d(x,z) \ne d(y,z)$.

The *metric dimension* of $S$ is the cardinality of the smallest
resolving set in $S$.

In Garey and Johnson ("Computers and Intractability") they mention
that finding the metric dimension of a general graph is NP-complete.

** Metric dimension of $n$-dimensional Euclidean space
The metric dimension of $\RR^n$ under the Euclidean norm is $n+1$.  We
can see this as follows: Let $H$ be an affine subspace of dimension
$n-1$.  Then $H$ is of the form $\{ x \in \RR^n : \langle a,  x \rangle = b\}$, for
some $a \in \RR^n$ with $||a|| = 1$, and $b \in \RR$.  Let $c \ne 0$.
Then the points $(b \pm c)a$ have the same distance from any point in
$H$, and thus cannot be resolved by them.  Namely, if $x \in H$, then
\begin{displaymath}
\begin{aligned}  ||x - (b+c) a||^2 & = ||x||^2 - 2(b+c) b + (b+c)^2 ||a||^2 
\\ & = ||x||^2 +b^2 + 2bc + c^2 - 2bc - 2b^2
\\ & = ||x||^2 - b^2 - c^2 $. 
\end{aligned}
\end{displaymath}
Thus $(b+c)a$
and $(b-c)a$ have the same distance from any point in $H$.
If $x^{(1)}, \dots, x^{(n)} \in \RR^n$, there is some affine space
$H$, as above, containing all $x^{(i)}$ showing that they can't be a
metric basis.  Namely, let $a \in \RR^n$ with $||a||=1$ satisfy
$\langle x^{(i)} - x^{(1)}, a \rangle = 0$ for $i=2, \dots, n$.  Such
an $a$ exists, since there are only $n-1$ such conditions.  Then we
may take $H = \{x \in \RR^n : a^T x = - \langle x^{(1)}, a \rangle \}$.

On the other hand, the $n+1$ points $e^{(i)}$, and $0$, where
$e^{(i)}$ is the unit vector with 1 in the $i$-th coordinate and 0
elsewhere, do constitute a metric basis.  Namely $||x - e^{(i)}||^2
= ||x||^2 + 1 - 2 x_i$.  Thus, knowing $||x||^2 = ||x-0||^2$ and
$||x-e^{(i)}||^2$ detemines $x_i$ for all $i$, and thus determines
$x$.
* Minimality check
In the above, we have used the fact that if $S$ is a resolving set for
a graph $G$, and $g \in \Aut(G)$, then $S^g$ is also a resolving set.
For the hypercube, if we have a resolving set $S$, choose $s \in S$,
and take $g$ to be the translation by $s$.  Thus $0 \in S^g$.  Let $T
= S^g \backslash \{0\}$.  If $S$ does not resolve $G$, there are
vertices $u \ne v \in V(G)$ such that $w(s \oplus u) = w(s \oplus
v)$ for all $s \in S$.  If $t \in S$, then $w((s\oplus t) \oplus (u
\oplus s)) = w((s \oplus t) \oplus (v \oplus s))$. Thus $((u \oplus
s), (v \oplus s))$ is not resolved by $T$.  But, we want the converse.

Let $A$ denotes any 0/1 $k \times n$ matrix.  if $0 \ne x \in
\{0,1,-1\}^n, e^T x = 0$ and $Ax = 0$.  Note that $x$ as the
difference of two 0/1 vectors $u,v$ will then satisfy $w(u) = w(v)$.

* Bounds
From a paper by Dankelmann, Morgan and Rivett-Carnac
[cite:@dankelmann2023metric]. If an $n$ vertex
graph is bipartite of diameter $D$, and metric dimension $k$ then
\begin{displaymath}
n \le
\begin{cases}
k \sum_{i=0}^{(D-3)/3} (i+1)^{k-1} + \left(\frac{D+3}{3}\right)^k
+ \left(\frac{D}{3}\right)^k & \text{if } D \equiv 0 \pmod 3 \\
k \sum_{i=0}^{(D-4)/3} (i+1)^{k-1} + 2\left(\frac{D+2}{3}\right)^k
 & \text{if } D \equiv 1 \pmod 3 \\
k \sum_{i=0}^{(D-2)/3} (i+1)^{k-1} + 2\left(\frac{D+1}{3}\right)^k
 & \text{if } D \equiv 2 \pmod 3
\end{cases}
\end{displaymath}
#+print_bibliography:
