\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\title{Resolving the Hypercube} \author{Victor S. Miller}
\newcommand{\secref}[1]{theorem~\ref{sec:#1}}
\renewcommand{\eqref}[1]{equation~\ref{eq:#1}}
\newcommand{\thmref}[1]{theorem~\ref{thm:#1}}
\newcommand{\defref}[1]{definition~\ref{def:#1}}
\newcommand{\propref}[1]{proposition~\ref{prop:#1}}
\newcommand{\lemref}[1]{lemma~\ref{lem:#1}}
\newcommand{\corref}[1]{corollary~\ref{cor:#1}}
\DeclareMathOperator{\Aut}{Aut}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cA}{\mathcal{A}}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem{problem}{Problem}
\begin{document}
\maketitle
\begin{abstract}
  The \emph{metric dimension} of a graph is a measure of how easy it
  is to distinguish vertices given distance information.  It is the
  smallest cardinality of a subset of vertices so that the set of
  distances from those nodes uniquely determines any other vertex.
  Finding the metric dimension is, in general, an NP-complete problem.
  Nevertheless it is of some interest to determine its exact value for
  various families of graphs.  In this note we discuss the use of
  \emph{MaxSat} solvers, and cooperating SAT and SMT solvers in
  determining the metric dimension of the \emph{hypercube}.  Since
  these graphs have a large symmetry group it is of great practical
  importance to make use of the these symmetries to speed up the
  computation.
\end{abstract}

\section{Introduction}
\label{sec:intro}

The concept of \emph{metric dimension} was introduced, independently,
by Harary and Melter \cite{harary1976metric}, and Slater
\cite{slater1975leaves} as an attempt to generalize the continuous
concept of dimension to discrete spaces.  They noted that in the real
Euclidean space $\RR^n$, there is a set of $n+1$ points, so that
every point in $\RR^n$ is uniquely determined by its distances from
those points, and that $n+1$ is the minimum possible value for the
size of such a set.  In fact the set may be taken to be the 0 vector,
along with the $n$ vectors which have exactly one non-zero coordinate
equal to 1.  Given any metric space one may define the same concept.
In particular, a finite undirected connected graph is a metric space
with the respect to the shortest distance between two vertices in the
graph.

Since the original papers, there have been a large number of works
concerning metric dimension of various graphs.  On one hand, it is
shown that the general problem of computing the metric dimension of a
graph is NP-complete
\cite[GT61]{garey1979computers,khuller1996landmarks,diaz2012complexity,hauptmann2012approximation}.

On the other hand, it is not necessarily true that it is NP-hard to
calculate the metric dimension for some interesting
families of graphs.  In particular, we treat the family of
\emph{hypercube} graphs.

In this section we define some basic terminology: \emph{resolving set}
and \emph{metric dimension}.

\begin{definition}
  Let $X$ be a metric space, with metric $d$.  Given $x \ne y \in X$,
  we say that $s \in X$ \emph{resolves} the pair $\{x,y\}$ if $d(s,x)
  \ne d(s,y)$.  A subset $S \subseteq X$ is a \emph{resolving set} if
  every pair of vertices is resolved by some element of $S$.  Note
  that $X$ is, itself, a resolving set.  The \emph{metric dimension}
  of $X$ is the smallest cardinality of a resolving set.
\end{definition}

Note that if $G$ is a finite, connected, undirected graph, it is
naturally a metric space, with the metric $d_G(x,y)$ the length of the
shortest path in $G$ between $x$ and $y$.  When we speak of the metric
dimension of a graph, it is with respect to the metric $d_G$.

The metric dimension of $G$ is usually denoted by $\beta(G)$, although
some authors use the notation $\mu(G)$.

The problem which we consider here is to calculate the metric
dimension of the \emph{Hypercube}.  This problem was first considered
as ``Problem B'' in \cite{erdos1963two}.  A related problem is
``Problem A''\footnote{This problem was first posed in
  \cite{shapiro1963combinatory}} in the same paper.  The two problems
have a close interplay.

\begin{definition}[Hamming Distance]
  Let $S$ be a finite set, and $x,y \in S^n$ be $n$-tuples of elements
  of $S$.  The \emph{Hamming distance} $d_H(x,y)$ is the number of
  indices, $i \in \{1, \dots, n\}$ such that $x_i \ne y_i$.
\end{definition}

\begin{definition}[Labeled Graph]
  An undirected \emph{labeled graph} $G$ is an unidirected graph along
  with a map $\ell: V(G) \rightarrow L$, where $L$ is some set of
  labels.
\end{definition}
We often abuse notation, when the context is clear, by omitting the
map $\ell$.
\begin{definition}[Weak Product]
Let $G, H$ denote finite undirected graphs.  Their \emph{weak product}
$G \square H$ is the graph such that $V(G \square H) = V(G) \times V(H)$,
and $\{(x,y),(x'y')\} \in E(G \square H)$ if and only if either $(x,x')
\in E(G)$ and $y=y'$ or $x=x'$ and $(y,y') \in E(H)$.
\end{definition}
\begin{definition}[Hypercube]
  Let $n$ be a positive integer. The $n$-dimensional \emph{hypercube}
  $Q^n$ is the $n$-fold weak product $K_2 \square \cdots \square K_2C$ where
  $K_2$ is complete graph on two vertices.  Another way of seeing
  $Q^n$ is the undirected graph whose vertices are are uniquely
  labeled by the set $\{0,1\}^n$ of $n$-tuples of 0/1.  Two such
  vertices are connected by an edge if and only if the Hamming
  distance between the labels of the vertices is 1.
\end{definition}
Note that if $x,y \in V(Q^n)$ are two vertices then $d_{Q^n}(x,y) =
d_H(\ell(x),\ell(y))$, the Hamming distance.
\begin{definition}[Weight]
  Let $S$ be a finite set with a distinguished element, denoted by 0.
  The \emph{weight} of an $n$-tuple of elements of $S$, $x$ is the number
  of coordinates of $x$ which are not 0.  Denote the weight of $x$ by $w(x)$.
\end{definition}

If $x,y \in V(Q^n)$ then $d_{Q^n}(x,y) = w(\ell(x) \oplus \ell(y))$, where $\ell(x)
\oplus \ell(y)$ is the coordinatewise sum of the elements of $\ell(x)$ and $\ell(y)$
taken modulo 2.  If $u \cdot v$ denotes the coordinatewise product of $u$
and $v$, then we have $w(u \oplus v) = w(u) + w(v) - 2 w(u \cdot v)$.

\begin{definition}[Automorphism Group]
  Let $G$ be a finite undirected graph.  An \emph{automorphism}, of
  $G$ is a one-to-one map $\phi: V(G) \rightarrow V(G)$ with the
  property that if $(x,y) \in E(G)$ then
  $(\phi(x), \phi(y)) \in E(G)$.  The set of automorphisms forms a
  group under composition, and is denoted by $\Aut(G)$.  If $G$ has
  the property that given $x, y \in V(G)$ there is an automorphism,
  $\phi$ such that $\phi(x) = y$, then $G$ is said to be \emph{vertex
    transitive}.
\end{definition}
\begin{definition}
  If $\sigma \in \cS_n$ is a permutation of $\{1, \dots, n\}$ and $x \in
  T^n$ for some set $T$, then define $x^\sigma$ by $(x^\sigma)_i =
  x_{\sigma^{-1}(i)}$ for $i=1, \dots, n$.  Note that if $\sigma, \tau
  \in \cS_n$, then $x^{\sigma \tau} = (x^{\sigma})^\tau$.  Namely
  $(x^{\sigma \tau})_i = x_{(\sigma \tau)^{-1}(i)} = x^{\tau^{-1}
    (\sigma^{-1}(i))} = ((x^\sigma)^\tau)_i$.
\end{definition}
\begin{definition}[Hyperoctahedral Group]
The \emph{hyperoctahedral group} of dimension $n$ is (in concrete
form) the set of all maps $\{0,1\}^n \rightarrow \{0,1\}^n$ of the
form $(x,\sigma)$ where $x \in \{0,1\}^n$ and $\sigma \in S_n$,
permutations of $\{1, \dots, n\}$, where $(x, \sigma) (y) = x \oplus
y^\sigma$.
  
\end{definition}
Note: The hyperoctahedral group of dimension $n$ is $\Aut(Q^n)$.

\begin{definition}[Distance Transitive]
Let $G$ be a finite undirected graph.  It is \emph{distance
  transitive} if given $x,y,z,w \in V(G)$ with $d_G(x,y) = d_G(z,w)$
there is an automorphism $\phi \in \Aut(G)$ such that $\phi(x) = z, \phi(y) = w$.
  
\end{definition}
Note that the hypercube $Q^n$ is distance transitive

\begin{lemma}[Distance Transitivity of the Hypercube]
  The hypercube $Q^n$ is distance transitive.
\end{lemma}
\begin{proof}
  Let $u,v,x,y,z,w \in V(Q^n)$ be such that $d_{Q^n}(u,v) = d_{Q^n}(x,y)$.
  By definition of $d_{Q^n}$, we have $w(u \oplus v) = w(x \oplus y)$.
  Thus there is a permutation, $\sigma$ of $[n]$ such that
  $(u \oplus v)^\sigma = x \oplus y$.  Let $\alpha = u^\sigma \oplus
  x$.  Then, we have, $v^\sigma \oplus \alpha = y$.  Namely, we have
  $x \oplus y = (u \oplus v)^\sigma = u^\sigma \oplus v^\sigma$.  Thus
  $y = x \oplus u^\sigma \oplus y = \alpha \oplus y$.
\end{proof}

\section{Detecting Matrices}
\label{sec:detecting}

\begin{definition}
  Let $n$ be a positive integer.  The set of \emph{test vectors}
  $T(n) := \{ x \in \{0,1,-1\}^n : x \ne 0\}$ is the set of nonzero $n$-vectors
whose coordinates are in the set $\{0,1,-1\}$.  The set of
\emph{balanced test vectors}, $B(n)$ is the subset of $T(n)$ whose sum
of coordinates is 0.  If $m \le n/2$ let $B_m(n)$ denote the subset of
$B(n)$ consisting of those vectors which have exactly $m$ 1's as their
coordinates.  It's straightforward to see that
\begin{displaymath}
  \# B_m(n) = \binom{n}{m} \binom{n-m}{m}.
\end{displaymath}
Namely, there are $\binom{n}{m}$ places to put a 1, and
$\binom{n-m}{m}$ remaining places to put a -1.
\end{definition}
\begin{definition}
\label{def:detecting}
  Let $m,n$ be positive integers.  An $m \times n$ 0/1 matrix, $A$,
  with all rows nonzero and distinct, is \emph{detecting}, if
  $Ax \ne 0$ for all $x \in T(n)$.  It is \emph{balanced detecting} if
  $Ax \ne 0$ for all $x \in B(n)$.
\end{definition}
Observe that if $A$ is a balanced detecting matrix, then the matrix
$A'$ which consists of the matrix $A$ with the addition of a row of
all 1's is detecting.
\begin{theorem}
  \label{thm:detecting}
  Let $n$ be a positive integer, and $S$ be a resolving subset of
  $Q^n$ containing 0. Then, the matrix, $A$, whose rows consist of the
  nonzero element of $S$ is balanced detecting.  Conversely, if $A$ is
  a balanced detecting matrix, then the set $S$ consisting of the 0
  vector along with the rows of $A$ is a resolving subset of $Q^n$.
\end{theorem}
\begin{proof}
  If the subset $S$ is resolving and $A$ is the constructed matrix it
  is balanced detecting.  Namely, for every $u \ne v \in
  Q^n$ with $w(u) = w(v)$ satsifies $A_i \cdot u \ne A_i \cdot v$ for
  some $i$.  However, $w(A_i \oplus u) - w(A_i \oplus v) = w(A_i) +
  w(u) - 2 w(A_i \cdot u) - (w(A_i) +
  w(v) - 2 w(A_i \cdot v)) = 2(w(A_i \cdot u) - w(A_i \cdot v)) \ne
  0$.  Thus $A_i$ resolves $(u,v)$.  But the vector $0$ resolves those
  $(u,v)$ of the same weight.  Conversely, if $A$ is balanced
  detecting we use the same calculation to resolve all pairs $(u,v)$
  of distinct weights, and use $0$ to resolve pairs of the same weight.
\end{proof}
\begin{theorem}
\label{thm:symmetry}
  Let $m,n$ be positive integers, and $A$ be an $m \times n$ balanced
  detecting matrix.  Then the matrix $A'$ such that $A'_1 = A_1$, and
  $A'_i = A_1 \oplus A_i$ for all $i > 1$ is balanced detecting.
\end{theorem}
\begin{proof}
  If $A$ is a balanced detecting matrix, by \thmref{detecting} the set
  $S$ consisting of the $0$ vector along with the vectors given by the
  rows of $A$ is resolving.  But then the set $S' = \{ A_1, 0 \} \cup \{
  A_i \oplus A_1, i=2,\dots, m\}$ is also resolving, obtained by
  XORing each element with $A_1$.  But the matrix $A'$ is obtained by
  omitting $0$ from $S'$, and is thus balanced detecting,
\end{proof}
\section{Symmetry}
\label{sec:symmetry}

The detecting matrix version of the problem has a very large symmetry
group. Using \emph{symmetry breaking} is essential for the
calculation.

By \thmref{symmetry} the following matrices generate symmetries of a
balanced detecting matrix
\begin{equation}
  \label{eq:generator}
  \begin{pmatrix}
    1 & 0 & 0 & \dots & 0 \\
    1 & 1 & 0 & \dots & 0 \\
    & & \vdots \\
    1 & 0 & 0 & \dots & 1
  \end{pmatrix}
\end{equation}

\begin{proposition}
  The group generated by the $n \times n$ permutation matrix and the
  matrix given in \eqref{generator} is isomorphic to $S_{n+1}$.
\end{proposition}
\begin{proof}
  The Coxeter generators of the symmetric group are the transpositions
  $\sigma_i := (i, i+1)$ for $1 \le i < n$, with the relations
  $\sigma_i^2 = 1$ for all $i$, $\sigma_i \sigma_j = \sigma_j
  \sigma_i$ if $|i-j| > 1$ and $(\sigma_i \sigma_{i+1})^3 = 1$.
\end{proof}

\section{Reductions}
\label{sec:reductions}

In this section we discuss reductions of the coin-weighing problem and
the metric dimension for the hypercube.  In particular, we show that
each problem is equivalent to constructing certain sets of 0/1
matrices.

\begin{problem}{Subset distinguishing}
We are given an unknown subset $S \subset \{1, \dots, n\}$.  We are
allowed to take \emph{measurements} of $S$ by choosing sets $X
\subseteq \{1, \dots, n\}$.  The information gained from $X$ is the
cardinality $\# (X \cap S)$.  A collection of subsets $\cS$ of $\{1, \dots,
n\}$ is \emph{distinguishing}, if $\# (X \cap S)$ for all $X \in \cS$
is sufficient to determine $S$.
\end{problem}

This problem (in the guise of coin weighing) is Problem A in
\cite{erdos1963two}.

We can encode the collection $\cS$ in terms of a 0/1 matrix.  The rows
of an $m \times n$ 0/1 matrix $A$ are the characteristic vectors of
the subsets.  Call such a matrix \emph{distinguishing} if for all $x
\ne y \in \{0,1\}^n$, we have $Ax \ne Ay$.  Denote by $\gamma_n$ the
minimum value of $m$ such that there exists a distinguishing $m \times
n$ 0/1 matrix.  Note that $A$ is distinguishing if and only if $Ax =
0$ implies that $x = 0$, where $x \in \{0, -1, 1\}$.

\begin{problem}[Resolving set for the hypercube]
A subset $S \subset \{0,1\}^n$ is \emph{resolving} if for all $x \ne y
\in \{0,1\}^n$ there is an $a \in S$ such that $w(a \oplus x) \ne w(a
\oplus y)$.  Here $w(x) := \sum_{i=1}^n x_i$ and $x \oplus y$ is the
vector $(x \oplus y)_i = x_i \oplus y_i$, where $\alpha \oplus \beta =
0$ if $\alpha = \beta$ and 1 otherwise.
\end{problem}
As with the set distinguishing problem we may state this in terms of
matrices, whose rows are the characteristic vectors of the members of
$S$.

We use the following notation: if $\alpha, \beta \in \{0,1\}$, denote
by $\alpha \oplus \beta = 0$ if $\alpha = \beta$ and 1 otherwise.  If
$x, y \in \{0,1\}^n$ denote by $x \oplus y$ the vector such that $(x
\oplus y)_i = x_i \oplus y_i$ for all $i$, and if $A \in \{0,1\}^{m
  \times n}$, $A \oplus x$ is the vector such that $(A \oplus x)_i =
A_i \oplus x$ for all $i$, where $A_i$ denotes the $i$-th row of $A$. 

We note that $\alpha \oplus \beta = \alpha + \beta - 2 \alpha \beta$
for $\alpha, \beta \in \{0,1\}$, and $\oplus$ is commutative and associative.

Note that if $S$ is a resolving set then so is $S \oplus a := \{x
\oplus a : x \in S\}$, for any $a \in \{0,1\}^n$.  Namely if $x \ne y
\in \{0,1\}^n$, we have $x \oplus a \ne y \oplus a$.  Therefore, by
definition of a resolving set, there is a $b \in S$ so that $w(b
\oplus (x \oplus a)) \ne w(b \oplus (y \oplus a))$.  However, we have
$b \oplus (x \oplus a) = (a \oplus b) \oplus x$ and
$b \oplus (y \oplus a) = (a \oplus b) \oplus y$. Therefore $a \oplus
b$ resolves $(x,y)$.  Thus, without loss of generality, we may assume
that $0 \in S$.

If $A$ is an $m \times n$ 0/1 matrix, and $x$ is a 0/1 column $n$
vector, define $A \oplus x$ to be vector $(A \oplus x)_i = (A_i \oplus
x)$, where $A_i$ denotes the $i$-th row.  We say that such a matrix is
\emph{resolving} if, for all $x \ne y \in \{0,1\}^n$ we have $A \oplus
x \ne A \oplus y$.

Let $\beta_n$ be the smallest $m$ such that there is a resolving 0/1
matrix of dimension $m \times n$.

\begin{definition}
  A vector $x \in \{0,1,-1\}^n$ is \emph{balanced} if $\sum_i x_i = 0$.
\end{definition}

\begin{definition}
  An $m \times n$ 0/1 matrix $A$ is \emph{balanced detecting} if $A x
  = 0$ implies that $x = 0$ for all balanaced $x \in \{0,1,-1\}^n$.
\end{definition}
\begin{lemma}
  If $A$ is a balanced detecting matrix, then $A'$ is a resolving
  matrix, where
  \begin{displaymath}
    A' =
    \begin{bmatrix}
      1 & 1 & \hdots & 1& 1 \\
      \hline \\
      & & A
    \end{bmatrix}.
  \end{displaymath}
\end{lemma}
\begin{proof}
  Note that the all 1's vector resolves all pairs $(x,y)$ such that
  $w(x) \ne w(y)$.  Observe that $w(x \oplus y) = w(x) + w(y) - 2 w(x
  \& y)$, where $(x \& y)_i := x_i y_i$.  Thus
  $w(A \oplus x) = A e + e^T x - 2 A x$, where $e$ is the column
  vector of length $n$ with all coordinates equal to 1.

  Thus $w(A \oplus x) = w(A \oplus y)$ if and only if
  $e^T(x-y) = 2 A (x-y)$.  This shows that if $w(x) \ne w(y) \bmod 2$
  then $(x,y)$ is resolved by all vectors, and if $w(x) \ne w(y)$ then
  $(x,y)$ is resolved by the all 1's vector.
\end{proof}
\begin{proposition}
Let $n$ be a positive integer.  If $m$ is the smallest integer such
that there exists an $m \times n$ balanced detecting matrix, then the
metric dimension $\beta_n = m + 1$.
\end{proposition}
\begin{proof}
  Let $m$ be such that there is a balanced detecting matrix $A \in
  \{0,1\}^{m \times n}$. Then, by the above remark,
    the matrix $A'$ is a resolving matrix.  Thus $\beta_n \le m+1$.
    Conversely, if $B$ is a $\beta_n \times n$ resolving matrix, we
    may assume, without loss of generality, that $B_1 = 0$.  Let $B'$
    denote the $(\beta_n - 1) \times n$ matrix $B'_i = B_{i+1}$ for
    $i=1, \dots, \beta_n - 1$.  Then $B'$ must be a balanced detecting
    matrix. Thus, if there is no balanced detecting
    matrix of dimension $(m-1) \times n$ this shows that $\beta_n - 1
    \ge m$.
\end{proof}
If $A$ is an $m \times n$ matrix and $S \subseteq \{1, \dots, n\}$,
denote by $A(S) := \sum_{j \in S} A^T_j$.
\begin{proposition}
  An $m\times n$ 0/1 matrix, $A$ is balanced detecting if and only if
  for each $1 \le j \le n$ the
  collection $A x$ where $x\in \{0,1\}^n$, and $w(x) = j$ are
  distinct.  Note that the vectors in this collection are the sums of
  the columns of $A$ over all subsets of $[n]$.
\end{proposition}
\begin{proof}
  If $S \subseteq \{1, \dots, n\}$ is a subset of $[n]$, denote by
  $x^{(S)}$  thn vector $(x^{(S)}_i) = 1$ if $i \in S$ and 0
  otherwise.  If $S \ne T \subseteq [n]$ satisfy $\#S = \#T = j$, then
  $x_S - X_T \in B_j(n)$ where $j = \# (S \backslash T)$.  Thus
  $A(x_S - X_T) \ne 0$, by definition of balanced detecting.
\end{proof}
\begin{corollary}
  If an $m \times n$ 0/1 matrix is balanced detecting then for $1 \le
  n/2$ we have
  \begin{displaymath}
    (j+1)^m \ge \binom{n}{j}.
  \end{displaymath}
\end{corollary}
\begin{proof}
  If we sum up exactly $j$ columns of $A$, the resulting vector has
  coordinates in $\{0,1, \dots, j\}$.  Thus there are at most
  $(j+1)^m$ such sums.  But there are exactly $\binom{n}{j}$ possible
  subsets of $\{1, \dots, n\}$ of cardinality $j$.  In order for the
  sums to be distinct, we must then have $(j+1)^m \ge \binom{n}{j}$.
\end{proof}
We give a slightly tighter bound by examining the entropy of
some random variables.  This is a slight generalization of a method of
Pippenger, which we review below.

\subsection{Entropy Lower Bounds}
\label{sec:entropy}

By using entropy arguments about the entropy of certain random
variables we can get tighter lower bounds.  First we
recall an argument of Pippenger \cite{pippenger1977information}.

If $X$ is a random variable whose range, $R$, is finite, define
\emph{binary entropy}
\begin{displaymath}
  H(X) := - \sum_{x \in R} \Pr(X = x) \log_2 \Pr(X = x).
\end{displaymath}
If $X,Y$ are two random variables define $H(X | Y) = H(X, Y) - H(Y)
\ge 0$, where $H(X,Y) := H((X,Y))$.  We note that $H(X | Y) \ge 0$,
and $=0$ if and only if $X$ depends on $Y$.

\begin{proposition}
  Let $A$ be an $m \times n$ 0/1 detecting matrix and
  $X^{(d)}$ be a random variable satisfying
  \begin{displaymath}
    \Pr(X^{(d))} = j) = \binom{d}{j} 2^{-d}, j=0, 1, \dots, d.
  \end{displaymath}
  \begin{equation}
    \label{eq:pippenger}
    m \ge \left \lceil \frac{n}{\max(H(X^{(d)}), d = 1, \dots n)}
    \right \rceil.
  \end{equation}
\end{proposition}
\begin{proof}
  Let $Y \in [n]$ be a set valued random variable which is uniform
  among all subsets of $[n]$.  If $D \subseteq [n]$, let $X^{(D)} := \#(Y
  \cap D)$.  We have
  \begin{displaymath}
    \Pr(X^{(D)} = j) = \binom{n}{j} 2^{-\#D}, j=0, \dots, \#D,
  \end{displaymath}
  and 0 otherwise.  If $D_1, \dots, D_m$ are detecting, then, by
  definition, knowledge of $D_1, \dots, D_m$ uniquely determines the
  value of $Y$.  Thus $0 = H(Y | X^{(D_1)}, \dots, X^{(D_m)})$.
  However $n = H(Y) = H(Y | X^{(D_1)}, \dots, X^{(D_m)}) +
  H(X^{(D_1)}, \dots, X^{(D_m)}) - H(X^{(D_1)},
  \dots, X^{(D_m)} | Y) \le \sum_{j=1}^m H(X^{(D_j)}).$
\end{proof}
\begin{proposition}
  Let $A$ be an $m \times n$ 0/1 balanced detecting matrix.
  Let $Y^{(s)}$ be a set valued random variable which takes on each subset
  of $[n]$ of cardinality $s$ with equal probability $1/\binom{n}{s}$.
  If $D \subseteq [n]$ satisfies $\# D = d$, then we define the random
  variables $X^{(D,s)} = \# (Y^{(s)} \cap D)$.
  If $\#D = d$ then the distribution of $X^{(D,s)}$ is
  \begin{displaymath}
    \Pr(X^{(D,s))} = j) = \binom{n}{s}^{-1}\binom{d}{j}
    \binom{n-d}{s - j}, j=0, 1, \dots, \min(d,s),
  \end{displaymath}
  and 0 otherwise.
  % min(d,m) = d + m - max(d,m), so m - min(d,m) = max(d,m) - d = max(0,
  % m-d)
  Then, for all $s = 0, \dots, n, d=1, \dots, n$ we have
  \begin{equation}
    \label{eq:better}
    m \ge \frac{\log_2 \binom{n}{s}}{H(X^{(D,s)})}.
  \end{equation}
\end{proposition}

\section{A General Method}
\label{sec:general}

Most of the previous papers on the subject calculate the metric
dimension by reducing it to a \emph{minimal hitting set} problem.
\begin{definition}{Hitting Set}
Let $X$ be a set and $S_1 , \dots, S_m \subseteq \cS$ be subsets.

A \emph{hitting set} for $\{S_i\}$ is a subset $U \subseteq X$ such
that $U \cap S_i \ne \emptyset$ for all $i=1, \dots, m$.
A \emph{minimal hitting set} is one for which $|U|$ is minimal.
\end{definition}
If $G$ is a finite connected undirected graph, for $x,y \in V(G), x
\ne y$
we set $S_{x,y} = \{v \in V(G): d_G(v,x) != d_G(v,y)\}$, the set of
vertices of $G$ which resolve the pair $(x,y)$.  Thus $U$ is a
resolving set for $G$ if and only if $U \cap S_{x,y} \ne \emptyset$
for all $x \ne y \in V(G)$.
A minimal resolving set for $G$ is then a minimal hitting set
for $V(G)$.

Many of the previous papers find the metric dimension of a graph by
posing the minimal hitting set problem as an integer linear program:

\begin{equation}
  \label{eq:ilp}
  \begin{aligned}
    \text{Minimize } & \sum_{v \in V(G)} t_v \\
    \text{Subject to} \\
    \sum_{v \in S_{x,y}} t_v & \ge 1 \text{ for } x \ne y \in V(G) \\
    t_v \in \{0,1\}, \forall v \in V(G).
  \end{aligned}
\end{equation}

\section{An iterative method}
\label{sec:iterative}

Although this is correct, in general, the size of this program is
rather large, since there are $\binom{|G|}{2}$ constraints.  In
special case that the graph is $Q^n$ the $n$-dimensional hypercube, a
nunber of the set $S_{x,y}$ are equal to $V(G)$, in which case they
can be omitted from the constraints, and there are a number of
duplicates.  Since $|Q^n| = 2^n$, posing the problem in this way
quickly becomes impractical.  In addition, when the graph $G$ has a
large group of metric automorphisms, it is essential to use some sort
of symmetry breaking.

In order to find balanced detecting matrices, or to show that there no
balanced detecting matrices of dimension $m \times n$, we can use two
cooperating constraint solvers.  Let $M(m,n)$ denote the set of $m \times n$ 0/1
matrices.  Then the problem that we want to solve can be expressed as
follows
\begin{equation}
  \label{eq:logical}
  \exists A \in M(m,n) \forall x \in B(n), Ax \ne 0.
\end{equation}
This can be restated as follows
\begin{equation}
  \label{eq:logical:restate}
  \exists A \in M(m,n) \neg \exists x \in B(n), Ax = 0.
\end{equation}
This suggests the following method.  Have two solvers (we can either
use SAT solvers or SMT solvers).  The first solver maintains a state
of a set of elements of $B(n)$: $C:=\{x_1, \dots, x_m\}$.  That solver
tries to solve the problem
\begin{displaymath}
  \exists A \in M(m,n), Ax \ne 0 \forall x \in C.
\end{displaymath}
If there is no solution, then the original problem is UNSAT.  If there
is a solution, it then passes it to the second solver, which tries to
solve the problem
\begin{displaymath}
  \exists x \in B(n), A x = 0.
\end{displaymath}
If there is no solution, the original problem is SAT, where $A$ the
desired solution.  If there is a solution, it is passed to the first
solver, to add to its set, $C$.  This back and forth process continues
until the original problem is declared SAT or UNSAT.

One remark is that all of the constraints implied by $x \in B(n)$ are
not necessary.  If $C \subseteq B(n)$ define the set
$\cA_m(C) := \{A \in M(m,n), \forall x \in C, Ax \ne 0\}$.  We are
interested in the smallest possible set $C$ such that $\cA_m(C) = \cA_m(B(n))$.
In particular, we wish to analyze the situation in
which we have a subset $C \subset B(n)$, and $y \in B(n), y \not \in
C$.  Under what circumstances does it happen that
\begin{displaymath}
  \forall A \in M(m,n), \bigwedge_{x \in C} Ax \ne 0 \Rightarrow Ay
  \ne 0.
\end{displaymath}
Taking the contrapositive, this is equivalent to
\begin{displaymath}
  \forall A \in M(m,n), Ay = 0 \Rightarrow \bigvee_{x \in C} Ax = 0.
\end{displaymath}
If we define $N(x) := \{A \in M(m,n): A x = 0\}$, this is equivalent
to
\begin{displaymath}
  N(y) \subseteq \bigcup_{x \in C} N(x).
\end{displaymath}
If we define $N'(x) := \{z \in \{0,1\}^n : z^T x = 0\}$, we see that
$N(x) = N'(x)^m$.  Thus our condition is reduced to
\begin{displaymath}
  N'(y) \subseteq \bigcup_{x \in C} N'(x).
\end{displaymath}
\begin{lemma}
If $n \ge 3$ we have $\{0,1\}^n = \cup_{x \in B(n)} N'(x)$.
\end{lemma}
\begin{proof}
  For each $y \in \{0,1\}^n$ we exhibit an $x \in B(n)$ such that $y
  \in N'(x)$.
  We have  $0 \in Z$, since $0 \in N'(x)$ for all $x$.  If
  $y$ satisfies $w(y) = 1$, let $y_i = 1$. Then, since
  $n \ge 3$, there are $j,k \ne i$.  Choose $x$ such that $x_j = - x_k = 1$ and
  $x_\ell = 0$ for $\ell \ne j,k$. Then $y \in N'(x)$.  Finally, if
  $w(y) \ge 2$, there are $j \ne k$ such that $y_j = y_k = 1$.  Let
  $x$ be such that $x_j = -x_k = 1$ and $x_\ell = 0$ for
  $\ell \ne j,k$. Then $y \in N'(x)$.
\end{proof}
Thus to find a minimum necessary set $x \in B(n)$ we are reduced to a
minimum set covering problem.

\section{Symmetry Breaking}
\label{sec:symmetry}

In this section we discuss the use of the symmetries to optimize both
the search for a minimal balanced detecting matrix and to show that no
smaller dimension suffices.  We will restrict our discussion to the
following situation:

\begin{quotation}
  We are given a boolean formula $f(x_1, \dots, x_n)$ in boolean
  variables, $x_1, \dots, x_n$.  It is know that there is permutation
  group $G \subseteq \cS_n$ such that $f = f^\sigma$ for all
  $\sigma \in G$, where
  $f^\sigma(x_1, \dots, x_n) := f(x_{\sigma^{-1}(1)}, \dots,
  x_{\sigma^{-1}(n)})$.  We wish to exibit $\xi \in \{0,1\}^n$ such
  that $f(\xi) = 1$ or show that no such $\xi$ exists.  We have a
  total lexicographic order on the set of such $\xi$: $\xi \le \xi'$
  if and only if either $\xi = \xi'$ or there is an $1 \le i < n$,
  such that $\xi_j = \xi'_j$ for $j=1, \dots, i$, and
  $\xi_{i+1} = 0, \xi'_{i+1} = 1$.  We can restrict the set of
  considered $\xi$ to those for which $\xi \le \xi^\sigma$, for
  $\sigma$ contained in some subset of $G$.
\end{quotation}
\begin{definition}[Lexicographic Order]
  Let $S$ be a totally ordered set, and $n$ a positive integer.  The
  \emph{lexicographic order} on the $n$-fold product $S^n$ is as
  follows:

  $(x_1, \dots, x_n) \le (x'_1, \dots, x'_n)$ if and only if either $x
  = x'$ or there is an $1 \le i < n$ such that $x_j = x'_j$ for $j=1,
  \dots, i$ and $x_{i+1} < x'_{i+1}$.
\end{definition}
\section{Certificates}
\label{sec:certificates}

When we assert that the metric dimension of $Q^n$ has a particular
value, $m$, it isn't sufficicent to exhibit a purported balanced
detecting matrix of dimension $(m-1) \times n$ and to assert that some
program failed to find one of dimension $(m-2) \times n$.  In this
section we discuss \emph{certificates} for these assertions.  By a
certificate, we mean some data which can be used in an easily
checkable proof of these assertions.  This is in contrast to much of
the earlier literature, such as \cite{beardon2013resolving}, in which
the values of the metric dimension are asserted as being the result of
a long calculation, without any certification.

Since we have used SAT and SMT solvers to make the above assertions,
we can use the fact each assertion above can be proven by asserting
that a particular CNF formula is UNSAT.  Many SAT solvers can produce
what are known as DRUP (directed reverse unit propagation) proofs of
UNSAT, which we will describe below.

In order that such a proof to be a valid certificate, one must also
supplement it with a proof that the CNF formula used is encoded
correctly.

The statement that $A$ is an $m \times n$ balanced detecting matrix is
equivalent to the the statement that the only integer point in the
polytope $P := \{x \in [-1,1]^n : A x = 0, \sum_{i=1}^n x_i = 0 \}$ is
$0$.  By writing $x = y-z$ for $y,z \in [0,1]$ this is equivalent to
the polytope
$P' = \{A y = Az : y, z \in [0,1]^n, \sum_{i=1}^n (y_i - z_i) = 0,
\sum_{i=1}^n x_i \ge 1, y_i + z_i \le 1, \text{ for } i=1, \dots, n\}$
containing no integer points.  One can supplement $P'$ with a series
of $\{0, \frac 1 2 \}$ cuts, so that the resulting polytope is the
empty set.  That may be certified by means of exhibiting a Farkas
proof of emptiness.

The statement that there is no $m \times n$ balanced detecting matrix
is equivalent to the statement that the polytope
$P = \{A \in [0,1]^{m \times n} : A x = 0, \forall x \in B(n)\}$
contains no nonzero integer points.  We show below that the set $B(n)$
can be cut down, and still make the above valid.


\bibliography{metric_dimension}
\bibliographystyle{plain}
\end{document}
