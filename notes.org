#+Title: Metric Dimension
#+Latex_header: \DeclareMathOperator{\wt}{wt}
#+Latex_header: \newcommand{\RR}{\mathbb{R}}
* <2023-03-23 Thu> Introduction
Let $G$ denote a finite graph (directed or undirected). If $u,v \in
V(G)$ are two vertices of $G$ define $d(u,v)$ as being the length of
the shortest path from $u$ to $v$, and $+\infty$ if there is no such
path.  A *resolving set* for $G$ is a subset $S \subseteq V(G)$ such
that for each $u \ne v \in V(G)$ there is a $w \in S$ such that
$d(u,w) \ne d(v,w)$.  Clearly resolving sets exist, since $S = V(G)$
is one such.  The minimum cardinality of a resolving set is called the
*metric dimension* of $G$.

We are particularly interested in the metric dimension of the
hypercube $Q_n$ whose vertices are $\mathbb{F}_2^n$, and edges are
$(u,v)$ such that $u + v$ has weight 1.  Thus the distance metric $d$
is the *Hamming distance*.
** Using MAXSAT
One can approach this problem using ILP (integer linear programming)
or MAXSAT (maximum satisfiability).

For each pair $(u,v) \in V(G)$ with $u \ne v$ define the set
$P_{u,v} := \{w \in V(G) : d(v,w) \ne d(u,w)\}$.  We'll call this set the *pair
resolver*.

Clearly a set $S$ is resolving if and only if for each
$u \ne v \in  V(G)$
 we have $S \cap P_{u,v} \ne \emptyset$.  That is, $S$ contains
at least one element of each $P_{u,v}$.  In other terminology, $S$ is
a *hitting set* for the set system $\{P_{u,v} : u \ne v \in V(G) \}$.
** Questions
This brings up a number of practical questions:

1) Given a subset $S$ which is not a resolving set, efficiently
   produce a pair $(u,v)$ such that $d(w,u) = d(w,v)$ for all $w \in
   S$.  Can we do this without explicitly listing all resolving pairs?
2) Note that if $\sigma \in \text{Aut}(G)$ and $S$ is resolving then
   so is $\sigma(S)$.  This is true because $d(u,v) = d(\sigma u,
   \sigma v)$ for all $u,v \in V(G)$.

Possible approach: One only need to characterize the set of pairs
which have the same Hamming weight.

One might be able to do this inductively: Given a subset $S$, split it
into two subsets $S_0$ and $S_1$, distinguished by the first
coordinate, and denote by $\pi: \mathbb{F}_2^n \rightarrow
\mathbb{F}_2^{n-1}$ the projection onto the last $n-1$ coordinates.
A pair $(u,v)$ will be resolved by $S$ if and only if
a) $u_1 = v_1$ and $(\pi(u), \pi(v))$ is distinguished by $\pi(S)$.
b) $u_1 \ne v_1$ and $\pi(u)$ and $\pi(v)$ ...

An auxilliary question: Given a subset $S \subseteq \mathbb{F}_2^n$,
find an element $u \in \mathbb{F}_2^n$, such that $\max_{v \in S}
d(u,v)$ is minimized.  Call such a $u$ a *centroid* for $S$.

Maximum is hard to deal with.  Use the fact that average is a lower
bound to the maximum.  Suppose that $A_{i,j}$ is the $i$-th bit of the
$j$-th word in $S$.  Suppose that $x$ is the unknown centroid.

Form
\begin{displaymath}
\begin{split}
\sum_j \sum_i (x_i - A_{i,j})^2 & = \sum_i \sum_j (x_i + A_{i,j} -
2 x_i A_{i,j}) \\
 & = \sum_{i,j} A_{i,j} + |S| \text{wt}(x) - 2 \sum_i (\sum_j
A_{i,j}) x_i \\
& = \sum_{i,j} A_{i,j} - 2\sum_i (\sum_j (A_{i,j} - 1/2) - x_i).
\end{split}
\end{displaymath}

** Stabilizer by the hyperoctahedral group
The hyperoctahedral group is the group of automorphisms of $Q_n$.
More concretely, each automorphism is a pair $(\sigma, a)$ where
$\sigma \in S_n$, and $a \in \mathbb{F}_2^n$.  This acts as
$x \mapsto y$, where $y_i = x_{\sigma(i)} + a_i$.  What is the
stabilizer of an element?  We must have $x_i + a_i = x_{\sigma(i)}$.
This means that for each $\sigma$ there is a unique $a$ which makes
this true.  So each element has a stabilizer of order $n!$.  This
makes sense since the group is transitive.  What about subsets?
* Extended formulations
Here's the question: can we describe the hitting set problem more
economically?  More specifically can we specify the collection of
resolving sets more compactly with auxilliary variables?

So, to be more precise, Let $\mathcal{F}$ be a CNF formula in the set
of variables $V$.  We can think of it as specify a subset of $2^V$.  A
hitting set of this would be another subset $\mathcal{H}$ such that
for every $S \in \mathcal{F}$ there is a $T \in \mathcal{H}$ such that
$T \cap S \ne \emptyset$. 

* Some Results.

Let $Q^n$ denote the Hypecube, whose vertices are members
of $\mathbb{F}_2^n$ and whose edges connect two vertices whose weigh
differs by 1.   If $x \in \mathbb{F}_2^n$, let $\wt(x)$ denote the
*Hamming weight* of $x$, $\#\{1 \le i \le n: x_i = 1\}$, and $d(x,y)
:= \wt(x \oplus y)$.

Given $u,v \in \mathbb{F}_2^n$ let $R_{u,v} := \{ x \in
\mathbb{F}_2^n : d(x,u) \ne d(x,v) \}$.

Proposition: We have $R_{u,v} = u \oplus R_{0, u \oplus v}$.

Proposition: We have $R_{0,u} = \{ x \in \mathbb{F}_2^n:  2 \wt(u
\wedge x) \ne \wt(u)\}$. In particular, it includes all elements of
$\mathbb{F}_2^n$ if $u$ has odd weight.

Proof: Note that $\sum_{i=1}^n (-1)^{x_i} = \sum_{i=1}^n (1 - 2 * x_i)
= n - 2 \wt(x)$.  We have
$\wt(u \oplus x)  - \wt(x) = \sum_{i=1}^n ((u_i \oplus x_i) - x_i) =
$\sum_{i, u_i=1} (1- 2 * x_i) = \wt(u) - 2 \wt(u \wedge x)$.  So if $\wt(u)$ is
odd, this is always non-zero.

The equations to detect $(u,v)$ are similar (but not the same) as the
original ones:

* Bases
If $G$ is a permutation group, acting on a finite set $\Omega$, then a
*base* for $G$ is a subset $S \subseteq \Omega$ such that the subgroup
$\{ g \in G: g x = x, \forall x \in S\}$ is trivial. If $x_1, \dots,
x_m \in \Omega$ denote by $G_{x_1, \dots, x_m}$ the pointwise
stabilizer of all the $x_i$.  That is $G_{x_1, \dots, x_m} = \{g \in
G: g x_i = x_i, \forall 1 \le i \le m \}$.  The symmetry breaking
constaints can be of the following form:

Once we have chosen $x_1, \dots, x_m$, choose $x_{m+1}$ being members
of a set of representatives of distinct orbits of $\Omega$ under
$G_{x_1, \dots, x_m}$.  Call that set $S_{x_1, \dots, x_m}$.  The
clauses reprsenting this will be
$(x_1 \wedge \dots \wedge x_m \Rightarrow x)$ for all $x \in S_{x_1,
\dots, x_m}$.

Define: Let $G$ be a finite graph.  If $x,y \in V(G)$ the *resolving
set* of $(x,y)$, $R_{x,y} = \{ z \in V(G) : d(z,x) \ne d(z,y)\}$ is
the set of vertices which are at distinct distances from $x$ and $y$.

Define: An equivalence relation on the vertex set $V(G)$.  Say that
$x \sim y$ if, for all $z,w \in V(G)$ we have $d(x,z) = d(x,w)$ if and
only if $d(y,z) = d(y,w)$.  It is now clear that every resolving set
consists of a union of equivalence classes under this relation.
 
It is clear that if $\sigma$ is an automorphism of
$X$ then it is also a metric automorphism (i.e. $\sigma$ preserves
adjacency), but that might be other metric automorphisms.  A
particular example of a metric automorphism which is not a graph
automorphism occurs with the hypercube $Q^n$.  If $e$ is the all 1's
vector then $d(e \oplus x, y) = n - d(x,y)$. So the metric
automorphism group of $Q^n$ has order $2^{2n} n!$

* Some Proofs

Lemma: We have $R_{u,v} = u + R_{0,u+v}$.
Proof: By definition, $d(x,a) = \wt(x + a)$.  Thus, if $d(x,u) \ne
d(x,v)$, we have $\wt(x+u) \ne \wt(x+v)$.  But $\wt(x+u) = d(x+u,0)$
and $\wt(x+v) = d(x+u,u+v)$.

Corollary: If $S \subseteq V(Q^n)$ is a resolving set, then so is $u +
S$ for all $u \in V(Q^n)$.  Without loss of generality, we may assume
that any resolving set contains 0.

Lemma: If $e=(1, \dots, 1) \in V(Q^n)$, we have, for all $x \in
V(Q^n)$, $\wt(x+e) = n - \wt(x)$.

Corollary: For all $x \in R_{u,v}$ we have $x+e \in R_{u,v}$.

Definition: Denote by $\beta_n$ the metric dimension of $Q^n$.

Lemma: For all $n$ we have $\beta_n \le \beta_{n+1}$.
Proof:  Let $S$ be a minimum size resolving set for $Q^{n+1}$.  Let
$\pi : V(Q^{n+1}) \rightarrow V(Q^n)$ denote the map which removes the
last coordinate: $\pi((x_1, \dots, x_{n+1}) = (x_1, \dots, x_n)$, and
$\psi: V(Q^n) \rightarrow V(Q^{n+1})$ the map that adds a 0 coordinate
at the end.  That is $\psi((x_1, \dots, x_n)) = (x_1, \dots, x_n, 0)$.
Without loss of generality, by adding $e$ to any element of $S$ whose
last coordinate is 1, we may assume that the last coordinate of all
the elements of $S$ are 0.  It is clear that if $x,y \in V(Q^n)$ that
$d(x,y) = d(\psi(x), \psi(y))$.  However, by the assumption on $S$, we
have, for all $u \in S$, $u = \psi(\pi(u))$.  Thus $\pi(S)$ is a
resolving set for $Q^n$. QED.

Lemma: For all $m,n$ we have $\beta_{m+n} \le \beta_m + \beta_n$.  In
particular, since $\beta_1 = 1$, we have $\beta_{n+1} \le \beta_n +
1$.

Proof: Let $S$ be a resolving set for $Q^n$ and $T$ a resolving set
for $Q^m$.  Without loss of generality, we may assume that $0 \in S, 0
\in T$.  Define two maps $\phi: V(Q^m) \rightarrow V(Q^{m+n})$, $\phi:
V(Q^n) \rightarrow V(Q^{m+n})$ as follows $\phi((x_1, \dots, x_m)) =
(x_1, \dots, x_m, 0, \dots, 0)$, and $\psi((y_1, \dots, y_n)) = (1,
\dots, 1, y_1, \dots, y_n)$.  I claim that $U := \psi(S) \cup \phi(T)$
is a resolving set for $Q^{n+m}$.  Define maps $\rho: V(Q^{n+m})
\rightarrow V(Q^n)$ by $\rho((x_1, \dots, x_{m+n})) = (x_{m+1}, \dots,
x_{m+n}))$ and $\sigma: V(Q^{n+m}) \rightarrow V(Q^m)$ by
$\sigma((x_1, \dots, x_{m+n})) = (x_1, \dots, x_m)$.
We show that for all $x\ne y \in
V(Q^{m+n))$ there is an element of $U$ that resolves $(x,y)$. There
are three cases.

Note that for all $x,y$, $d(x,y) = d(\sigma(x), \sigma(y)) +
d(\rho(x), \rho(y))$.
Note that we have $\sigma \psi u = e$ and $\rho \psi u = u$,
$\sigma \phi v = v, \rho \phi v = 0$.

1) $\wt(\sigma(x)) = \wt(\sigma(y))$ and $\rho x \ne \rho y$.  Let $u \in S$ resolve $(\rho(x),
   \rho(y))$.  Then
   \begin{displaymath}
   \begin{aligned}
   d(\psi(u), x) - d(\psi(u), y) &= d(\sigma \psi u, \sigma x) + d(\rho \psi u, \rho  x)
                                                     -(d(\sigma \psi u, \sigma y) + d(\rho \psi u, \rho x)) \\
                                               &= d(e, \sigma x)  + d(u, \rho x) - (d(e, \sigma y)  + d(u, \rho y)) \\
                                               & = d(u, \rho x) - d(u, \rho y)
  \end{aligned}
   \end{displaymath}
   By hypothesis there is a $v \in T$ which resolves $(\rho x,
   \rho y)$.
   
2) $\wt(\rho(x)) = \wt(\rho(y))$ and $\sigma x \ne \sigma y$.  For $v \in T$
   \begin{displaymath}
   \begin{aligned}
   d(\phi(v), x) - d(\phi(v), y) &= d(\sigma \phi u, \sigma x) + d(\rho \phi u, \rho  x)
                                                     -(d(\sigma \phi u, \sigma y) + d(\rho \phi u, \rho x)) \\
                                               &= d(v, \sigma x)  + d(0, \rho x) - (d(v, \sigma y)  + d(0, \rho y)) \\
                                               & = d(v, \sigma x) - d(v, \sigma y)
  \end{aligned}
   \end{displaymath}
   By hypothesis there is a $v \in T$ which resolves $(\sigma x,
   \sigma y)$.
3) In the remaining cases either $\wt \sigma x \ne \wt \sigma y$ or
   $\wt \rho x \ne \wt \rho y$.  I assert that either 0, or $\psi 0$
   resolve $(x,y)$.  Namely
   \begin{displaymath}
   \begin{aligned}
   d(0, x) - d(0, y) &= d(\sigma 0, \sigma x) + d(\rho 0, \rho  x)
                                                     -(d(\sigma 0, \sigma y) + d(\rho 0, \rho x)) \\
                                               &= d(0, \sigma x)  + d(0, \rho x) - (d(0, \sigma y)  + d(0, \rho y)) \\
                                               & = (\wt \sigma x - \wt \sigma y) + (\wt \rho x - \wt \rho y)
  \end{aligned}
   \end{displaymath}
   and
   \begin{displaymath}
   \begin{aligned}
   d(\psi 0, x) - d(\psi 0, y) &= d(\sigma \psi 0, \sigma x) + d(\rho 0, \rho  x)
                                                     -(d(\sigma \psi 0, \sigma y) + d(\rho \psi 0, \rho x)) \\
                                               &= d(e, \sigma x)  + d(0, \rho x) - (d(e, \sigma y)  + d(0, \rho y)) \\
                                               &= -(\wt \sigma x - \wt \sigma y) + (\wt \rho x - \wt \rho y)
   \end{aligned}
   \end{displaymath}

* More efficient generation of hitting sets

Since the resolving set $R_{u,v} = u + R_{0, u+v} = v + R_{0, u+v}$,
we concentrate, first, on describing $R_{0,u}$.

Lemma: The set $R_{0,u} = \{ x : 2\wt(x \wedge u) \ne \wt(u)\}$.  In
particular, this means that if $\wt(u)$ is odd, then R_{0,u} =
\mathbb{F}_2^n$.

Proof: We first describe the complement of $R_{0,u}$.  By definition
$x \not \in R_{0,u}$ if $\wt(x) = \wt(x + u)$.  However $x = x
\wedge u + x \wedge \neg u$, where the two terms are disjoint.
Similarly $x+u = \neg x \wedge u + x \wedge \neg u$.
Thus $\wt(x) = \wt(x+u)$ if and only if $\wt(x \wedge u) = \wt(\neg x \wedge
u)$. Here $\wedge$ is elementwise.  The latter is true if and only if
$2 \wt(x \wedge u) = \wt(u)$.

We know find the size of the orbits of $R_{0,u}$ under the action of
the hyperoctahedral group.  To do that first, we find the order of the
stabilizer.  It suffice to consider only those $u$ of the form $(1,
\dots, 1, 0, \dots 0)$.  That is those in which $u_i = 1$ for $i=1,
\dots, t$ for $t$ even, and $u_i = 0$ for $i=t+1, \dots, n$.

Lemma: If $\wt(u), \wt(u')$ are even and $u \ne u'$ then $R_{0,u} \ne
R_{0,u'}$.

Proof: Since we may identify elements of $\mathbb{F}_2^n$ with subsets
of $[n] := \{1, \dots, n\}$.  Without loss of generality we may assume
that $\#U$ and $\#U'$ are both even, and that $\#U \ge \#U'$.
Let $X_1$ be a subset of $U \cap U'$ of cardinality $\lfloor \#(U \cap
U') / 2 \rfloor$, $X_2$ a subset of $U' \backslash U$ of cardinality
$\lceil \#(U' \backslash U) / 2\rceil$.
If we choose $X$ so that $w := \# (X \cap U') \ne \frac 1
2 \#U'$, then $x \in R_{0,u'}$.  Choose $X$ so that $\#(X \cap (U
\backslash U')) = \lfloor \# (U \backslash U') / 2 \rfloor$.
R_{0, u'}$.  If $\#(U \backslash U') \ge \frac 1 2 \#U$, then we may
choose $X$ to be contained in $\#(U \backslash U')$ and have
cardinality $\frac 1 2 \#U$
It suffices to prove that there exists a
$X \subset [n]$ such that $2 \#(X \cap U) = \#U$, and $2 \#(X \cap U')
\ne \#U'$.   It is clear that such an $X$ exists if and only if such
an $X$ exists with $X \subseteq U \cup U'$.
So let $w_1 = \#(X \cap (U \cap U'))$, $w_2 = \#(X \cap (\overline{U}
\cap U'))$, $w_3 = \#(X \cap (U \cap \overline{U'}))$.
Then $\#(X \cap U) = w_1 + w_3$, $\#(X \cap U') = w_1 + w_2$.
Similarly let
$z_1 = \#(\overline{X} \cap (U \cap U'))$,
$z_2 = \#(\overline{X} \cap (\overline{U} \cap U'))$,
$z_3 = \#(\overline{X} \cap (U \cap \overline{U'}))$.
Then $\#U = w_1 + w_3 + z_1 + z_3$,
$\#U' = w_1 + w_2 + z_1 + z_2$.
$\#U = 2 w_1 + 2 w_3$,
$\#U' = 2 w_1 + 2 w_2 + 2 y$ with $y \ne 0$
\begin{displaymath}
A =
\begin{pmatrix}
1 & 0 & 1 & 1 & 0 & 1 & 0\\
1 & 1 & 0 & 1 & 1 & 0 & 0\\
2 & 0 & 2 & 0 & 0 & 0 & 0\\
2 & 2 & 0 & 0 & 0 & 0 & 2
\end{pmatrix}
\end{displaymath}
Eliminating:
Elimnate $w_1$:
(a) $\#U - \#U'$ = w_3 - w_2 + z_3 - z_2$
(b) $\#U = 2 z_1 + 2 z_3 - 2 w_3$,
(c) $\#U' = 2 z_1 + 2 z_2 - 2 w_2 - y$.
Eliminate $z_1$: (b) - (c)
(d) $\#U - \#U' = 2 z_3 + 2 w_2 - 2 w_3 - 2 z_2 + 2 y$.
Add (a) + (1/2) (d): $\#U - \#U' = 2 z_3 - 2 z_2 +  y$

Without loss of generality we may assume that $\#U \ge \#U'$.
The kernel 
* <2023-04-25 Tue> Finding resolvers

Here's the problem:

We're given A partition $[r,s,s]$ of $n=r + 2s$ corresponding to
 putting all 0's in the $r$ part, all 1's in the first $s$ part and
 all $-1$s in the second $s$ part. We'd like to find all 0/1 tuples
 so that $x \cdot s != 0$ in efficient way.  First, the ones in the
 first part (or size $r$) can be arbitrary.  The only restriction both
 necessary and sufficient, is that the number of 1's in both of the
 $r$ parts are not equal.  Since we're only interested in those
 assignments which are inequivalent under permutations preserving the
 partition, we should put all the 1's first, before the 0's.  The
 number of assignments that yield 0 is then
\begin{displaymath}
2^r \sum_{j=0}^s \binom{s}{j}^2.
\end{displaymath}
Recall that if $f(x) = \sum_{j=0}^s \binom{s}{j}x^j = (1+x)^s$, then
$f(x) ^ 2 = \sum_{j=0}^{2s} (\sum_{i=0}^{\min{j,s}} \binom{s}{i}
\binom{s}{j-i}) x^j$.  The coefficient of $x^s$ in this is
$\sum_{i=0}^s \binom{s}{i}\binom{s}{s-i} = \sum_{i=0}^s
\binom{s}{i}^2$.  Thus this is $\binom{2s}{s}$.

* <2023-04-25 Tue> Certifying resolving

If we're given a putative resolving set as an $m \times n$ matrix, $A$
whose rows are the members of the set, either it does not resolve, on
which case a SAT solver can find a pair of nodes which can't be
distinguished, say, $u,v$.  This is equivalent to $A (u-v) = 0$.  If
it does resolve, we'd like a good certificate.  This is equivalent to
showing that the only solution to $A x = 0$ where all the coordinates
of $x$ are $0,\pm 1$ is the 0 vector.  This would be equivalent to the
there being no 0/1 solutions to $A (x-y) = 0$ with $x_i + y_i \le 1$
and $\sum_i (x_i + y_i) \ge 1$.  So, how many $0,\frac 12$ cuts would
we need to show this?  Somehow I think that I can reduce this to a
decoding problem for linear codes over GF(2).

First note that we can massage the matrix $A$ so that it's a 0/1
matrix.  If $e$ is the all 1 vector we then have $Ax + Ay  \le A e$.
Adding this to $Ax - Ay = 0$ we get $2Ax \le A e$. 

* <2023-04-27 Thu> An LP certificate?

Suppose that we want to verify that the metric dimension is $> m$.
This would be equivalent to the fact that for all $m \times n$ 0/1
matrices $A$ we have $Ax = 0$ there exists $x \in \{0,1,-1\}^n$, such that
$e^T x = 0$, where $e$ is the all 1's vector, and $x \ne 0$.

Another try: Consider only those vectors $y$ which have precisely one
coordinate +1 and one coordinate -1 and the rest 0.   Note that all
test vectors $x$ can be written as a sum of such vectors.  Thus if all
$y$ vectors satisfy $Ay = 0$ then so do all $x$ vectors.  So, taking
negation, if there exists an $x$ vector with $Ax \ne 0$ then there
exists a $y$ vector with $Ay \ne 0$.  However, even if there is a $y$
vector with $Ay \ne 0$, that doesn't mean that there isn't a sum of
such that yields 0.

This suggests an incremental method.  Start with a solution to all
weight 2 vectors.  From the solution we can find a description of all
disjoint sums which yield 0.  Add those clauses as conflict clauses.
In particular, one can just consider pairs 

Incrementally we can do the following: set up two sat solvers.
One of them will either find a counterexample, or, by showing UNSAT,
that all pairs are resolved.  We can have assumptions which are set to
the the current assignment for the $A$ matrix.  If it finds a
counterexample it will also generate other counterexamples by
permuting coordinates, with each permutation preserving the current A
matrix.   There may be too many permutations, so one would just want
to use a small generating set of the stabilizer.  An observation,
since there are lots of symmetries, one would like symmetry breaking
clauses.  First, since negating a test vector with nonzero value also
gives one with a nonzero value one can assume that the the nonzero
value is $\ge 1$.  We also can assume that, within a sector, that the
-1 come first, then 0's, then 1.  We can encode that in terms of the
assumption variables.  Note that if we take as necessary that all
columns are distinct, then all sectors have size 1, so this is
unnecessary.

This looks like a possible Skolem function: Given a boolean formula
$\phi(X,Y)$, where we want $\forall X \exists Y \phi(X,Y)$, is there a
boolean function $F(X)$ such that $\phi(X,F(X))$ is a tautology?

$E(X,Y,Y') := \phi(X,Y) \vee \neg \phi(X,Y') \vee (Y' \Leftrightarrow F(X))$
If $E$ is UNSAT, things are good, otherwise it produces a
counterexample.  How to repair?  They use MAXSAT.  Hard clauses
$E(X,Y,Y') := \phi(X,Y) \vee \neg \phi(X,Y')$.
soft $Y' \Leftrightarrow F(X)$

The other SAT solver will have the initial "bare bones" problem with
successive conflict clauses.

Another aside: If $n$ is large it's probably better to use Hermite
normal form of the putative $A$ matrix to find its kernel.  We then
have a lattice which will contain all possible countexamples.  One
could aggresively reduce this.

This is equivalent (?) to the following: let $V = \{A \in
\RR^{m \times n} : A x = 0, \forall x \in \{0,-1,1\}^n, e^T x = 0\}$.
Then we want all 0/1 matrices $A$ to be contained in $V$.  Since $V$
is convex this is equivalent to $[0,1]^{m \times n} \subseteq V$.
More details:  It's clear that $V$ is a linear subspace.  So let $W$
be the linear subspaces of $\RR^n$ which is spanned by $\{x \in
\{0,-1,1\}^n, e^T x = 0, x \ne 0\}$.  Then $V = \{A \in \RR^{m \times
n} : A v = 0, \forall v \in W\}$.  Conversely, if $[0,1]^{m \times n}
\not \subseteq V$, that's the set in which we can find a resolving
matrix.

Something is wrong with above since $[0,1]^n$ is full dimensional but
$V$ clearly is not.

Aside: If $U, V$ are convex sets is $U \backslash V$ either empty or
convex?   No, clearly not, consider the complement of $[0,1]^n$.

* <2023-04-27 Thu> Using a solver with assumptions
A solver with assumptions has the following property:
You can invoke it giving it a list of literals which are assumed
true.  The use of this is to be able to "turn clauses on and off".
For example this is used in RC2.

* <2023-05-02 Tue> Negation
I thought that the following should produce a negation of a CNF
#+begin_src python
  def implies(pool: IDPool,
              form1: Iterable[CLAUSE],
              form2: Iterable[CLAUSE]) -> Iterable[CLAUSE]:
      """
      Clauses instantiating cl1 -> cl2.
      """
      avatars = []
      for clause in form1:
          # Make lit equisatisfiable with clause
          if len(clause) > 1:
              lit = pool._next()
              yield [-lit] + clause
              yield from ([-elt, lit] for elt in clause)
          else:
              lit = clause[0]
          avatars.append(- lit)
      yield from (avatars + clause for clause in form2)

      def negate(pool: IDPool, formula: Iterable[CLAUSE]) -> Iterable[CLAUSE]:
      """
      Negate a formula.
      """
      yield from implies(pool, formula, [[]])
#+end_src

Suppose that $F$ is a CNF, $F = \bigwedge_i C_i$ where $C_i$ is a clause
(a disjunction of literals), $C_i = \bigvee_j \ell_{i,j}$.  Introduce
new variables $x_i$, the clauses $\neg x_i \vee C_i$, and $\neg
\ell_{i,j} \vee x_i$ for all $i,j$.  Then $F$ is equisatisfiable with
$\bigwedge_i x_i$.  Thus, $\neg F$ is equisatisfiable with $\bigvee_i
\neg x_i$.  Suppose that $S$ is a compatible set of literals, such
that $F(S)$ is satisfiable (here $F(S)$ means the result of unit
propation after assuming all the literals in $S$).  Can it happen that
$\neg F(S)$ is satisfiable?  This would be equivalent to there exists
and $i$ such that $\neg C_i(S)$ is satisfiable.

More details: suppose that $F(X,Y)$ is a formula, where the variables
in $X$ and $Y$ are disjoint.  Suppose that $x_0$ is an assignment of the
variables in $X$, such that there is a $y_0$ with $F(x_0, y_0)$ true.
If we look at $\neg F(X,Y)$ is it true there there is no assignment
$y_1$ such that $F(x_0,y_1)$ is false?

Say $A$ is a desired set, and we find a formula such that
$A = \{ x : \exists y, F(x,y) \}$.  We want to describe the complement
of $A$.  This would mean that for all $x \in A$ for all $y$ $F(x,y)$
is false.

Suppose that we have $F(X,Y), G(X,Y)$ as CNF and we wish to encode
$F(X,Y) \vee G(X,Y)$.  Again, it might happen that if there are
$x,y_1, y_2$ such that $F(x,y_1)$ is true, $G(x,y_1)$ is false, but
$F(x,y_2)$ and $G(x,y_2)$ are both false.

But suppose that $z,w$ are new variables and we encode $F(X,Y)
\Rightarrow z, G(X,Y) \Rightarrow w$.

In the BDD encoding the auxilliary variables are completely determined
by the $X$ variables.

* <2023-05-02 Tue> The results

Definition: If $G$ is an unidirected graph, let $d_G(x,y)$ denote the
length of the shortest path in $G$ between $x$ and $y$.  A subset $S
\subseteq V(G)$ is *resolving* if for every $x \ne y \in V(G)$ there
is a $z \in S$ such that $d_G(x,z) \ne d_G(y,z)$.  The *metric
dimension* of $G$ is the cardinality of the smallest resolving set.

Definition: If $S$ is a set, the *Hamming Distance* between two
$n$-tuples in $S^n$ is the number of positions in which they differ.

Notation: If $n\ge 1$ is an integer, denote by $Q^n$ the *hypercube of
dimension $n$*.  This is the undirected graph whose vertices are
labeled with $n$-tuples of 0/1, with two vertices connected by an edge
if the *Hamming distance* between their labels is 1.

We are particularly interested in the metric dimension of the hypercube:
$Q^n$.   Denote by $\oplus$ the operation on the set $\{0,1\}$ given
by $0 \oplus 0 = 1 \oplus 1 = 0$, and $0 \oplus 1 = 1 \oplus 0 = 1$.
We note that for $x,y \in \{0,1\}$ we have $(-1)^{x \oplus y} =
(-1)^{x+y}$.  We also have $(-1)^x = 1 - 2x$.  We extend the $\oplus$
operation to $n$-tuples coordinatewise.

Notation: if $A$ is a 0/1 matrix, or $v$ a 0/1 vector, denote by $A'$
(resp. $v'$) the matrix whose $(i,j)$ entry is $(-1)^{A_{i,j})$
(resp. $i$-th entry is $(-1)^{v_i}$).
Definition: An $m \times n$ matrix, $A$, with entries in $\{0,1\}$ is a
*sensing matrix* if $A'u' \ne A'v'$ for all $u \ne v \in \{0,1\}^n$.

Note that if $u,v \in V(Q^n)$ then $\sum_i (-1)^{u_i} (-1)^{v_i} =
\sum_i (1 - 2 (u_i \oplus v_i)) = n - 2 d_{Q^n} (u,v)$.

Note that $d_{Q^n}(z,u) \ne d_{Q^n}(z,v)$ if and only if
$0 \ne \sum_i (1-2z_i)(1-2u_i) - (1-2z_i)(1-2v_i) =
2\sum_i (1-2z_i)(u_i-v_i)$.  Or
$\sum_i (u_i - v_i) = 2\sum_i z_i (u_i - v_i)$
Let $A$ denote the matrix whose rows are the $z_i$.
Then
\begin{displaymath}
B := 
\begin{bmatrix}
1 & 1 & \dots & 1 \\
& & -2A \\
\end{bmatrix}
\end{displaymath}
Then $A$ is a sensing matrix if and only if $2A x = e^T x$ for all vectors
$x \ne 0$ whose coordinates are in $\{0,-1,1\}$, $e$ is the all 1's vector.

Note: If the vectors, $x$ are restricted to those of the form
$\phi(u) - \phi(v)$, then $e^T x = 0$.

Lemma: A subset $S \subset V(Q^n)$ is a resolving set for $Q^n$ if and
only if the $m\times n$ matrix whose rows are the labels of $S$ is a
sensing matrix.

Lemma: Let $a \in \{0,1\}^n$, and $e = (1, \dots, 1)$.  A set $S
\subseteq V(Q^n)$ is resolving if and only if $a \oplus S$ is
resolving, where $a + S := \{a + \ell(v) : v \in S\}$.  A set $S$ is
resolving if and only if for every $x \in S$ the set $S' =
S\backslash\{x\} \cup \{e + x\}$ is resolving.

Definition: A sensing matrix is normalized if its first row is 0.

Corollary: The metric dimension of $Q^n$ is $\ge m$ if and only if
there exists an $m \times n$ normalized sensing matrix.

Definition: A 0/1 matrix, $A$ is *subsensing* if none of its rows is 0
and $Ax \ne 0$ for all $x \in \{0,-1,1\}$ such that $e^T x = 0$.

Corollary: The metric dimension of $Q^n$ is $\ge m$ if and only if
there exists an $(m-1) \times n$ subsensing matrix.

Proof: Let $A$ be a normalized sensing matrix.  

Notation: If $A$ is a matrix and $S$ is a subset of its columns,
denote by $A_S$ the sum of the columns indexed by $S$. 

Proposition: An $m \times n$ 0/1 matrix $A$ is a subsensing matrix if
and only if the the quantities $A_S$ for $S$ all subsets of column
indices of cardinality $\lfloor n / 2 \rfloor$ are distinct.

Corollary: An $m \times n$ subsensing matrix exists if and only if one
exists whose rows and columns are non zero and lexicographically
strictly increasing with respect to row/column index.

Proof: By the proposition, it is clear that permuting the rows and/or
columns of a subsensing matrix does not affect the property of being
subsensing.

* <2023-05-03 Wed> Hamming Graphs
Let $S$ be a finite set.  The *Hamming distance* on the set $S^n$ is
defined by:
$d((a_1, \dots, a_n), (b_1, \dots, b_n)) = \#\{1 \le i \le n: a_i \ne b_i\}$

The $n$-th order Hamming graph over $S$ is a graph whose vertices are
labeled by distinct $n$-tuples of elements of $S$, with an edge
between two vertices when the Hamming distance between their labels is
1.

We define a map from $\phi: S^n \rightarrow \{0,1\}^{S \times n}$, by
a *one-hot* mapping, $s \mapsto v$ where $v_t = 0$ if $t\ne s$ and
$v_s=1$.

Lemma: If $s,t \in S^n$, then $2 H(s,t) = d_H(\phi(s), \phi(t))$.

* <2023-05-05 Fri> Quantitative statements

Definition: A *separating vector* of length $n$ is  a vector $x$  with
coordinates in $\{0,-1,1\}$ and satisfying $\sum_{i=1}^n x_i = 0$, and
with $\#{ 1 \le i \le n : x_i = 1\} \ge 2$.

Lemma: The number of separating vectors of length $n$ is
$(3^n - 1)/2 -2 \binom{n}{2}$.

Proof: The number of separating vectors is
$\sum_{j=2}^{\lfloor n/2 \rfloor} 2^j \binom{n}{n-2j,j,j}$.
Namely, then number of separating vectors, $x$, with $\#\{1 \le i \le
n : x_i = 1\}$ is $2^j \binom{n}{n-2j,j,j}$.  However
$\binom{n}{n-2j,j,j} = \binom{n}{2j} \binom{2j}{j}$
Let $F(X) = \sum_{j=0}^{\lfloor n/2 \rfloor} \binom{n}{2j} X^{j}$.
and $G(X) = \sum_{j=0}^{n} \binom{2j}{j} (2X)^j$.
Note that $2 F(X) = (1 + \sqrt{X})^n + (1 - \sqrt{X})^n$

#+begin_export ascii
From OEIS:

A002426		Central trinomial coefficients: largest coefficient of (1 + x + x^2)^n. 

a(n) is asymptotic to d*3^n/sqrt(n) with d around 0.5.. - Benoit
Cloitre, Nov 02 2002, d = sqrt(3/Pi)/2 = 0.4886025119... - Alec
Mihailovs (alec(AT)mihailovs.com), Feb 24 2005
#+end_export

* <2023-05-12 Fri> Inclusion-Exclusion
Suppose that we have a finite set $A$ of cardinality $n$, and a
collection of subsets $S_i \subseteq A$, each satisfying
$|S_i|/|A| \ge \alpha > \frac 1 2$. We wish to give a lower bound to
the a set $C$ of indices such that $\bigcap_{j \in C} S_i =
\emptyset$.  Let $T_i = A \backslash S_i$.
Let $\chi$ denote the characteristic function.
$|S_i \cap S_j| = \sum_{x \in A} \chi_i(x) \chi_j(x) = \sum_x
(\ch_i(x) - \alpha) \chi_j(x) + \alpha\sum_x \chi_j(x)$

Lovasz Local Lemma?  The events are indexed by 0/1/-1 vectors summing
to 0.  If we have a random 0/1 vector, the probability that dotting
that with the event vector will be $2^{-n}$ times $\sum_{j=0}^k\binom{n}{2j}
\binom{2j}{j}$, where the event vector has $k$ 1's and $k$ -1's.  So
which other events are conditionally independent.

I think that independence is just linear independence.

If $v_1, \dots, v_r$ are 0/1/-1 vectors let $S(v_i)$ denotes the set
of all 0/1 vectors in the whose dot product with all the $v_i$ are 0.

Case: $k$-vectors.  Call a set of indices a *sector* if all of the
columns corresponding to those indices are equal, and that subset is
maximal with respect to that property.  

* More symmetries?

Let $A$ be a feasible $m \times n$ matrix.  We know that any
permutation of the rows and/or columns of $A$ is also feasible.  In
addition, for any row $c$ the matrix $C \oplus A$ is feasible, where
$C$ is the $m \times n$ matrix all of whose rows are equal to $c$.  We
would like $c$ to be one of the rows of $A$, so that one of the rows
of $C \oplus A$ is the all 0 row.  Among all of these matrices, we'd
like to choose the one which is lexicographically minimal.  This seems
too complicated.

For SAT problems Knuth (in volume 4.6, section 7.2.2.2) points out
that it is only necessary to find an *endomorphism*.  That is, if $x
:= x_1, \dots, x_n$ are boolean variables, then $\tau: \{0,1\}^n
\rightarrow \{0,1\}^n$ is an *endomorphism* of a formula $F$ if
whenever $x$ satisfies $F$ then so does $\tau(x)$.  Notice that it is
not necessary to specify any behavior for $\tau(x)$ when $x$ does
*not* satisfy $F$.  He points out that it is then valid to add the
constraints $x \le \tau(x)$, where $\le$ is any total order (most
likely lex).

I conjecture that there is a feasible $A$ if and only if there is a
feasible $A$ all of whose nonzero rows have weight $\lfloor n/2
\rfloor$.  Here's a try: Let $c$ be a row whose weight is $< \lfloor
n/2 \rfloor$.   We need to show that if $x$ is a balanced $\{0, \pm
1\}$ vector such that $c \cdot x \ne 0$, then there is a position is
$c$ containing a 0, so that $c'$, obtained from $c$ by changing that
to a 1, has the property that $c' \cdot x \ne 0$.  It really reduces
to the case that $c \cdots x = 1$, since $-x$ is also a test vector.
So now this means that we can find a position where $c$ contains a 0,
but $x$ contains a 0 or a 1.  And we need to do this for all $x$ such
that $c \cdot x = 1$.  Note that the only nonzero elements of such an
$x$ that matter, are those under the support of $c$.  Suppose that
under that support there are $s$ 1's and $s-1$ -1's.  This means that
$2s-1 \le r$, where $r$ is the weight $c$.  So this can't work since,
if there's at least 1, -1 left outside of the support, it can go into
every coordinate position.  So an alternative would be to show that
the *number* of resolved $x$'s never decreases.  In particular, this
would show that for every good $x$ that's killed, at least one bad $x$
will be made good.  So, the situation is this: there are $s$ 1's and
$s-1$ -1's under the support of $c$, and $j$ is an index outside of
the support of $c$ such that there are $x$, when restricted to $c$ has
the pattern given, but $x_j = -1$.  Since $x$ is balanced, its sum
outside of the support of $c$ is $-1$.  The vectors that are bad of
$c$ are those with $t$ 1's and $t$ -1's under the support of $c$.  All
we have to do is to show that there is one such with a $\pm 1$ in
position $j$.  If $2s-1 < r$ then we can always move a -1 from outside
of the support of $c$ to the inside of the support of $c$, since $r
\le n/2$.  More specifically, there is always an $x$ with maximal
weight: $a$ 1's and $a$ -1's where $a = \lfloor n / 2 \rfloor$.
position under the support of $c$.

We want to show that for every $c$, such that $\wt(c) < \lfloor n/2
\rfloor$, there is a map $f$ from the set $S := \{x : c \cdot x = 1\}$ to
the set $T := \{x : c \cdot x = 0\}$ with the property that if $x_j =
-1, x \in S$ for $c_j = 0$ then $f(x)_j \ne 0$.

Here's a strategy: if the weight of $x$ is large enough, there will
always be a guarantee of a $+1$ outside of the support of $c$ in
addition to the $-1$.  We can then consider the vector that swaps
those two.  So we only have the case that the only nonzeros outside of
the support of $c$ are $-1$'s.  Since, by choice, this means that
there is precisely one $-1$.  The extreme case happens if the support
of $c$ has size 1.  When we set a position outside the support of $c$
from 0 to 1 we actually make a lot of bad vectors good: Any vector
whose support is disjoint from $c$, and any vector which isn't
balanced in $c \cup \{j\}$.  So we just need to calculate the
difference between the number of balanced vectors in $c \cup \{j\}$
and the unbalanced vectors in $c \cup \{j\}$.

Let $B(r)$ be the number of balanced vectors of length $r$.
At the beginning there are $B(n) - B(r)$ good vectors.
After setting $r \leftarrow r+1$ there are $B(n) - B(r+1)$ good
vectors.  So as long as $B(r+1) \le B(r)$ we've won.  But this ignores
the interaction with the other rows, since it is not true that the set
of good vectors is increasing, just their number.   We partition the
sets of columns into $P_1 \cup P_2 \cup \dots \cup P_s$ so that the
columns in each $P_i$ are constant (i.e. all have the same value).
Changing a 0 to a 1 in some position, $j$, will remove one column from the
part containing $j$ and either make a new part of size 1 or merge it
into another part.

A part is "pure" if all the elements are = 1, otherwise it's
"impure".  If all parts are impure then the set of bad vectors must
all be balanced within in each part (since all 0's kills everything).
Is this true?

* A False theorem

If $m$ is a positive integer, denotes by $[m] = \{1, \dots, m\}$.  If
p$U$ is a finite set, and $S_1, \dots, S_m \subseteq$.  If $T \subseteq
[m]$, denote by $B_m[V,T] = \bigcap_{j \in T} \cap \bigcap_{j \in
[m]\backslash T} (V \cap S_j) \left( V \cap (V \backslash S_j)\right)$.  If $x:U
\rightarrow A$, where $A$ is an additive abelian group, and $S
\subseteq U$, denote by $x(S) = \sum_{a \in S} x(a)$.

Proposition: Let $U$ be a finite set, $S_1, \dots, S_m \subseteq U$,
and $x:U \rightarrow A$, where $A$ is an additive abelian group.  Then
$x(U) = 0$ and $x(S_j) = 0$ for all $1 \le j \le m$ if and only if
$x(B_m[U,T]) = 0$ for all $T \subseteq [m]$.

Proof: The "only if" part is straightforward, since, for all $j$ we
have $S_j = \bigcup_{T \subset [m], j \in T} B_m[U,T]$, where the
union is disjoint.  For the "if" part we prove the statement by
induction on $m$.  If $m=1$, note that $x(U) = x(U \backslash S_1) +
x(S_1) = x(B_1[U,\emptyset]) + x(B_1[U,\{1\}])$. If $m > 1$, if $T
\subseteq [m-1]$, by induction, we have $x(B_{m-1}[U,T]) = 0$.
We have $x(B_{m-1}[U,T]) = x(B_{m-1}[U,T] \cap S_m) + x(B_{m-1}[U,T]
\backslash S_m)$. [This is what is wrong: it is not necessarily true
that the first term on the right hand side is 0].

Perhaps we can apply it to the case where the universe is $S_m$ and
the subsets are $S_m \cap S_1, \dots, S_m \cap S_{m-1}$.

#+begin_export ascii
[0000]
[1100]
[1010]

x_1 + x_2 + x_3 + x_4 = 0
x_1 + x_2                     =  0
x_1           + x_3          = 0

x_3 + x_4 = 0
x_1 + x_2 = 0
x_2 - x_3  = 0

==> x_1 = -x_2 = -x_3 = x_4
#+end_export

What I'd like to show is that if a nonzero row has weight $< \lfloor
n/2 \rfloor$, that there is a position in that row containing a 0,
which we can make a 1, but not allow more 0 sums in the other rows.
